# Pipeline Optimization Analysis — 2026-02-17

## What This Aims to Solve

The training data pipeline (stages 01-11 with sub-stages 06-06h) transforms YouTube coaching videos into vector-embedded knowledge chunks for RAG retrieval. It has grown to 18 stages with 5 LLM stages, 6 deterministic stages in the 06 chain alone, and 5 validation gates. The pipeline works but has:

- **Cost inefficiency**: 06g makes 2,564 individual LLM calls across 59 files (some videos need 500+ calls)
- **Unnecessary LLM spend**: 06b uses Opus/Sonnet for ALL videos even when 06 confidence is high (44% APPROVE, 56% FLAG, 0% REJECT)
- **No cross-video deduplication**: Only within-video teaser dedup exists. Coaching content repeats across videos
- **Stage bloat**: Some adjacent DET stages (06c+06d, 06g+06h) share inputs/outputs and could merge
- **Subprocess overhead**: Pipeline-runner spawns 110 subprocesses for 10 videos

## Process: 5-Agent Team Debate

### Agent 1 — Page Builder
Built `/app/test/pipeline/page.tsx`: interactive visualization of all 18 current pipeline stages with color-coded flow diagram (EXT=blue, LLM=purple, DET=green, TS=orange), flags table, validation gates, and expandable stage details. Added to test dashboard at `/test/pipeline`.

### Agent 2 — Reviewer
Cross-referenced every stage in the visualization against actual script code. Verified stage names, descriptions, tools, outputs, categories, flags, and validation gates. Fixed inaccuracies. Page grew from 764 to 1046 lines.

### Agents 3 & 4 — Devil's Advocates (Alpha vs Beta)
- **Alpha** advocated for radical changes. Initially claimed 06g produced zero output (wrong).
- **Beta** defended the current architecture and caught Alpha's data error with comprehensive analysis (06g actually processes 2,564 seeds with 24 repairs, 229 anchors, 42 conversation blocks).
- Alpha corrected, adopted Beta's counter-proposals.
- 2 rounds of structured debate reached full consensus on 7 optimizations.

### Agent 5 — Optimizer Builder
Built the optimized pipeline visualization below the current one on the same page, showing all 7 agreed proposals.

## Agreed Optimizations (HIGH-LEVEL, ARCHITECTURE ONLY)

| # | Proposal | Type | Impact | Risk |
|---|---|---|---|---|
| R1 | Batch 06g seeds 5-10 per prompt | OPTIMIZE | ~80% fewer 06g LLM calls | Low |
| R2 | Skip 06g for non-infield videos | OPTIMIZE | Eliminate 06g for ~30-40% of videos | Low |
| R3 | Tiered model for 06b (Haiku/Opus) | OPTIMIZE | ~5-10x cost reduction on easy cases | Low |
| R4 | Pipeline-runner subprocess optimization | INFRASTRUCTURE | Reduce 110 subprocess invocations | Very low |
| R5 | Merge 06c+06d → patch-sanitize | MERGE | -1 stage, -1 I/O cycle | Low |
| R6 | Merge 06g+06h → adjudicate-propagate | MERGE | -1 stage, cleaner data flow | Low |
| R7 | New 09b.DET.cross-dedup | NEW STAGE | Eliminate cross-video duplicates | Medium |

**Result**: 18 → 17 stages, all quality signals preserved, all validation gates preserved.

## What Was NOT Analyzed (Scope for Next Session)

### Individual Script-Level Optimizations
The debate focused on **stage ordering, merging, and architectural changes**. The agents did NOT optimize the internals of individual scripts. Areas to explore:

1. **06.LLM.video-type** (~800 lines Python)
   - Does the prompt do too much in one call? (video type + speaker roles + conversation boundaries + teaser detection)
   - Could splitting into focused sub-tasks improve accuracy?
   - What's the token cost per video? Is the prompt efficient?
   - Is the 10-check validation suite at the end catching real issues or mostly noise?

2. **06b.LLM.verify** (~600 lines Python)
   - 5-pass verification (role coherence, boundaries, commentary, collapsed speakers, structural) — are all passes equally valuable?
   - Fallback chain (full prompt → JSON conversion → schema repair) — how often is each fallback triggered?
   - What's the REJECT/FLAG/APPROVE distribution per video type?

3. **06e.LLM.quality-check**
   - What's the false positive rate on artifact detection?
   - Is the repair_confidence threshold (0.85) optimal?
   - Could cheaper models handle quality-check?

4. **06g.LLM.damage-adjudicator**
   - 12.3% LLM failure rate (316 of 2,564 calls) — why?
   - 7 files have 100% failure rate — what's special about them?
   - Near-zero repair acceptance — is the 0.90 threshold too aggressive?

5. **07.LLM.content**
   - Token usage per video type (infield vs talking_head vs podcast)
   - Technique/topic extraction accuracy
   - ~100-segment windowing with overlap — is overlap size optimal?

6. **Prompt optimization** across all LLM stages
   - Are prompts too verbose? Could they be shortened without quality loss?
   - Structured output vs free-text — which stages benefit from which?
   - Temperature/sampling params — are they tuned?

7. **Pipeline-runner internals**
   - asyncio implementation — are there bottlenecks in the event loop?
   - Semaphore count (default 10) — is this optimal for the hardware?
   - Error recovery — what happens on partial failure?

8. **Validation scripts**
   - validate_manifest.py coverage — are there gaps?
   - batch_report.py drift detection — is chi-squared the right test?
   - Semantic judge integration — how reliable is it?

### Data Quality Deep Dive
- What does the quarantine file actually contain? How many videos are quarantined and why?
- Waiver usage patterns — are waivers being used as a crutch?
- Stage 08 taxonomy gate — what concepts are being flagged?

## File References
- Pipeline test page: `/app/test/pipeline/page.tsx`
- Pipeline scripts: `/scripts/training-data/` (01-11 + 06b-06h)
- Orchestrators: `/scripts/training-data/batch/sub-batch-pipeline`, `/scripts/training-data/batch/pipeline-runner`
- Schemas: `/scripts/training-data/schemas/`
- Prompts: `/scripts/training-data/prompts/`
- Validation: `/scripts/training-data/validation/`
- Pipeline config: `/scripts/training-data/batch/pipeline.config.json`
- Batch manifests: `/docs/pipeline/batches/`

## Instructions for Next AI Session
1. Read this file first for context
2. Read the pipeline test page (`/app/test/pipeline/page.tsx`) to see both current and optimized visualizations
3. Focus on **individual script internals** — the high-level architecture is settled
4. Read actual script code, not just descriptions
5. Check real output data in `/data/` for empirical evidence
6. Prioritize by impact: LLM stages (06, 06b, 06e, 06g, 07) are the most expensive
7. Look at prompt files in `/scripts/training-data/prompts/` for optimization opportunities
8. Consider whether any LLM stage could use a cheaper model without quality loss
