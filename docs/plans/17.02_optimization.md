# Plan: 17.02 Pipeline Optimization

**Status:** Draft
**Created:** 2026-02-17

---

## Context

Audit of runbook, ASCII diagram, and all pipeline scripts revealed alignment issues. Most critically, the pipeline's quality-gate and repair system is half-removed — config keys deleted but enforcement code left behind, repair logic orphaned in a function nothing calls. This plan starts by defining what the system SHOULD be, then aligns everything to it.

---

## Milestone 0: Define and Align the Quality Gate / Repair Policy

**This milestone must complete first. Everything else depends on it.**

### The Problem: Half-Removed System

The pipeline had a "gate policy" system controlling whether Stage 07 processes videos based on 06b verification verdicts. The user removed the config (`stage07_gate_policy: "approve_only"`) and the reverify pre-gate from `sub-batch-pipeline`, but left enforcement code scattered across multiple files. Additionally, 06e repair application was only ever wired into the `--revalidate` path, never the normal `process_video_file()` flow.

### Current State — Every Location That Still References the Old System

**Terminology that exists in code right now:**

| Term | File | Line(s) | Status |
|------|------|---------|--------|
| `_stage07_gate_decision()` | `validate_manifest.py` | 123-135 | ACTIVE — hardcoded APPROVE-only, no config override |
| `gate_policy_violations` | `validate_manifest.py` | 1204, 2096, 2110, 2420, 2530 | ACTIVE — counts + reports errors |
| `verification_gate` summary key | `validate_manifest.py` | 2417-2421 | ACTIVE — in validation output JSON |
| `check_verification_gate()` | `07.LLM.content` | ~5310-5321 | ACTIVE — blocks non-APPROVE from processing |
| `--skip-verification` | `07.LLM.content` | 5562-5564 | ACTIVE — exists but pipeline never passes it |
| `--verification-root` | `07.LLM.content` | 5525-5529 | ACTIVE — reads 06b verdicts |
| `revalidate_video_file()` | `07.LLM.content` | 4779-5047 | ACTIVE — has repair code, but only `--revalidate` calls it |
| `--revalidate` | `07.LLM.content` | 5571-5573 | ACTIVE — flag exists but pipeline never passes it |
| `REPAIR_ACCEPT_THRESHOLD` | `07.LLM.content` | 270 | ACTIVE — 0.85, used by revalidate only |
| `REPAIR_ACCEPT_THRESHOLD` | `06e.LLM.quality-check` | 54 | ACTIVE — 0.85 |
| `"Legacy: ... kept for revalidate compat"` | `07.LLM.content` | 266 | Comment referencing old system |
| `stage07_gate_policy` | `pipeline.config.json` | — | REMOVED (uncommitted) |
| `CFG_STAGE07_GATE_POLICY` | `sub-batch-pipeline` | — | REMOVED (uncommitted) |
| `--verification-gate-policy` | `sub-batch-pipeline` → stage 07 | — | REMOVED (uncommitted) |
| `--allow-flag` | `sub-batch-pipeline` CLI | — | REMOVED (uncommitted) |
| `emit_reverify_quarantine()` | `sub-batch-pipeline` | — | REMOVED (uncommitted) |
| `06b.LLM.reverify` | script file | — | DELETED from disk (uncommitted) |
| `reverify_patched` references | `iteration-history.md` | multiple | Historical — OK to leave |

### What the System SHOULD Be (Decision Required)

The implementing agent must resolve these questions with the user BEFORE writing any code:

**Question 1: What should happen with 06b verification verdicts?**

Stage 06b produces verdicts: APPROVE, FLAG, REJECT. Currently the pipeline:
- Quarantines REJECT videos (via post-06b hook) ✓
- Blocks FLAG videos from Stage 07 processing (via hardcoded gate) ← stale behavior
- The user removed the config for this but the code still enforces it

Options:
- **A) REJECT = quarantine, FLAG/APPROVE = process normally** — FLAG is informational metadata only, doesn't block processing. The gate check in Stage 07 and the gate_policy_violations counter in validate_manifest.py get removed.
- **B) Keep APPROVE-only gate but make sub-batch-pipeline pass `--skip-verification`** — effectively the same as A but messier.
- **C) Something else** — user defines.

**Question 2: Should 06e repairs be applied during normal Stage 07 processing?**

Currently `process_video_file()` loads 06e quality data but only stamps it as metadata. The LLM sees the ORIGINAL damaged text. Repair application (replace/delete/merge) only happens in the orphaned `--revalidate` path.

Options:
- **A) Always apply repairs in `process_video_file()`** — move the repair block from `revalidate_video_file()` into `process_video_file()`, right after the 06e quality data load (line ~4507). The LLM then sees repaired text. Keep `--revalidate` as a way to re-apply repairs on existing output without re-calling the LLM.
- **B) Apply repairs in a new deterministic stage between 06e and 07** — e.g., "06e2.DET.apply-repairs" that modifies the conversations.json. Stage 07 then reads already-repaired text.
- **C) Keep as-is (repairs are metadata only)** — explicitly document that 06e repair_text is advisory, never applied.

**Question 3: What happens to `--revalidate`?**

If repairs are always applied in the normal flow (Q2-A), `--revalidate` becomes "re-run normalization on existing output without LLM call." Its repair code would be shared with `process_video_file()` via a common function. Should it:
- Stay as a useful offline tool
- Be removed (no longer needed if repairs happen on first run)

### Implementation Steps (After User Answers)

#### Step 0a: Remove stale gate enforcement from Stage 07

Based on Q1 answer. If A (most likely):

In `07.LLM.content`:
- Remove or simplify `check_verification_gate()` — FLAG should not block
- Remove `--skip-verification` flag (no longer needed if gate is gone)
- Keep `--verification-root` for informational logging only (or remove entirely)
- Update all 5 code paths that call `check_verification_gate()` (lines ~5449, ~5739, ~5835, ~5915, ~6022)

#### Step 0b: Remove stale gate tracking from validate_manifest.py

In `validate_manifest.py`:
- Remove or redefine `_stage07_gate_decision()` (line 123)
- Remove `gate_policy_violations` counter and all references
- Remove or repurpose `verification_gate` section in output JSON
- The `error:stage07_gate_policy_violation` check type goes away

#### Step 0c: Move repair application into normal Stage 07 flow

Based on Q2 answer. If A (most likely):

In `07.LLM.content`:
1. Extract the repair application block (lines 4881-4956 in `revalidate_video_file()`) into a standalone function:
   ```python
   def apply_06e_repairs(
       transcript_artifacts: List[Dict],
       seg_by_id: Dict[int, Dict],
       threshold: float = REPAIR_ACCEPT_THRESHOLD,
   ) -> Tuple[int, int]:  # (accepted, rejected)
   ```

2. Call it in `process_video_file()` right after line 4507 (where 06e data is loaded):
   ```python
   if transcript_artifacts:
       repairs_accepted, repairs_rejected = apply_06e_repairs(
           transcript_artifacts, seg_by_id
       )
       print(f"{LOG_PREFIX}   Repairs: {repairs_accepted} applied, {repairs_rejected} rejected")
   ```

3. Update `revalidate_video_file()` to call the same shared function.

4. Remove the "Legacy: ... kept for revalidate compat" comment (line 266) — no longer legacy.

#### Step 0d: Clean up dead code and terminology

- Remove `06b.LLM.reverify` deletion: `git add scripts/training-data/06b.LLM.reverify` (commit the deletion)
- Remove `--stage07-gate-policy` from `validate_manifest.py` argparse if still present
- Grep for "reverify" across entire codebase, remove non-historical references
- Grep for "revalidate compat" / "legacy" comments that reference the old system

#### Step 0e: Update all documentation

- `docs/pipeline/ASCII`: Update Stage 07 box description — mention repair application
- `docs/pipeline/audits/pipeline_validation_runbook.md`: Stage 07 note should say "applies 06e repairs + produces enrichments"
- `docs/pipeline/validation_harness.md`: Remove any references to gate policies

#### Step 0f: Verify the fix

Run the pipeline on CANARY.1 to confirm:
1. FLAG videos are processed (not blocked) ← new behavior
2. 06e repair_text is applied to segment text before LLM enrichment
3. Validation passes without gate_policy_violation errors
4. `--revalidate` still works as an offline re-normalization tool

**Files to edit:**
- `scripts/training-data/07.LLM.content`
- `scripts/training-data/validation/validate_manifest.py`
- `scripts/training-data/batch/sub-batch-pipeline` (commit existing changes)
- `scripts/training-data/batch/pipeline.config.json` (commit existing changes)
- `docs/pipeline/ASCII`
- `docs/pipeline/audits/pipeline_validation_runbook.md`
- `docs/pipeline/validation_harness.md`

---

## Milestone 1: Fix ASCII Diagram Data Flow

**Problem:** The ASCII in `docs/pipeline/ASCII` shows a strictly linear chain `06 → 06b → 06c → 06d → 06e → 06f → 06g → 06h → 07`, implying Stage 07 depends on 06h as its primary input. This is wrong.

**Actual data flow:**
```
06 → 06b → 06c ──────────────────────────────────→ 07 (primary input)
                  \                                  ↑
                   → 06d → 06e ──────────────────→ 07 (quality overlay + repairs from 06e)
                             \
                              → 06f → 06g → 06h   (quality sidecar, NOT input to 07)
```

Stage 07 reads:
- Primary: `data/06c.DET.patched/` (via `--input-root`, default)
- Quality + repairs: `data/06e.LLM.quality-check/` (via `--quality-root`)
- Anchor IDs from 06h: opportunistic, not required

**Deliverables:**
1. Redraw the ASCII diagram to show the fork after 06c:
   - Main path: `06c → 07 → 08 → 09 → 10`
   - Quality sidecar: `06c → 06d → 06e → 06f → 06g → 06h` (parallel branch)
   - Show 06e feeding into 07 as a dotted/secondary input arrow (repairs + quality)
   - Show 06h feeding anchor IDs to 07 as an optional dotted arrow
2. Add a legend entry explaining the fork
3. Update Stage 07 box to mention repair application (after Milestone 0)
4. Keep all other box descriptions intact — only the arrows/flow change

**Files to edit:** `docs/pipeline/ASCII`

---

## Milestone 2: Add Parallelism to LLM Stages

**Problem:** Runbook line 98 says "All LLM stages must be run in parallel of 5." No script implements this. The sub-batch-pipeline runs stages sequentially.

**Scope:** Add `--parallel N` support to LLM stages: 06, 06b, 06e, 06g, 07.

**How it currently works:** Each LLM script iterates over video files sequentially. For each video, it invokes Claude CLI (`claude` binary) to process one file, waits for completion, then moves to the next.

**Implementation approach:**

### Step 2a: Add `--parallel N` argument to each LLM script

Each of the 5 LLM scripts (06, 06b, 06e, 06g, 07) follows the same pattern:
- Parse args → discover input files → loop over files → call Claude CLI → write output

Add to each script's argparse:
```python
parser.add_argument("--parallel", type=int, default=1,
    help="Max concurrent LLM calls (default: 1)")
```

Replace the sequential for-loop with a `concurrent.futures.ThreadPoolExecutor` (or `ProcessPoolExecutor`) bounded by `--parallel`. Each worker processes one video file independently (they already write to separate output files, so there are no write conflicts).

**Per-script changes:**

| Script | Current loop location | Worker function to extract |
|--------|----------------------|---------------------------|
| `06.LLM.video-type` | Find the main for-loop over input files | Extract single-video processing into `process_one_video()` |
| `06b.LLM.verify` | Same pattern | Same approach |
| `06e.LLM.quality-check` | Same pattern | Same approach |
| `06g.LLM.damage-adjudicator` | Same pattern (but N calls per video for seeds) | Keep per-video sequential seed processing, parallelize across videos |
| `07.LLM.content` | Same pattern | Same approach |

Error handling: If any worker fails, log the error and continue (don't abort the whole batch). Collect failed video IDs and print a summary at the end. Exit with non-zero if any failures.

### Step 2b: Wire `--parallel` through sub-batch-pipeline

In `scripts/training-data/batch/sub-batch-pipeline`:

1. Add a config key to `pipeline.config.json`:
```json
{
  "execution": {
    "parallel_llm_calls": 5
  }
}
```

2. In `load_pipeline_config()`, read `CFG_PARALLEL_LLM` from config.

3. In `run_stage()`, for LLM stages (06, 06b, 06e, 06g, 07), append `--parallel "$CFG_PARALLEL_LLM"` to `stage_args`.

### Step 2c: Update documentation

- Update runbook to document the `--parallel` flag and config key
- Update ASCII diagram to note parallelism capability

**Files to edit:**
- `scripts/training-data/06.LLM.video-type`
- `scripts/training-data/06b.LLM.verify`
- `scripts/training-data/06e.LLM.quality-check`
- `scripts/training-data/06g.LLM.damage-adjudicator`
- `scripts/training-data/07.LLM.content`
- `scripts/training-data/batch/sub-batch-pipeline`
- `scripts/training-data/batch/pipeline.config.json`
- `docs/pipeline/audits/pipeline_validation_runbook.md`
- `docs/pipeline/ASCII`

---

## Milestone 3: Add Quarantine Support to Stage 09

**Problem:** The sub-batch-pipeline only passes `--quarantine-file` to Stage 07. Stage 09 (`09.EXT.chunk-embed.ts`) has no quarantine support. Quarantined videos get chunked and embedded unnecessarily, wasting Ollama GPU compute.

**What quarantine means in this pipeline:**

Quarantine is a per-sub-batch mechanism that marks individual videos as "failed quality" so downstream stages skip them. The quarantine file lives at `data/validation/quarantine/<sub_batch_id>.json` and has this structure:

```json
{
  "quarantine_level": "error",
  "quarantined_video_count": 2,
  "quarantined_video_ids": ["abc123defgh", "xyz789abcde"],
  "videos": [
    {
      "video_id": "abc123defgh",
      "checks": ["stage06b_reject", "misattribution_high"],
      "quarantined_at": "2026-02-16T..."
    }
  ]
}
```

Videos get quarantined at these points during a pipeline run:
1. **After 06b**: `REJECT` verdicts → `quarantine_updater.py` adds to quarantine file
2. **After 07**: cross-stage validation errors → quarantine_updater adds more
3. **After 08**: taxonomy gate failures (from stage 08 report) → quarantine_updater adds more
4. **After 09**: chunk integrity errors → quarantine_updater adds more (but too late — compute already spent)

**The gap:** Between points 2-3 (quarantine grows) and Stage 09 running, quarantined videos exist in the quarantine file but Stage 09 doesn't read it. Concretely:

- Video X passes 06b (APPROVE) and gets enriched in 07
- Post-07 cross-stage validation finds evidence mismatches → quarantines video X
- Stage 08 runs, may quarantine more videos
- Stage 09 runs — sees video X's `*.enriched.json` in `data/07.LLM.content/` and processes it
- Ollama generates embeddings for all chunks (GPU compute, ~5-15 seconds per video)
- Stage 10 would eventually block it at ingest via readiness gates, but the embedding work was wasted

**How many videos does this affect?** In practice, the quarantine rate is low (typically 0-2 per sub-batch of 10). But at scale (150 batches x 10 sub-batches x 10 videos = 15,000 videos), even 1-2% quarantine means 150-300 wasted embedding runs.

**Implementation:**

### Step 3a: Add `--quarantine-file` to `09.EXT.chunk-embed.ts`

The script already has a manifest-scoped processing loop. Add:

```typescript
// New CLI arg
if (arg === "--quarantine-file" && argv[i + 1]) {
  quarantineFile = argv[++i];
}

// Load quarantine IDs (same multi-format reader as 07.LLM.content)
function loadQuarantineIds(filePath: string): Set<string> {
  const raw = JSON.parse(fs.readFileSync(filePath, "utf-8"));
  const ids = new Set<string>();
  for (const key of ["quarantined_video_ids", "video_ids"]) {
    const arr = raw[key];
    if (Array.isArray(arr)) arr.forEach((id: string) => ids.add(id));
  }
  const videos = raw["videos"];
  if (Array.isArray(videos)) {
    videos.forEach((v: any) => {
      if (typeof v === "string") ids.add(v);
      else if (v?.video_id) ids.add(v.video_id);
    });
  }
  return ids;
}
```

In the main processing loop, before processing each video's enriched JSON:
```typescript
const videoId = extractVideoId(filePath);
if (quarantineIds.has(videoId)) {
  console.log(`[09.EXT.chunk-embed] SKIP: ${filePath} (quarantined)`);
  skippedQuarantine++;
  continue;
}
```

### Step 3b: Wire quarantine through sub-batch-pipeline

In `run_stage()` (around line 439), extend the quarantine-passing logic:

```bash
# Currently: only stage 07 gets quarantine
# Change to: stages 07 AND 09 get quarantine
if [[ "$stage" == "07" || "$stage" == "09" ]]; then
  quarantine_file="$RUN_QUARANTINE_FILE"
  if [[ -z "$quarantine_file" ]]; then
    if [[ -f "$auto_quarantine" ]]; then
      quarantine_file="$auto_quarantine"
    fi
  fi
fi
```

And extend the arg-passing block similarly (currently gated on `"$stage" == "07"`).

**Files to edit:**
- `scripts/training-data/09.EXT.chunk-embed.ts`
- `scripts/training-data/batch/sub-batch-pipeline`

---

## Milestone 4: Integrate DET.split-manifest into Pipeline

**Problem:** The runbook documents split manifests (infield vs non_infield paths), and `DET.split-manifest` exists as a standalone tool, but the sub-batch-pipeline never calls it. All videos go through all stages regardless of type.

**What the split enables:** Non-infield videos (talking_head, podcast — 0 approach conversations) can skip 5 stages:

| Stage | Type | Skip for non-infield? | Why |
|-------|------|----------------------|-----|
| 06b.LLM.verify | LLM | YES | Verifies conversation boundaries/speaker roles — no conversations to verify |
| 06c.DET.patch | DET | YES | Patches from 06b. No 06b = nothing to patch |
| 06d.DET.sanitize | DET | YES | Evidence allowlists for conversations. No conversations |
| 06g.LLM.damage-adjudicator | LLM | YES | Adjudicates segment damage in conversation context. No conversations |
| 06h.DET.confidence-propagation | DET | YES | Propagates 06g scores. No 06g = nothing to propagate |

**Stages that still run for non-infield:**
| Stage | Why |
|-------|-----|
| 06e.LLM.quality-check | ASR artifact detection is video-type agnostic |
| 06f.DET.damage-map | Reads 06e quality data — still useful without 06d |
| 07.LLM.content | Runs with talking_head/podcast prompt variant |
| 08-09 | Taxonomy validation and chunking still apply |

**Net savings:** 2 LLM calls + 3 DET stages per non-infield video.

**Implementation:**

### Step 4a: Call DET.split-manifest after Stage 06 in pipeline

In `run_pipeline()`, after Stage 06 completes successfully, call DET.split-manifest:

```bash
# After stage 06 completes
if [[ "$stage" == "06" ]]; then
  echo "--- Splitting manifest by video type ---"
  python3 "$REPO_ROOT/scripts/training-data/DET.split-manifest" \
    --manifest "$manifest"
  # Now we have ${manifest%.txt}.infield.txt and ${manifest%.txt}.non_infield.txt
fi
```

### Step 4b: Define separate stage orders

```bash
INFIELD_STAGE_ORDER=("06b" "06c" "06d" "06e" "06f" "06g" "06h" "07" "08" "09")
NON_INFIELD_STAGE_ORDER=("06e" "06f" "07" "08" "09")
```

### Step 4c: Fork the pipeline after Stage 06

After split, run remaining stages twice with different manifests and stage orders:
1. Infield manifest → full `INFIELD_STAGE_ORDER`
2. Non-infield manifest → reduced `NON_INFIELD_STAGE_ORDER`

This requires refactoring `run_pipeline()` to accept a stage order parameter.

### Step 4d: Merge validation at end

End-of-run validation should still cover all videos in the original manifest, not just one split. The `validate_sub_batch()` call should use the original (unsplit) manifest.

**Files to edit:**
- `scripts/training-data/batch/sub-batch-pipeline`
- `docs/pipeline/ASCII` (show the fork)
- `docs/pipeline/audits/pipeline_validation_runbook.md` (clarify the split is automated)

---

## Milestone 5: User Decision — Low/Informational Issues

**Action:** Present the following items to the user and ask for direction on each.

### Issue A: 06h output pattern incomplete in pipeline tracking

`STAGE_PATTERNS["06h"]` is `*.confidence.report.json`, but 06h also outputs `*.conversations.json` (with confidence scores merged in). The `--status` and `--view` commands only track the report file.

**Options:**
1. Add a second pattern or change the primary pattern to `*.conversations.json`
2. Leave as-is (the report file is the canonical status indicator)
3. Track both patterns (requires STAGE_PATTERNS to support arrays)

### Issue B: Stages 01-05 arg interface inconsistency

Stage 05 doesn't accept `--manifest` (uses `--sources` or single-file args). Stages 02-04 do accept `--manifest`. This doesn't affect the pipeline (01-05 are outside RUNNABLE_STAGES) but is an inconsistency.

**Options:**
1. Add `--manifest` to Stage 05 for consistency
2. Leave as-is (stages 01-05 are run separately and this works fine)

### Issue C: Runbook parallelism number inconsistency

Runbook says "parallel of 5", MEMORY.md says "10 parallel LLM calls". After Milestone 2 implements the feature, which number should be the default?

**Options:**
1. Default to 5 (conservative, per runbook)
2. Default to 10 (per user preference in MEMORY.md)
3. Make it configurable with no hardcoded default

---

## Execution Order

```
Milestone 0 (gate/repair policy)  — FIRST, blocks everything else
Milestone 1 (ASCII fix)           — after M0 (needs repair decision for Stage 07 box)
Milestone 2 (parallelism)         — after M0 (needs Stage 07 changes settled)
Milestone 3 (Stage 09 quarantine) — after M0 (no direct dependency, but serialize to avoid conflicts)
Milestone 4 (split-manifest)      — after M1 (ASCII needs the fork drawn first)
Milestone 5 (user decisions)      — present to user before implementing any low items
```

Milestone 0 is the critical path. Once it's done, Milestones 1-3 can run in parallel. Milestone 4 follows Milestone 1.
