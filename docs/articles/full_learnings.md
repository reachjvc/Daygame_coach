# Full Learnings: Article Writing Observations

**Purpose:** A living field journal of what the editor has liked and disliked across article drafts. These are observations, not laws. They reflect patterns seen so far and should be updated, softened, or overridden as new feedback comes in.

**For AI:** Read this before writing or revising any article. Treat high-confidence observations seriously. Treat tentative ones as guidance worth testing. If new feedback contradicts something here, update this document — don't force new writing to conform to old patterns.

**How confidence works:**
- **High confidence** — Flagged or praised 3+ times across different drafts/sections. Strong pattern.
- **Medium confidence** — Flagged or praised 1-2 times. Likely real but could be context-dependent.
- **Tentative** — Observed once or inferred. Treat as a hypothesis to test, not a rule to follow.

**Updated:** 2026-02-05
**Source:** Primarily from 10 feedback rounds on the 80/20 field report article (01-8020-rule), Jan-Feb 2026. A small sample — these patterns may evolve.

---

## 1. Alive vs. Dead Writing

The editor's most consistent value. Writing they like, they call "alive." Writing they reject feels "dead."

**Confidence: High**

### What has felt alive so far

Writing that made the editor mark "Excellent" or "Good" had these traits:
- The reader encounters something they didn't already know
- A specific person, study, or finding is cited by name
- A concept from one domain is concretely applied to the reader's situation

**Excellent example — specificity:**
> "Anders Ericsson's research on deliberate practice found the same asymmetry: most time spent 'practicing' generates nothing. The gains cluster in specific moments of struggle. Everything outside those moments is maintenance at best."

Editor: *"This works because it's so specific, and such an interesting fact."*

**Excellent example — cross-domain application:**
> "There's a concept in Brazilian Jiu-Jitsu analysis called studying 'off the move.' When reviewing footage, elite coaches don't focus on the submission itself. They study what happened *before* — the grips, the weight distribution, the small adjustments that made the finish inevitable."

Editor: *"This I like, it's specific and then applied to our situation."*

### What has felt dead so far

Writing flagged as AI or "dead" tended to be generic — something anyone could write without domain knowledge or research:

> "Most of what you write in your notes is useless. Not wrong, not harmful — just useless."

Flagged as AI. No fact, no evidence, no specificity. A claim dressed up as insight.

> "The average field report is 80% filler. Not bad writing — just unnecessary writing."

Editor: *"This is horrible, particularly saying the average field report is 80% filler, and 'not bad writing'. Field reports are never about 'good writing' so this point is totally misplaced."*

### A useful test (not the only one)

One question that has helped: **Could a random person with zero domain knowledge have written this?** If yes, it's probably dead. But there may be other ways to make writing alive that we haven't tested yet — storytelling, vivid analogies, concrete scenes. Specificity is the approach that has worked so far; it's not necessarily the only one.

---

## 2. AI Tells

The editor has a strong sense for when AI wrote something. These are patterns that have been flagged so far.

### Short punchy sentences that add no substance

**Confidence: High** — flagged multiple times across different sections.

> "These aren't arbitrary constraints. They're recognition that depth beats breadth."

Editor: *"These 2 last sentences in the Brian Cain paragraph are so AI. Short sentences that do not really add value or depth or practicality."*

> "Skip the rest. Seriously."

Editor: *"Hate this, particularly the 'seriously' — it's very AI."*

> "Find the 20%. Ignore the rest."

The pattern: sentences that sound like mic-drops but introduce no new information. They feel like AI filler — rhythmically satisfying but empty.

### Openings that try to sound provocative

**Confidence: High** — every provocative-rhetoric opening attempted on the 80/20 article was flagged.

- "Most of what you write in your notes is useless." → AI
- "Eighty percent of a field report teaches you nothing." → Weak
- "The average field report is 80% filler." → Horrible
- "That's not a metaphor." → "No one thinks of this as being a metaphor though"

These failed because they manufactured tension through rhetoric rather than earning it through content. The openings that worked better led with specific research findings.

**Note:** This doesn't mean bold openings can never work. It means *empty* bold openings haven't worked. An opening that leads with a genuinely surprising fact, a vivid story, or a concrete analogy might land differently — we haven't tested those approaches enough yet.

### Known AI phrases

**Confidence: High** — from writing style guide + confirmed in feedback:
- "Let's dive in"
- "It's important to note"
- "In this article we'll explore"
- "Crucial component"
- "In conclusion"
- "Seriously." (standalone emphasis)
- "Let that sink in."
- Announcing structure before delivering content ("In this section, we'll look at...")

### Parallel structure

**Confidence: Medium** — observed but not explicitly flagged by name.

AI tends toward perfectly balanced, parallel constructions. Real writing is messier. If every paragraph follows the same rhythm, it can feel robotic.

---

## 3. Context Awareness

**Confidence: High** — flagged repeatedly.

The editor caught multiple instances where the writing forgot what it was actually describing.

### Domain accuracy matters

> "The few minutes before the tipping point."

Editor: *"A daygame approach is often only a few minutes, so you must always keep in mind what we're actually talking about, otherwise you write something that doesn't make sense in the context of the main point of the article."*

A daygame approach is ~2-5 minutes. Writing "the few minutes before the tipping point" doesn't work when the whole interaction is a few minutes.

> "Not bad writing — just unnecessary writing."

Editor: *"Field reports are never about 'good writing' so this point is totally misplaced."*

Field reports aren't essays. Nobody evaluates their prose quality.

> "Someone posts a 2,000-word report covering fifteen interactions."

Editor: *"Not rooted in reality."* Also: *"People don't approach 10 people a session."*

**Observation:** When the writing fabricates scenarios that don't match how things actually work in the domain, the editor catches it immediately. If unsure about norms (how many approaches per session, how long interactions last), check the research docs or ask.

### Title-content alignment

**Confidence: High** — flagged explicitly.

The 80/20 article was titled "for Learning from Social Interactions" but the content was about field reports.

Editor: *"What is this article trying to be? For field reports? For journaling in general? For all social interactions? Consider who would even write notes about interactions, only that would happen in a field report, which is fine, but then the rest of it needs to look different too."*

The title, thesis, and content should agree on what the article is about.

---

## 4. Practicality

**Confidence: Medium-High** — flagged several times, but we haven't yet seen what "practical enough" looks like in a finished article.

### The gap between "what" and "how"

> "Identify one or two key moments per session. Analyze those deeply. Let the rest go."

Editor: *"Doesn't yet seem practical. We haven't gotten to the nitty gritty of what to actually do."*

> "After a session, don't start writing immediately. Let it settle for ten minutes."

Editor: *"Is this actually true, or something you made up? Is it founded in the research paper?"*

The editor wants practical advice that someone could actually follow, and wants to know the advice is grounded in something real, not invented.

**Observation:** "Find the 20%" is still abstract. What does that look like in practice? What questions does the person ask? What do they write down? We haven't cracked this yet — future drafts need to try different approaches to making the advice concrete.

### Claims need grounding

If the article presents advice as fact ("let it settle for ten minutes"), it should trace to a source. If it's a synthesis or hypothesis, say so. Don't present invention as established fact.

**Confidence: High** — the editor explicitly asked "is this true or something you made up?" about practical advice.

---

## 5. Handling Editor Feedback

### Ideas are not mandates

**Confidence: High** — the editor explicitly flagged this.

The editor casually suggested "could be interesting" to explore Pareto applied recursively (20% of 20% = 4%). Subsequent drafts hardcoded it as a structural element.

Editor: *"You just took a previous comment where I wrote something like 'could be interesting to do the pareto of the pareto' and then you made it a rule that we MUST use that rule, but it was just an idea for what could be interesting, rather than generic."*

When the editor floats an idea, treat it as an option to explore. Present alternatives. Let them choose.

### Match fix scope to flag scope

**Confidence: High** — from writing style guide, consistent with feedback behavior.

- Flag on a **sentence/phrase** → rewrite that sentence, offer 2-3 alternatives
- Flag on a **whole section** → rewrite the section, try a different angle
- Mixed flags → preserve what works, replace what doesn't

Don't restructure a whole section because one sentence was weak.

### Source attribution: verify, don't assume

**Confidence: High** — flagged emphatically.

> "A popular post on Skilled Seducer..."

Editor: *"We MUST confirm that it was actually a popular post. That is relative to the other posts, so are we sure? This is the kind of thing we must must must make sure we don't get wrong, because it's a factual claim."*

Every factual claim — even framing claims like "popular" or "well-known" — needs verification. When in doubt, use neutral framing ("a post on Skilled Seducer discusses...").

---

## 6. Article Structure

### Repetition

**Confidence: High** — the last three sections of the 80/20 article were all flagged for this.

- Objections section: *"Feels like saying the same thing. Not really moving the article along in a logical way."*
- Meta-lesson section: *"Again, nothing really new being said."*

The article made its point by section 3-4, then restated it for 3 more sections. Each section should add genuinely new information.

### Length

**Confidence: Medium** — inferred from the repetition flags.

The 80/20 drafts were ~80 lines. Maybe 40-50 contained actual substance. The rest was repetition and rhetorical padding. This suggests the article should be shorter — but we haven't yet found the right length. A tight article that nails every section is better than a longer one that pads, but we don't have enough data to say what the ideal length is.

### Earning a section's place

Questions that have helped identify weak sections:
1. Does this section introduce information the reader doesn't have yet?
2. Does it advance the argument or restate it?
3. Would the article lose something if this section were deleted?

---

## 7. Voice and Tone

### Third-person, research-backed

**Confidence: High** — from writing style guide, never contradicted.

AI should not write first-person anecdotes ("I stopped writing field reports"). Authority comes from evidence — named researchers, specific findings, concrete data.

### Lecturing

**Confidence: Medium** — flagged indirectly through AI tells.

Phrases like "The question isn't whether you're reflecting. It's whether you're reflecting on the right things" sound authoritative but don't say anything concrete. They're the written equivalent of motivational posters.

The editor has consistently preferred writing that teaches (shows the reader something new) over writing that impresses (sounds confident without substance).

### What we haven't tested

We haven't explored: how much storytelling can work, whether humor lands, how personal examples (from research subjects, not from AI) would feel, what a more conversational tone would look like. The third-person research-backed voice is what's worked so far, but other voices might also work for different article types or tiers.

---

## 8. Research Citation Style

### Named researchers > generic claims

**Confidence: High** — the strongest "Excellent" marks were all on named-researcher citations.

**Works:** "Anders Ericsson's research on deliberate practice found the same asymmetry..."

**Doesn't work:** "Research shows that..." / "Studies consistently find that..."

Naming the researcher + their specific finding signals real knowledge. Generic citations ("research shows") feel hollow.

### Verify before citing

**Confidence: High** — editor was emphatic about this.

Every cited claim must be traceable to the actual source. Don't generalize beyond what the study showed. Don't attribute findings to the wrong researcher. If a source is a forum post, call it a forum post.

### Interesting facts as raw material

**Confidence: Medium-High** — consistent across all drafts.

The editor's favorite parts were always specific research findings. The article's job seems to be: find genuinely interesting facts and present them in the reader's context. The research is raw material; the article is curation. But this is one approach — other articles might work differently (e.g., narrative-driven, analogy-driven).

---

## 9. Examples

### Fabricated scenarios get caught

**Confidence: High** — flagged multiple times.

> "I opened with X, she said Y, I responded with Z..."

Editor: *"This is quite dead. We would probably want a better, more specific example."*

Generic placeholder examples feel fake. If the article needs an example, it should either come from actual research/field reports, be specific enough to feel real, or be acknowledged as hypothetical.

### Domain-accurate examples

**Confidence: High** — overlaps with Context Awareness (Section 3).

Examples must reflect how things actually work. If approaches are 2-5 minutes, don't write examples assuming 20-minute conversations. If field reports are rough notes, don't write examples assuming polished prose.

---

## 10. Pre-Submission Checklist

These are the checks that have caught problems in previous drafts. Not exhaustive — new checks may emerge.

- [ ] Does each section have something specific (a fact, finding, or concrete example)?
- [ ] Could a person with no domain knowledge have written any of this? (Rewrite those parts)
- [ ] Are all factual claims (including qualifiers like "popular") verified?
- [ ] Does each section add something new, or is it repeating a previous point?
- [ ] Any short punchy sentences that add no information? (Probably cut)
- [ ] Do examples match domain reality?
- [ ] Is practical advice specific enough to follow?
- [ ] Any known AI phrases? (Check Section 2)
- [ ] Does the title match what the article actually covers?

---

## Appendix: Observed Traps (Quick Reference)

These are patterns that have caused problems. The "fix" column shows what has worked or what the editor suggested — other fixes might also work.

| Trap | Example | What worked / was suggested |
|------|---------|---------------------------|
| Generic opening | "Most of what you write is useless" | In this article, leading with a specific finding worked better. Other approaches (story, analogy) untested. |
| Empty mic-drop | "Seriously." / "Ignore the rest." | Cutting the sentence. If it adds no info, it's filler. |
| Domain ignorance | "The few minutes before the tipping point" | Check domain norms. Approaches are short. |
| Unverified qualifier | "A popular post on..." | Verify or use neutral framing. |
| Pseudo-practicality | "Focus on the 20%" | Show HOW concretely. We haven't nailed this yet. |
| Idea → mandate | Editor's idea → hardcoded in next draft | Present as option with alternatives. |
| Repetition | Three sections saying "depth > breadth" | One section, then move on to new ground. |
| Rhetorical authority | "The question isn't X. It's Y." | Earn authority through evidence. Tone alone doesn't work. |
| Placeholder examples | "I said X, she said Y" | Use real or realistically specific examples. |
| Generic citation | "Research shows..." | Name the researcher and finding. |

---

## Open Questions (Things We Haven't Resolved)

These are areas where we don't have enough data yet:

- **Storytelling as an opening approach** — Could a vivid analogy or narrative scene work as an opening instead of a research fact? Untested.
- **Humor** — How much, if any? No data yet.
- **Optimal article length** — We know padding is bad, but we don't know what the right length looks like.
- **How personal can examples get** — Using real (anonymized) field report excerpts vs. constructed examples.
- **Citation style** — Inline vs. footnotes. No preference expressed yet.
- **How much narrative vs. direct exposition** — No data yet.
- **Different tiers, different rules?** — A flagship long-form piece might have different needs than a quick takeaway article. Untested.
