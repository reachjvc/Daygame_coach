#!/usr/bin/env python3
"""
scripts/training-data/06b.verify

LLM Verification of Stage 06 (Video Type Classification) Output

Pure LLM quality gate — reads stage 06 conversations.json, asks an LLM to
critically evaluate speaker roles, conversation boundaries, and structural
coherence. Writes a verification report. Does NOT modify stage 06 output.

Reads:
  - Stage 06 output files:
      data/06.video-type/<source>/<video>/*.conversations.json

Writes:
  - Verification reports:
      data/06b.verify/<source>/<video>/*.verification.json

Use:

  A) From pipeline:
     ./scripts/training-data/06b.verify "source_name" "youtube_url"

  B) Test videos:
     ./scripts/training-data/06b.verify --test

  C) Single file:
     ./scripts/training-data/06b.verify --input data/06.video-type/video.conversations.json

  D) Batch from sources file:
     ./scripts/training-data/06b.verify --sources

Requirements:
  - Claude Code CLI installed and authenticated (claude command available)
"""

from __future__ import annotations

import argparse
import json
import re
import shlex
import subprocess
import sys
import time
from dataclasses import dataclass, asdict
from pathlib import Path
from typing import Any, Dict, List, Optional, Set, Tuple

import jsonschema

from batch.manifest_parser import load_manifest, load_manifest_sources, manifest_filter_files


# ---------------------------
# Configuration
# ---------------------------

PIPELINE_VERSION = "06b.verify-v1.5"  # v1.5: Remove non-schema role values (e.g., "mixed") from patch suggestions

DEBUG_MODE = False

CLAUDE_BINARY_PATHS = [
    "claude",
    Path.home() / ".vscode-server/extensions/anthropic.claude-code-2.1.17-linux-x64/resources/native-binary/claude",
    Path.home() / ".vscode/extensions/anthropic.claude-code-2.1.17-linux-x64/resources/native-binary/claude",
    "/usr/local/bin/claude",
]

LOG_PREFIX = "[06b.verify]"

# Contract enforcement: verification reports must match verification.schema.json.
SCHEMA_PATH = Path(__file__).resolve().parent / "schemas" / "verification.schema.json"
_CACHED_SCHEMA: Optional[Dict[str, Any]] = None

# Failure budget: halt batch if too many failures
MAX_CONSECUTIVE_FAILURES = 3
MAX_FAILURE_RATE = 0.20  # halt if >20% of videos fail


def _load_schema() -> Optional[Dict[str, Any]]:
    if not SCHEMA_PATH.exists():
        print(f"{LOG_PREFIX} WARNING: Schema not found at {SCHEMA_PATH}")
        return None
    return json.loads(SCHEMA_PATH.read_text(encoding="utf-8"))


def _get_schema() -> Optional[Dict[str, Any]]:
    global _CACHED_SCHEMA
    if _CACHED_SCHEMA is None:
        _CACHED_SCHEMA = _load_schema()
    return _CACHED_SCHEMA


def _validate_verification_schema(output_data: Dict[str, Any]) -> None:
    """Validate verification output against verification.schema.json. Raises on failure."""
    schema = _get_schema()
    if not schema:
        return
    try:
        jsonschema.validate(instance=output_data, schema=schema)
    except jsonschema.ValidationError as e:
        path = ".".join(str(p) for p in e.absolute_path) if e.absolute_path else "(root)"
        raise RuntimeError(f"Verification output schema invalid at {path}: {e.message[:200]}")


# ---------------------------
# Prompt
# ---------------------------

VERIFY_PROMPT = """You are a quality reviewer for a daygame coaching video analysis pipeline. Stage 06 classified this video's type, assigned speaker roles, and detected conversation boundaries. Your job is to CRITICALLY evaluate whether the output is correct.

You must be skeptical. Look for mistakes. The classification LLM sometimes makes errors — your job is to catch them.

Perform these 5 verification passes:

---

PASS 1: ROLE COHERENCE

For each conversation, read the dialogue sequentially like a script. Check:

A) Does the person labeled "coach" behave like a coach throughout?
   - Opens approaches (excuse me, hi, I just saw you...)
   - Leads the conversation (asks questions, gives compliments, cold reads)
   - Makes camera-facing comments between approaches
   - Introduces himself by name

B) Does the person labeled "target" behave like a target throughout?
   - Responds to questions (short answers, especially early on)
   - Gives personal info when asked (name, origin, occupation)
   - Reacts (laughing, surprise, hesitation)
   - Shorter turns than coach, especially at start

C) CRITICAL CHECK — Self-introductions:
   If ANY segment contains "I'm [name]", "my name is [name]", "I'm called [name]", or similar:
   - Who is introducing themselves?
   - Is the role assignment correct for that person?
   - In a daygame approach, when the coach asks "what's your name?" and then someone says "I'm [name]", that person is typically giving THEIR OWN name. But if the name matches the video title's coach name, it's the COACH introducing himself.

D) Are there any segments where the assigned role clearly contradicts the text?
   - A "target" segment that sounds like camera commentary
   - A "coach" segment that sounds like a shy response to a question
   - A "commentary" segment that's clearly part of a live conversation

---

PASS 2: CONVERSATION BOUNDARIES

For each conversation:

A) Does it start naturally?
   - A new greeting, opener, or direct address to a new person
   - NOT a continuation of a previous conversation

B) Does it end naturally?
   - Goodbye, number exchange, rejection, or pivot back to camera
   - NOT mid-conversation

C) Time gaps within the conversation:
   - Are there gaps >30 seconds between consecutive segments?
   - If so, is there a plausible explanation? (e.g., exchanging phone numbers = silence while typing)
   - Or does the gap suggest a MISSED conversation boundary?

D) Duration:
   - Very long conversations (>5 min): Is there sustained back-and-forth dialogue throughout, or does it seem like two approaches merged into one?
   - Very short conversations (<15 sec): Is this a real interaction (quick rejection, brief exchange) or a fragment that was wrongly split off?

E) Cold-open detection:
   - Does the FIRST conversation look like it might be a preview/teaser clip that actually comes from later in the video? (Common YouTube pattern: show an exciting moment before the intro)
   - If so, does the text overlap with any later conversation?

---

PASS 3: COMMENTARY VERIFICATION

For all segments with conversation_id = 0 (commentary):

A) Do they sound like camera-facing speech?
   - Instructional tone, addressing "guys", "gentlemen", "everyone"
   - Analyzing a previous approach, setting up the next one
   - Intro/outro of the video

B) Are there any commentary segments that sound like they belong in an approach?
   - Direct dialogue with another person
   - Questions expecting a response
   - Personal responses to questions

---

PASS 4: COLLAPSED SPEAKER VERIFICATION

If speaker_collapse.detected is true:

For each collapsed speaker, review a sample of their segments:

A) Do segments overridden as "target" actually sound like target speech?
   - Answering questions, giving short responses, reacting

B) Do segments overridden as "coach" actually sound like coach speech?
   - Opening, leading, questioning, complimenting

C) Do segments overridden as "other" make sense as bystander speech?

D) Are there any overrides that seem wrong?

---

PASS 5: STRUCTURAL COHERENCE

A) Video type: Does the classified type match the content?
   - "compilation" should have multiple approaches with commentary between them
   - "infield" should be mostly approaches
   - "talking_head" should be a monologue
   - "podcast" should be multi-speaker discussion

B) Conversations with zero target segments:
   - Is there a visible explanation in the text? (coach talking at someone who doesn't respond, headphones, immediate rejection)
   - Or are target segments possibly misclassified?

C) Overall: Does the output tell a coherent story of what happened in the video?

---

CONFIDENCE CALIBRATION:

For EVERY issue you flag, rate your confidence (0.0-1.0) that your suggested fix is correct:

- 0.95+: Unambiguous. Clear textual evidence, no alternative interpretation possible.
- 0.85-0.94: Very likely. Strong evidence, only minor ambiguity.
- 0.70-0.84: Probable. Evidence supports this but meaningful uncertainty exists.
- <0.70: Uncertain. Flag for human review but don't suggest a fix (set suggested_* to null).

Be CALIBRATED: if you say 0.90, you should be wrong ~10% of the time, not 50%.

---

VIDEO TITLE: "{title}"

VIDEO TYPE: {video_type} (confidence: {video_type_confidence})

SPEAKER LABELS:
{speaker_labels_text}

SPEAKER COLLAPSE: {collapse_info}

CONVERSATIONS SUMMARY:
{conversations_summary}

FULL TRANSCRIPT ({segment_count} segments):
{transcript}

---

OUTPUT: First, write a brief analysis for each of the 5 passes (2-4 sentences each). Then output a JSON object in a ```json code block.

CRITICAL SCHEMA REQUIREMENTS (06c.patch auto-applies fixes based on these fields):

1. video_type_check: If agrees=false, you MUST include:
   - "suggested_type": one of "infield", "compilation", "talking_head", "podcast" (NOT prose)
   - "confidence": 0.0-1.0

2. misattributions: Each entry MUST include:
   - "suggested_role": one of "coach", "target", "other", "voiceover" (NOT prose like "should be coach")
   - "confidence": 0.0-1.0

3. boundary_issues: Each entry MUST include:
   - "suggested_fix": one of "merge_with_next", "merge_with_previous", "split_at_segment_N", "reclassify_as_commentary", or null (NOT prose)
   - "confidence": 0.0-1.0
   - For split fixes, N must be a real segment ID (e.g., "split_at_segment_29")

4. collapse_issues: Each entry MUST include:
   - "current_override": the actual current value (use null if no override exists, NOT "none" or prose)
   - "suggested_override": one of "coach", "target", "other" (NOT prose like "should be SPEAKER_02")
   - "confidence": 0.0-1.0

EXAMPLE OUTPUT:

```json
{{
  "verdict": "FLAG",
  "video_type_check": {{
    "agrees": false,
    "suggested_type": "talking_head",
    "confidence": 0.92,
    "reasoning": "95% single speaker, not enough back-and-forth for podcast"
  }},
  "conversation_verdicts": [{{"conversation_id": 1, "verdict": "FLAG", "notes": "cold-open teaser duplicates later content"}}],
  "misattributions": [{{
    "segment_id": 490,
    "current_role": "target",
    "suggested_role": "coach",
    "confidence": 0.95,
    "evidence": "Says 'Hey, I'm James' — James is the coach per video title"
  }}],
  "boundary_issues": [{{
    "conversation_id": 1,
    "issue": "Segments 0-15 are cold-open teaser that duplicates content from later",
    "severity": "moderate",
    "suggested_fix": "split_at_segment_29",
    "confidence": 0.90
  }}],
  "collapse_issues": [{{
    "speaker_id": "SPEAKER_01",
    "segment_id": 120,
    "current_override": null,
    "suggested_override": "target",
    "confidence": 0.88,
    "evidence": "Segment is during Conv 2, should be Conv 2's target not Conv 1's"
  }}],
  "other_flags": ["Transcription error at seg 11: target name transcribed as 'James' (coach's name)"],
  "summary": "Video type misclassified as podcast, should be talking_head. One misattribution and boundary issue detected."
}}
```

VERDICT RULES:
- APPROVE: No errors found. Minor observations are fine.
- FLAG: Issues found that should be reviewed but aren't clearly wrong. E.g., a suspicious role assignment where you're >50% sure it's wrong, or a boundary that seems off but could be correct.
- REJECT: Clear errors that will corrupt downstream processing. E.g., coach and target roles are swapped for an entire conversation, or a conversation boundary is clearly wrong (two separate approaches merged).

Be specific. Cite segment IDs and quote text when flagging issues. Empty arrays are fine if no issues found for a category.

CRITICAL: When you identify multiple segments with the same issue (e.g., "segments 0, 5, 25, 118 are all coach audio on wrong speaker"), create a SEPARATE misattribution or collapse_issue entry for EACH segment. Do NOT summarize multiple fixable issues into one other_flags string. The other_flags array is ONLY for issues that cannot be expressed as individual segment fixes.
"""

# Conversion prompt for when analysis is good but JSON parsing fails
CONVERSION_PROMPT = """You previously analyzed a video verification task and produced this analysis:

---
{analysis}
---

Your analysis was thorough and correct. However, the JSON output was missing or malformed.

Please convert your analysis above into a valid JSON object. Output ONLY the JSON, no other text.

CRITICAL SCHEMA REQUIREMENTS (06c.patch auto-applies fixes based on these fields):

1. video_type_check: If agrees=false, you MUST include:
   - "suggested_type": one of "infield", "compilation", "talking_head", "podcast" (NOT prose)
   - "confidence": 0.0-1.0

2. misattributions: Each entry MUST include:
   - "suggested_role": one of "coach", "target", "other", "voiceover" (NOT prose)
   - "confidence": 0.0-1.0

3. boundary_issues: Each entry MUST include:
   - "suggested_fix": one of "merge_with_next", "merge_with_previous", "split_at_segment_N", "reclassify_as_commentary", or null
   - "confidence": 0.0-1.0
   - For split fixes, N must be a real segment ID (e.g., "split_at_segment_29")

4. collapse_issues: Each entry MUST include:
   - "current_override": the actual current value (use null if no override, NOT "none" or prose)
   - "suggested_override": one of "coach", "target", "other" (NOT prose)
   - "confidence": 0.0-1.0

Required schema:
{{
  "verdict": "APPROVE|FLAG|REJECT",
  "video_type_check": {{
    "agrees": true|false,
    "suggested_type": "infield|compilation|talking_head|podcast",
    "confidence": 0.92,
    "reasoning": "brief explanation"
  }},
  "conversation_verdicts": [{{"conversation_id": 1, "verdict": "OK|FLAG|ISSUE", "notes": "brief"}}],
  "misattributions": [{{
    "segment_id": 123,
    "current_role": "target",
    "suggested_role": "coach",
    "confidence": 0.88,
    "evidence": "why this is wrong"
  }}],
  "boundary_issues": [{{
    "conversation_id": 1,
    "issue": "description",
    "severity": "minor|moderate|major",
    "suggested_fix": "split_at_segment_29",
    "confidence": 0.90
  }}],
  "collapse_issues": [{{
    "speaker_id": "SPEAKER_01",
    "segment_id": 123,
    "current_override": null,
    "suggested_override": "target",
    "confidence": 0.88,
    "evidence": "why"
  }}],
  "other_flags": ["any other concerns as strings"],
  "summary": "one paragraph overall assessment"
}}

Rules:
- Extract ALL issues mentioned in your analysis
- Use empty arrays [] for categories with no issues
- Include segment IDs and evidence from your analysis
- Include confidence scores (0.0-1.0) for each issue based on your certainty
- For video_type_check: include suggested_type only if agrees=false, use null if agrees=true
- For boundary_issues: include suggested_fix as exact value, not prose
- For collapse_issues: use null for current_override if segment has no override, not "none" or "none (raw label)"
- The verdict should match what you concluded (FLAG if issues found, APPROVE if none, REJECT if critical errors)
- CRITICAL: Create a SEPARATE entry for EACH affected segment. Do NOT summarize multiple segment issues into one other_flags string.

Output the JSON now:
"""


# ---------------------------
# State Management
# ---------------------------

@dataclass
class ProcessingState:
    version: int
    completed_files: List[str]
    in_progress: Optional[str]
    failures: List[Dict[str, str]]


def load_state(state_path: Path) -> ProcessingState:
    if state_path.exists():
        try:
            data = json.loads(state_path.read_text())
            return ProcessingState(
                version=data.get("version", 1),
                completed_files=data.get("completed_files", []),
                in_progress=data.get("in_progress"),
                failures=data.get("failures", []),
            )
        except (json.JSONDecodeError, KeyError):
            pass
    return ProcessingState(version=1, completed_files=[], in_progress=None, failures=[])


def save_state(state_path: Path, state: ProcessingState) -> None:
    state_path.parent.mkdir(parents=True, exist_ok=True)
    state_path.write_text(json.dumps(asdict(state), indent=2))


# ---------------------------
# Claude CLI Interface
# ---------------------------

def find_claude_binary() -> Optional[str]:
    for path in CLAUDE_BINARY_PATHS:
        path = Path(path)
        if path.exists() and path.is_file():
            return str(path)
        if str(path) == "claude":
            try:
                result = subprocess.run(["which", "claude"], capture_output=True, text=True)
                if result.returncode == 0:
                    return "claude"
            except Exception:
                pass
    return None


def call_claude(prompt: str, retries: int = 3, timeout: int = 300) -> Optional[str]:
    claude_bin = find_claude_binary()
    if not claude_bin:
        raise RuntimeError("Claude CLI binary not found — cannot proceed")

    for attempt in range(retries):
        try:
            result = subprocess.run(
                [claude_bin, "-p", prompt, "--output-format", "text"],
                capture_output=True,
                text=True,
                timeout=timeout,
            )
            if result.returncode == 0:
                return result.stdout.strip()
            else:
                if attempt < retries - 1:
                    wait = 2 ** attempt
                    print(f"{LOG_PREFIX} Claude CLI error, retrying in {wait}s...")
                    print(f"{LOG_PREFIX}   stderr: {result.stderr[:200]}")
                    time.sleep(wait)
                    continue
                raise RuntimeError(f"Claude CLI failed: {result.stderr[:500]}")
        except subprocess.TimeoutExpired:
            if attempt < retries - 1:
                print(f"{LOG_PREFIX} Timeout, retrying...")
                time.sleep(2 ** attempt)
                continue
            raise RuntimeError(f"Claude CLI timeout after {timeout}s")
        except FileNotFoundError:
            raise RuntimeError("'claude' command not found. Install Claude Code CLI.")
    return None


def parse_json_response(response: str) -> Optional[Dict]:
    if not response:
        if DEBUG_MODE:
            print(f"{LOG_PREFIX} DEBUG: Response is empty/None")
        return None

    if DEBUG_MODE:
        print(f"{LOG_PREFIX} DEBUG: Response length: {len(response)} chars")
        print(f"{LOG_PREFIX} DEBUG: Response preview:\n{response[:1000]}")
        print(f"{LOG_PREFIX} DEBUG: Response end:\n...{response[-500:]}")

    try:
        code_block_match = re.search(r"```(?:json)?\s*(\{[\s\S]*?\})\s*```", response)
        if code_block_match:
            if DEBUG_MODE:
                print(f"{LOG_PREFIX} DEBUG: Found JSON in code block")
            return json.loads(code_block_match.group(1))

        start = response.find("{")
        if start != -1:
            bracket_count = 0
            for i, char in enumerate(response[start:], start):
                if char == "{":
                    bracket_count += 1
                elif char == "}":
                    bracket_count -= 1
                    if bracket_count == 0:
                        json_str = response[start:i + 1]
                        if DEBUG_MODE:
                            print(f"{LOG_PREFIX} DEBUG: Extracted JSON ({len(json_str)} chars)")
                        return json.loads(json_str)
            if DEBUG_MODE:
                print(f"{LOG_PREFIX} DEBUG: Unbalanced brackets, final count: {bracket_count}")
    except (json.JSONDecodeError, ValueError) as e:
        print(f"{LOG_PREFIX} JSON parse error: {e}")
        print(f"{LOG_PREFIX} Response preview: {response[:500]}...")

    # Save failed response for debugging
    debug_path = Path(__file__).parent / "debug_06b_failed_response.txt"
    debug_path.write_text(response)
    print(f"{LOG_PREFIX} DEBUG: Saved full response to {debug_path}")

    return None


def convert_analysis_to_json(analysis: str, timeout: int = 120) -> Optional[Dict]:
    """Convert a text analysis to JSON using a second LLM call.

    This is the fallback when the original response had good analysis
    but failed to produce valid JSON.
    """
    if not analysis or len(analysis) < 50:
        return None

    print(f"{LOG_PREFIX}   Attempting JSON conversion fallback...")

    prompt = CONVERSION_PROMPT.format(analysis=analysis)

    claude_bin = find_claude_binary()
    if not claude_bin:
        return None

    try:
        result = subprocess.run(
            [claude_bin, "-p", prompt, "--output-format", "text"],
            capture_output=True,
            text=True,
            timeout=timeout,
        )
        if result.returncode == 0:
            converted = parse_json_response(result.stdout.strip())
            if converted and "verdict" in converted:
                print(f"{LOG_PREFIX}   JSON conversion successful!")
                return converted
            else:
                print(f"{LOG_PREFIX}   JSON conversion produced invalid output")
    except subprocess.TimeoutExpired:
        print(f"{LOG_PREFIX}   JSON conversion timed out")
    except Exception as e:
        print(f"{LOG_PREFIX}   JSON conversion error: {e}")

    return None


# ---------------------------
# Verification Logic
# ---------------------------

def extract_video_title(filename: str) -> str:
    name = Path(filename).stem
    name = re.sub(r"\.(conversations|verification)$", "", name)
    # Remove audio format suffixes
    name = re.sub(r"\.audio\.asr\.(raw|clean)16k$", "", name)
    match = re.match(r"^(.+?)\s*\[", name)
    return match.group(1).strip() if match else name


def extract_video_id(filename: str) -> str:
    match = re.search(r"\[([^\]]+)\]", filename)
    return match.group(1) if match else Path(filename).stem


def build_verification_prompt(data: Dict) -> str:
    """Build the verification prompt from stage 06 output."""

    video_title = extract_video_title(data.get("source_file", data.get("video_id", "Unknown")))
    video_type_info = data.get("video_type", {})
    video_type = video_type_info.get("type", "unknown")
    video_type_confidence = video_type_info.get("confidence", 0)

    # Speaker labels text
    speaker_labels = data.get("speaker_labels", {})
    speaker_lines = []
    for spk_id, label in sorted(speaker_labels.items()):
        role = label.get("role", "unknown")
        conf = label.get("confidence", 0)
        reasoning = label.get("reasoning", "")[:100]
        speaker_lines.append(f"  {spk_id}: {role} ({conf*100:.0f}%) — {reasoning}")
    speaker_labels_text = "\n".join(speaker_lines) if speaker_lines else "  (none)"

    # Collapse info
    collapse = data.get("speaker_collapse")
    if collapse and collapse.get("detected"):
        collapse_info = (
            f"DETECTED — {len(collapse.get('collapsed_speakers', []))} collapsed speaker(s): "
            f"{', '.join(collapse.get('collapsed_speakers', []))}. "
            f"{collapse.get('total_segments_affected', 0)} segments affected, "
            f"{collapse.get('reassignment_rate', 0)*100:.0f}% reassigned."
        )
    else:
        collapse_info = "Not detected (all speakers have distinct roles)"

    # Conversations summary
    conversations = data.get("conversations", [])
    segments = data.get("segments", [])
    conv_lines = []
    for conv in conversations:
        cid = conv["conversation_id"]
        seg_ids = conv.get("segment_ids", [])
        start = conv.get("start_time", 0)
        end = conv.get("end_time", 0)
        duration = end - start

        # Count roles in this conversation
        conv_segs = [s for s in segments if s.get("conversation_id") == cid]
        role_counts: Dict[str, int] = {}
        for s in conv_segs:
            role = s.get("speaker_role", "unknown")
            role_counts[role] = role_counts.get(role, 0) + 1

        roles_str = ", ".join(f"{r}:{c}" for r, c in sorted(role_counts.items(), key=lambda x: -x[1]))
        conv_lines.append(
            f"  Conv {cid}: {start:.0f}-{end:.0f}s ({duration:.0f}s), "
            f"{len(seg_ids)} segments, roles: {roles_str}"
        )
    conversations_summary = "\n".join(conv_lines) if conv_lines else "  (no conversations)"

    # Commentary summary
    commentary_segs = [s for s in segments if s.get("conversation_id", 0) == 0]
    if commentary_segs:
        conversations_summary += f"\n  Commentary: {len(commentary_segs)} segments (conversation_id=0)"

    # Full transcript
    transcript_lines = []
    for seg in segments:
        sid = seg.get("id", 0)
        start = seg.get("start", 0)
        end = seg.get("end", 0)
        text = seg.get("text", "").strip()
        speaker_id = seg.get("speaker_id", "UNKNOWN")
        speaker_role = seg.get("speaker_role", "unknown")
        conv_id = seg.get("conversation_id", 0)
        seg_type = seg.get("segment_type", "unknown")

        override = seg.get("speaker_role_override")
        override_str = f" [override:{override}]" if override else ""

        if text:
            conv_label = f"conv={conv_id}" if conv_id > 0 else "commentary"
            transcript_lines.append(
                f"[{sid}] {speaker_id}({speaker_role}{override_str}) "
                f"({start:.1f}-{end:.1f}s) {seg_type} {conv_label}: \"{text}\""
            )

    transcript = "\n".join(transcript_lines)

    return VERIFY_PROMPT.format(
        title=video_title,
        video_type=video_type,
        video_type_confidence=f"{video_type_confidence*100:.0f}%",
        speaker_labels_text=speaker_labels_text,
        collapse_info=collapse_info,
        conversations_summary=conversations_summary,
        segment_count=len(segments),
        transcript=transcript,
    )


def verify_file(input_path: Path, output_path: Path, dry_run: bool = False) -> Dict[str, Any]:
    """Verify a single stage 06 conversations.json file."""

    print(f"\n{LOG_PREFIX} Verifying: {input_path.name}")

    with input_path.open("r", encoding="utf-8") as f:
        data = json.load(f)

    video_id = data.get("video_id", extract_video_id(str(input_path)))
    video_title = extract_video_title(data.get("source_file", str(input_path)))
    segments = data.get("segments", [])
    conversations = data.get("conversations", [])

    print(f"{LOG_PREFIX}   Video: \"{video_title}\" [{video_id}]")
    print(f"{LOG_PREFIX}   Segments: {len(segments)}, Conversations: {len(conversations)}")

    if not segments:
        raise RuntimeError(f"No segments found in {input_path}")

    if dry_run:
        print(f"{LOG_PREFIX}   [DRY RUN] Would verify this file")
        return {"verdict": None, "issues": 0}

    start_time = time.time()

    # Build and send verification prompt
    prompt = build_verification_prompt(data)
    print(f"{LOG_PREFIX}   Sending to LLM for verification ({len(prompt)} chars)...")

    max_retries = 3
    result = None
    raw_response = None
    llm_calls = 0
    used_conversion_fallback = False

    for attempt in range(max_retries):
        raw_response = call_claude(prompt, timeout=300)
        llm_calls += 1
        result = parse_json_response(raw_response)

        if result and "verdict" in result:
            break

        # JSON parsing failed - try conversion fallback before retrying full prompt
        if raw_response and len(raw_response) > 100:
            print(f"{LOG_PREFIX}   WARNING: JSON parsing failed, trying conversion fallback...")
            result = convert_analysis_to_json(raw_response)
            llm_calls += 1
            if result and "verdict" in result:
                used_conversion_fallback = True
                print(f"{LOG_PREFIX}   Conversion fallback succeeded!")
                break

        if attempt < max_retries - 1:
            print(f"{LOG_PREFIX}   WARNING: Conversion also failed, retrying full prompt ({attempt + 2}/{max_retries})...")
            time.sleep(2)

    if not result or "verdict" not in result:
        raise RuntimeError(f"Verification LLM failed after {max_retries} retries and conversion fallback")

    elapsed = time.time() - start_time

    # Build verification output
    verdict = result.get("verdict", "FLAG")
    misattributions = result.get("misattributions", [])
    boundary_issues = result.get("boundary_issues", [])
    collapse_issues = result.get("collapse_issues", [])
    other_flags = result.get("other_flags", [])

    total_issues = (
        len(misattributions)
        + len([b for b in boundary_issues if b.get("severity") in ("moderate", "major")])
        + len(collapse_issues)
        + len(other_flags)
    )

    output = {
        "video_id": video_id,
        "verified_at": time.strftime("%Y-%m-%dT%H:%M:%SZ"),
        "pipeline_version": PIPELINE_VERSION,
        "verdict": verdict,
        "video_type_check": result.get("video_type_check", {}),
        "conversation_verdicts": result.get("conversation_verdicts", []),
        "misattributions": misattributions,
        "boundary_issues": boundary_issues,
        "collapse_issues": collapse_issues,
        "other_flags": other_flags,
        "summary": result.get("summary", ""),
        "metadata": {
            "input_file": str(input_path),
            "processing_time_sec": elapsed,
            "llm_calls": llm_calls,
            **({"note": "Used JSON conversion fallback"} if used_conversion_fallback else {}),
        },
    }

    _validate_verification_schema(output)

    # Write output
    output_path.parent.mkdir(parents=True, exist_ok=True)
    with output_path.open("w", encoding="utf-8") as f:
        json.dump(output, f, indent=2, ensure_ascii=False)

    # Print summary
    print(f"{LOG_PREFIX} Verdict: {verdict}")
    if misattributions:
        print(f"{LOG_PREFIX}   Misattributions: {len(misattributions)}")
        for m in misattributions:
            print(f"{LOG_PREFIX}     seg {m.get('segment_id')}: {m.get('current_role')} → {m.get('suggested_role')} — {m.get('evidence', '')[:80]}")
    if boundary_issues:
        print(f"{LOG_PREFIX}   Boundary issues: {len(boundary_issues)}")
        for b in boundary_issues:
            print(f"{LOG_PREFIX}     conv {b.get('conversation_id')}: [{b.get('severity')}] {b.get('issue', '')[:80]}")
    if collapse_issues:
        print(f"{LOG_PREFIX}   Collapse issues: {len(collapse_issues)}")
    if other_flags:
        print(f"{LOG_PREFIX}   Other flags: {len(other_flags)}")
        for flag in other_flags:
            print(f"{LOG_PREFIX}     {flag[:100]}")
    print(f"{LOG_PREFIX}   Summary: {result.get('summary', '')[:200]}")
    print(f"{LOG_PREFIX}   Time: {elapsed:.1f}s")
    print(f"{LOG_PREFIX}   Output: {output_path}")

    return {
        "verdict": verdict,
        "issues": total_issues,
        "misattributions": len(misattributions),
        "boundary_issues": len(boundary_issues),
        "collapse_issues": len(collapse_issues),
    }


# ---------------------------
# Path helpers
# ---------------------------

def repo_root() -> Path:
    return Path(__file__).resolve().parents[2]


def input_root() -> Path:
    return repo_root() / "data" / "06.video-type"


def output_root() -> Path:
    return repo_root() / "data" / "06b.verify"


def test_input_root() -> Path:
    return repo_root() / "data" / "test" / "06.video-type"


def test_output_root() -> Path:
    return repo_root() / "data" / "test" / "06b.verify"


def compute_output_path(input_path: Path, output_dir: Path) -> Path:
    stem = input_path.stem
    if stem.endswith(".conversations"):
        stem = stem[:-len(".conversations")]
    return output_dir / f"{stem}.verification.json"


def parse_sources_file(path: Path) -> List[Tuple[str, str]]:
    sources: List[Tuple[str, str]] = []
    for raw in path.read_text(encoding="utf-8").splitlines():
        line = raw.strip()
        if not line or line.startswith("#"):
            continue
        if "|" in line:
            name, url = line.split("|", 1)
            sources.append((name.strip(), url.strip()))
            continue
        parts = shlex.split(line)
        if len(parts) >= 2:
            sources.append((parts[0], parts[1]))
    return sources


def find_input_files(in_dir: Path) -> List[Path]:
    return sorted(in_dir.rglob("*.conversations.json"))


# ---------------------------
# CLI
# ---------------------------

def main() -> None:
    parser = argparse.ArgumentParser(
        description="LLM verification of stage 06 video type classification output"
    )
    parser.add_argument(
        "name", nargs="?",
        help="Source name (folder under data/06.video-type/)"
    )
    parser.add_argument(
        "youtube_url", nargs="?",
        help="YouTube URL (unused, accepted for pipeline compatibility)"
    )
    parser.add_argument(
        "--input",
        help="Input .conversations.json file or directory"
    )
    parser.add_argument(
        "--output",
        help="Output directory (defaults to data/06b.verify/)"
    )
    parser.add_argument(
        "--test",
        action="store_true",
        help="Process test videos (data/test/06.video-type/)"
    )
    parser.add_argument(
        "--sources",
        nargs="?",
        const="docs/pipeline/sources.txt",
        help="Process all sources from sources.txt file"
    )
    parser.add_argument(
        "--manifest",
        help="Manifest file: only process videos listed (docs/pipeline/batches/P001.txt)."
    )
    parser.add_argument(
        "--dry-run",
        action="store_true",
        help="Preview what would be processed"
    )
    parser.add_argument(
        "--overwrite",
        action="store_true",
        help="Overwrite existing output files"
    )
    parser.add_argument(
        "--debug",
        action="store_true",
        help="Enable debug logging for LLM responses"
    )

    args = parser.parse_args()

    global DEBUG_MODE
    DEBUG_MODE = args.debug

    # Test Claude CLI availability
    claude_bin = find_claude_binary()
    if not claude_bin:
        print(f"{LOG_PREFIX} Error: Claude CLI binary not found")
        print(f"{LOG_PREFIX} Searched paths:")
        for p in CLAUDE_BINARY_PATHS:
            print(f"  - {p}")
        print(f"{LOG_PREFIX} Install Claude Code CLI: https://claude.ai/code")
        raise SystemExit(1)

    try:
        result = subprocess.run(
            [claude_bin, "--version"],
            capture_output=True, text=True, timeout=10,
        )
        if result.returncode != 0:
            print(f"{LOG_PREFIX} Warning: Claude CLI not responding properly")
        else:
            print(f"{LOG_PREFIX} Using Claude CLI: {claude_bin}")
    except subprocess.TimeoutExpired:
        print(f"{LOG_PREFIX} Warning: Claude CLI slow to respond")

    # Route to appropriate mode
    if args.test:
        _run_directory(test_input_root(), test_output_root(), args)
    elif args.manifest:
        _run_manifest(args)
    elif args.input:
        _run_input(args)
    elif args.sources:
        _run_sources(args)
    elif args.name:
        _run_named_source(args)
    else:
        raise SystemExit("Provide a source name, --input, --test, --manifest, or --sources")


def _run_input(args) -> None:
    input_path = Path(args.input)
    if not input_path.exists():
        input_path = repo_root() / args.input
    if not input_path.exists():
        raise SystemExit(f"Input not found: {args.input}")

    if input_path.is_file():
        out_dir = Path(args.output) if args.output else output_root()
        output_path = compute_output_path(input_path, out_dir)

        if output_path.exists() and not args.overwrite:
            print(f"{LOG_PREFIX} Output exists, skipping: {output_path}")
            return

        result = verify_file(input_path, output_path, dry_run=args.dry_run)
        print(f"\n{LOG_PREFIX} Done. Verdict: {result.get('verdict')}, Issues: {result.get('issues', 0)}")
        return

    out_dir = Path(args.output) if args.output else output_root()
    _run_directory(input_path, out_dir, args)


def _run_sources(args) -> None:
    sources_path = repo_root() / args.sources
    if not sources_path.exists():
        raise SystemExit(f"Sources file not found: {sources_path}")

    total_files = 0
    total_issues = 0

    for src_name, _ in parse_sources_file(sources_path):
        src_in_dir = input_root() / src_name
        if not src_in_dir.exists():
            print(f"{LOG_PREFIX} Skipping {src_name}: no 06.video-type output")
            continue

        src_out_dir = output_root() / src_name
        files = find_input_files(src_in_dir)

        for input_file in files:
            output_file = compute_output_path(input_file, src_out_dir)
            if output_file.exists() and not args.overwrite:
                continue
            try:
                result = verify_file(input_file, output_file, dry_run=args.dry_run)
                total_issues += result.get("issues", 0)
                total_files += 1
            except Exception as e:
                print(f"{LOG_PREFIX} Error: {e}")

    print(f"\n{LOG_PREFIX} Done. Verified {total_files} files, {total_issues} total issues")


def _run_manifest(args) -> None:
    """Run for videos listed in a manifest file."""
    manifest_path = Path(args.manifest)
    if not manifest_path.is_absolute():
        manifest_path = repo_root() / manifest_path
    if not manifest_path.exists():
        raise SystemExit(f"Manifest file not found: {manifest_path}")

    sources_map = load_manifest_sources(manifest_path)

    for src_name, vid_ids in sorted(sources_map.items()):
        src_in_dir = input_root() / src_name
        if not src_in_dir.exists():
            print(f"{LOG_PREFIX} Skipping {src_name}: no 06.video-type output")
            continue

        src_out_dir = output_root() / src_name
        files = manifest_filter_files(find_input_files(src_in_dir), vid_ids)
        if not files:
            print(f"{LOG_PREFIX} Skipping {src_name}: no manifest videos found in input")
            continue

        print(f"{LOG_PREFIX} Manifest: {src_name} ({len(files)} videos)")
        _run_directory_with_files(files, src_in_dir, src_out_dir, args)


def _run_directory_with_files(files: List[Path], in_dir: Path, out_dir: Path, args) -> None:
    """Process a specific list of files."""
    if not files:
        return

    print(f"{LOG_PREFIX} Input : {in_dir}")
    print(f"{LOG_PREFIX} Output: {out_dir}")
    print(f"{LOG_PREFIX} Files : {len(files)}")

    state_path = out_dir / ".06b_verify_state.json"
    state = load_state(state_path)

    processed = 0
    skipped = 0
    failed = 0
    consecutive_failures = 0
    verdicts: Dict[str, int] = {"APPROVE": 0, "FLAG": 0, "REJECT": 0}

    for input_file in files:
        file_key = str(input_file.relative_to(in_dir))

        if file_key in state.completed_files and not args.overwrite:
            skipped += 1
            continue

        output_file = compute_output_path(input_file, out_dir)

        if output_file.exists() and not args.overwrite:
            skipped += 1
            state.completed_files.append(file_key)
            save_state(state_path, state)
            continue

        state.in_progress = file_key
        save_state(state_path, state)

        try:
            result = verify_file(input_file, output_file, dry_run=args.dry_run)
            processed += 1
            consecutive_failures = 0

            v = result.get("verdict", "FLAG")
            if v in verdicts:
                verdicts[v] += 1

            if not args.dry_run:
                state.completed_files.append(file_key)
                state.in_progress = None
                save_state(state_path, state)

        except Exception as e:
            print(f"{LOG_PREFIX} Error processing {input_file}: {e}")
            state.failures.append({"file": file_key, "error": str(e)})
            state.in_progress = None
            save_state(state_path, state)
            failed += 1
            consecutive_failures += 1

            if consecutive_failures >= MAX_CONSECUTIVE_FAILURES:
                print(f"\n{LOG_PREFIX} HALTING: {consecutive_failures} consecutive failures")
                break

    print(f"\n{LOG_PREFIX} Done.")
    print(f"  Processed: {processed}")
    print(f"  Skipped:   {skipped}")
    print(f"  Failed:    {failed}")
    print(f"  Verdicts:  {verdicts}")

    total_attempted = processed + failed
    if total_attempted > 0:
        failure_rate = failed / total_attempted
        if failure_rate > MAX_FAILURE_RATE and total_attempted >= 5:
            print(f"\n{LOG_PREFIX} HALTING: Failure rate {failure_rate:.0%} exceeds {MAX_FAILURE_RATE:.0%} threshold")
            sys.exit(1)

    reject_count = verdicts.get("REJECT", 0)
    if reject_count > 0:
        print(f"\n{LOG_PREFIX} WARNING: {reject_count} video(s) received REJECT verdict")
        print(f"{LOG_PREFIX} Review verification reports before proceeding to stage 07")
        sys.exit(1)


def _run_named_source(args) -> None:
    """Run for a named source (from pipeline: ./06b.verify source_name url)."""
    name = args.name
    in_dir = input_root() / name
    if not in_dir.exists():
        raise SystemExit(f"Input directory not found: {in_dir}")

    out_dir = Path(args.output) if args.output else output_root() / name
    _run_directory(in_dir, out_dir, args)


def _run_directory(in_dir: Path, out_dir: Path, args) -> None:
    files = find_input_files(in_dir)
    if not files:
        print(f"{LOG_PREFIX} No .conversations.json files found in: {in_dir}")
        return
    _run_directory_with_files(files, in_dir, out_dir, args)


if __name__ == "__main__":
    main()
