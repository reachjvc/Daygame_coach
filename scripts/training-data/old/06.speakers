#!/usr/bin/env python3
"""
scripts/training-data/06.speakers

Speaker classification for tonality JSON files.

Reads:
  data/05.tonality/<source>/**/*.tonality.json

Writes:
  data/06.speakers/<source>/**/*.tonality.json

Use:
  A) One source (video / playlist / channel):
     ./scripts/training-data/06.speakers "daily_evolution" "https://www.youtube.com/watch?v=utuuVOXJunM"

  B) Batch from sources file:
     ./scripts/training-data/06.speakers --sources
     ./scripts/training-data/06.speakers --sources docs/sources.txt
"""

from __future__ import annotations

import json
import re
from pathlib import Path
from typing import Dict, List, Optional, Tuple

import argparse
import shlex

import numpy as np


COACH_TEXT_PATTERNS = [
    r"^(excuse me|hey|hi),?(\s|$)",
    r"^(one|two|quick) (second|sec)",
    r"^i (just )?(saw|noticed) you",
    r"^this is (so |really )?random",
    r"^i had to (come|stop|say)",
    r"what's your name",
    r"where (are you|you) from",
    r"what (do you do|are you up to|brings you)",
    r"how (long|old|tall)",
    r"do you (have|live|work|study)",
    r"can i (get|have|take) your",
    r"why don't (you|we)",
    r"let('s| me| us)",
    r"i (like|love|noticed) (your|how|that)",
    r"you (look|seem|have)",
    r"i('m| am) (going to|gonna)",
    r"we (should|could|can)",
    r"(hey |hi )?(guys|everyone)",
    r"as you (can see|saw|noticed)",
    r"notice (how|that|here)",
    r"the (reason|key|trick|point)",
    r"what you('re| are) (about to|going to)",
    r"(this|that) is (what|why|how|the)",
]

GIRL_TEXT_PATTERNS = [
    r"^(yeah|yes|no|maybe|kind of|sort of|i guess|i think so|thank you|thanks)(\.|,|!|\?)?$",
    r"^(oh|wow|really|nice|cool|okay|ok|sure|right|exactly)(\.|,|!|\?)?$",
    r"^(haha|hehe|lol)$",
    r"^i('m| am) (from|a|an|studying|working)",
    r"^i (work|study|live|do|have)",
    r"^my (name|job|work) is",
    r"^it's .{1,30}$",
    r"^that's (nice|cool|sweet|funny|cute|interesting)",
    r"^you('re| are) (so |very |really )?(funny|nice|sweet|cute)",
    r"^(and |so )?(what about you|how about you|you\?)",
    r"^where are you from",
    r"^what('s| is) your (name|job)",
]


def classify_by_text(text: str, prev_speaker: Optional[str] = None) -> Dict:
    """Classify speaker based on text content patterns."""
    text_lower = text.lower().strip()
    word_count = len(text.split())

    coach_score = 0.0
    girl_score = 0.0
    reasons: List[str] = []

    for pattern in COACH_TEXT_PATTERNS:
        if re.search(pattern, text_lower, re.IGNORECASE):
            coach_score += 2
            reasons.append(f"coach_pattern:{pattern[:20]}")

    for pattern in GIRL_TEXT_PATTERNS:
        if re.search(pattern, text_lower, re.IGNORECASE):
            girl_score += 2
            reasons.append(f"girl_pattern:{pattern[:20]}")

    # Short responses are more likely target
    if word_count <= 5 and "?" not in text:
        girl_score += 1
        reasons.append("short_response")

    if word_count <= 2 and len(text) < 15:
        girl_score += 2
        reasons.append("very_short_response")

    # Questions (longer) are more likely coach initiating
    if text.strip().endswith("?") and word_count > 3:
        coach_score += 1
        reasons.append("asking_question")

    # Long explanations are more likely coach
    if word_count > 20:
        coach_score += 1
        reasons.append("long_statement")

    # Alternation tie-breaker
    if prev_speaker and coach_score == girl_score:
        if prev_speaker == "coach":
            girl_score += 0.5
            reasons.append("alternation_from_coach")
        elif prev_speaker == "target":
            coach_score += 0.5
            reasons.append("alternation_from_target")

    total = coach_score + girl_score
    if total <= 0:
        return {"speaker": "unknown", "confidence": 0.0, "reasons": reasons}

    if coach_score > girl_score:
        confidence = min(0.9, coach_score / (total + 1))
        return {"speaker": "coach", "confidence": confidence, "reasons": reasons}

    if girl_score > coach_score:
        confidence = min(0.9, girl_score / (total + 1))
        return {"speaker": "target", "confidence": confidence, "reasons": reasons}

    return {"speaker": "ambiguous", "confidence": 0.3, "reasons": reasons}


def classify_speaker_audio(features: Dict) -> Dict:
    """Heuristic speaker classification using pitch and spectral cues."""
    if not features or not features.get("pitch"):
        return {"speaker": "unknown", "confidence": 0.0, "reasons": ["no_pitch_data"]}

    pitch = features["pitch"]
    spectral = features.get("spectral", {})

    pitch_mean = pitch.get("mean_hz")
    pitch_range = pitch.get("range_hz", 0.0) or 0.0
    brightness = spectral.get("brightness_hz", 0.0) or 0.0

    score_male = 0.0
    score_female = 0.0
    reasons: List[str] = []

    if pitch_mean is not None:
        if pitch_mean < 140:
            score_male += 3
            reasons.append("low_pitch")
        elif pitch_mean < 180:
            score_male += 1
            reasons.append("medium_low_pitch")
        elif pitch_mean > 220:
            score_female += 3
            reasons.append("high_pitch")
        elif pitch_mean > 180:
            score_female += 1
            reasons.append("medium_high_pitch")

    if pitch_range > 100:
        score_female += 1
        reasons.append("high_pitch_variability")

    if brightness > 2000:
        score_female += 1
        reasons.append("bright_voice")
    elif brightness < 1500:
        score_male += 1
        reasons.append("darker_voice")

    total = score_male + score_female
    if total <= 0:
        return {"speaker": "unknown", "confidence": 0.0, "reasons": reasons}

    if score_male > score_female:
        confidence = score_male / (total + 1)
        return {"speaker": "coach", "confidence": confidence, "reasons": reasons}

    if score_female > score_male:
        confidence = score_female / (total + 1)
        return {"speaker": "target", "confidence": confidence, "reasons": reasons}

    return {"speaker": "ambiguous", "confidence": 0.5, "reasons": reasons}


def detect_voiceover(segment: Dict) -> bool:
    """Detect voiceover/commentary heuristically."""
    features = segment.get("features", {})
    energy = features.get("energy", {})
    quality = features.get("quality", {})

    dynamics = energy.get("dynamics_db", 10)
    duration = float(segment.get("end", 0) - segment.get("start", 0))
    low_energy = quality.get("low_energy", False)

    return bool(dynamics < 3 or duration > 30 or low_energy)


def classify_speaker_hybrid(
    segment: Dict,
    prev_speaker: Optional[str] = None,
    segment_type: Optional[str] = None,
) -> Dict:
    """
    Hybrid speaker classification combining audio + text + context.
    Priority:
    1) segment_type == "commentary" -> coach
    2) text patterns
    3) audio cues
    4) alternation fallback
    """
    if segment_type == "commentary":
        return {
            "label": "coach",
            "confidence": 0.85,
            "reasons": ["commentary_segment"],
            "method": "segment_type",
        }

    text = segment.get("text", "").strip()
    features = segment.get("features", {})

    text_result = classify_by_text(text, prev_speaker)
    audio_result = classify_speaker_audio(features)

    text_speaker = text_result.get("speaker", "unknown")
    text_conf = float(text_result.get("confidence", 0.0))
    audio_speaker = audio_result.get("speaker", "unknown")
    audio_conf = float(audio_result.get("confidence", 0.0))

    combined_reasons = (text_result.get("reasons", []) or []) + (audio_result.get("reasons", []) or [])

    # Agreement -> best case
    if text_speaker == audio_speaker and text_speaker not in ("unknown", "ambiguous"):
        return {
            "label": text_speaker,
            "confidence": min(0.95, (text_conf + audio_conf) / 2 + 0.2),
            "reasons": combined_reasons,
            "method": "hybrid_agreement",
        }

    # Text priority if strong
    if text_conf > 0.6 and text_speaker not in ("unknown", "ambiguous"):
        return {
            "label": text_speaker,
            "confidence": text_conf,
            "reasons": combined_reasons,
            "method": "text_priority",
        }

    # Audio priority if decent
    if audio_conf > 0.5 and audio_speaker not in ("unknown", "ambiguous"):
        return {
            "label": audio_speaker,
            "confidence": audio_conf,
            "reasons": combined_reasons,
            "method": "audio_priority",
        }

    # Alternation fallback
    if prev_speaker == "coach":
        return {
            "label": "target",
            "confidence": 0.4,
            "reasons": combined_reasons + ["alternation_fallback"],
            "method": "alternation",
        }
    if prev_speaker == "target":
        return {
            "label": "coach",
            "confidence": 0.4,
            "reasons": combined_reasons + ["alternation_fallback"],
            "method": "alternation",
        }

    return {
        "label": "unknown",
        "confidence": 0.2,
        "reasons": combined_reasons,
        "method": "unknown",
    }


def _json_default(o):
    """JSON serializer for numpy types."""
    if isinstance(o, np.generic):
        return o.item()
    raise TypeError(f"Object of type {o.__class__.__name__} is not JSON serializable")


def process_file(path: Path, output_path: Path) -> None:
    """Load a .tonality.json, label speakers per segment, and write updated JSON."""
    with path.open("r", encoding="utf-8") as f:
        data = json.load(f)

    segments = data.get("segments", [])
    prev_speaker: Optional[str] = None

    for segment in segments:
        # Voiceover first
        if detect_voiceover(segment):
            segment["speaker"] = {
                "label": "voiceover",
                "confidence": 0.7,
                "reasons": ["voiceover_pattern"],
                "method": "voiceover",
            }
            prev_speaker = "coach"
            continue

        segment_type = segment.get("segment_type")
        classification = classify_speaker_hybrid(segment, prev_speaker, segment_type)

        segment["speaker"] = {
            "label": classification["label"],
            "confidence": classification["confidence"],
            "reasons": classification.get("reasons", []),
            "method": classification.get("method", "unknown"),
        }

        label = segment["speaker"]["label"]
        if label in ("coach", "target"):
            prev_speaker = label

    output_path.parent.mkdir(parents=True, exist_ok=True)
    with output_path.open("w", encoding="utf-8") as f:
        json.dump(data, f, indent=2, default=_json_default)


def repo_root() -> Path:
    # scripts/training-data/<thisfile> -> repo root = parents[2]
    return Path(__file__).resolve().parents[2]


def extract_video_id(url: str) -> Optional[str]:
    url = (url or "").strip()
    if not url:
        return None
    m = re.search(r"[?&]v=([^&]+)", url)
    if m:
        return m.group(1)
    m = re.search(r"youtu\.be/([^?&/]+)", url)
    if m:
        return m.group(1)
    return None


def parse_sources_file(path: Path) -> List[Tuple[str, str]]:
    sources: List[Tuple[str, str]] = []
    for raw in path.read_text(encoding="utf-8").splitlines():
        line = raw.strip()
        if not line or line.startswith("#"):
            continue
        if "|" in line:
            name, url = line.split("|", 1)
            name = name.strip()
            url = url.strip()
            if name and url:
                sources.append((name, url))
            continue
        parts = shlex.split(line)
        if len(parts) >= 2:
            sources.append((parts[0], parts[1]))
    return sources


def run_for_source(source: str, youtube_url: str, overwrite: bool) -> None:
    root = repo_root()
    video_id = extract_video_id(youtube_url)

    in_dir = root / "data" / "05.tonality" / source
    out_dir = root / "data" / "06.speakers" / source

    if not in_dir.exists():
        raise SystemExit(f"[speaker] ERROR: input folder does not exist: {in_dir}")

    files = sorted(in_dir.rglob("*.tonality.json"))
    if video_id:
        needle = f"[{video_id}]"
        files = [f for f in files if needle in f.as_posix() or needle in f.name]

    if not files:
        raise SystemExit(f"[speaker] ERROR: no .tonality.json files found in: {in_dir}")

    out_dir.mkdir(parents=True, exist_ok=True)

    for file in files:
        rel = file.relative_to(in_dir)
        dest = out_dir / rel
        if dest.exists() and not overwrite:
            continue
        print(f"[speaker] {file} -> {dest}")
        process_file(file, dest)


def main() -> None:
    parser = argparse.ArgumentParser(description="Speaker classification for tonality JSON files.")
    parser.add_argument("source", nargs="?", help="Source name (folder under data/05.tonality).")
    parser.add_argument("youtube_url", nargs="?", help="YouTube URL (video/playlist/channel). Video URL filters by ID.")
    parser.add_argument(
        "--sources",
        nargs="?",
        const="docs/sources.txt",
        help="Process all sources from a sources.txt file (default: docs/sources.txt).",
    )
    parser.add_argument("--overwrite", action="store_true", help="Overwrite already-written outputs.")
    args = parser.parse_args()

    if args.sources:
        sources_path = Path(args.sources)
        if not sources_path.is_absolute():
            sources_path = (repo_root() / sources_path).resolve()
        if not sources_path.exists():
            raise SystemExit(f"[speaker] Missing sources file: {sources_path}")
        for name, url in parse_sources_file(sources_path):
            run_for_source(name, url, overwrite=args.overwrite)
        return

    if not args.source:
        raise SystemExit(
            "Provide either:\n"
            "  - one source:  ./scripts/training-data/06.speakers \"daily_evolution\" \"https://www.youtube.com/watch?v=utuuVOXJunM\"\n"
            "  - --sources:   ./scripts/training-data/06.speakers --sources docs/sources.txt\n"
        )

    run_for_source(args.source, args.youtube_url or "", overwrite=args.overwrite)


if __name__ == "__main__":
    main()
