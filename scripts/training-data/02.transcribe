#!/usr/bin/env python3
"""
scripts/training-data/02.transcribe

STEP 2 — RAW TRANSCRIPTION (large-v3 only)

Usage:
  A) Single source:  ./02.transcribe "daily_evolution" "https://youtube.com/watch?v=..."
  B) Batch sources:  ./02.transcribe --sources docs/sources.txt
  C) Single file:    ./02.transcribe --audio path/to/audio.wav --out output.full.json
  D) Manifest batch: ./02.transcribe --manifest docs/pipeline/batches/P001.txt

Transcription Model (UPDATED 03-02-2026):
  large-v3 with condition_on_previous_text=False (default)
    - Prevents hallucination loops (repeated phrases)
    - Uses raw audio to capture quieter voices (e.g., women's responses in infield)
    - Trade-off: Minor capitalization inconsistencies on proper nouns

  To enable condition_on_previous_text (better capitalization, risk of hallucination):
    ./02.transcribe --audio <path> --out <output> --condition-on-prev

Note: This script ONLY does transcription. Alignment and diarization are
      handled by subsequent scripts (03.align, 04.diarize).

Input:  data/01.download/<source>/<video>/*.audio.asr.raw16k.wav
Output: data/02.transcribe/<source>/<video>/<video>.full.json + .txt
"""

from __future__ import annotations

import argparse
import json
import os
import re
import shlex
import shutil
import subprocess
import sys
import threading
import time
from contextlib import contextmanager
from dataclasses import dataclass
from datetime import datetime
from pathlib import Path
from typing import Any, Callable, Dict, Iterator, List, Optional, Set, Tuple

import numpy as np

from pipeline_manifest import load_manifest, load_manifest_sources, manifest_filter_dirs

# --------------------------
# Torch import
# --------------------------

try:
    import torch  # type: ignore
except Exception:
    torch = None

# Optional audio loaders
try:
    import torchaudio  # type: ignore
except Exception:
    torchaudio = None

try:
    import soundfile as sf  # type: ignore
except Exception:
    sf = None

try:
    import librosa  # type: ignore
except Exception:
    librosa = None


# --------------------------
# Small helpers
# --------------------------

def log(msg: str) -> None:
    print(msg, flush=True)


def repo_root() -> Path:
    return Path(__file__).resolve().parents[2]


def now_iso() -> str:
    return datetime.utcnow().replace(microsecond=0).isoformat() + "Z"


def safe_name(name: str) -> str:
    cleaned = re.sub(r"[^A-Za-z0-9._-]+", "_", (name or "").strip())
    return cleaned.strip("_") or "source"


def extract_video_id(url: str) -> Optional[str]:
    url = (url or "").strip()
    if not url:
        return None
    m = re.search(r"[?&]v=([^&]+)", url)
    if m:
        return m.group(1)
    m = re.search(r"youtu\.be/([^?&/]+)", url)
    if m:
        return m.group(1)
    return None


def parse_sources_file(path: Path) -> List[Tuple[str, str]]:
    out: List[Tuple[str, str]] = []
    for raw in path.read_text(encoding="utf-8").splitlines():
        line = raw.strip()
        if not line or line.startswith("#"):
            continue
        if "|" in line:
            name, url = line.split("|", 1)
            name = name.strip()
            url = url.strip()
            if name and url:
                out.append((name, url))
            continue
        parts = shlex.split(line)
        if len(parts) >= 2:
            out.append((parts[0], parts[1]))
    return out


def _r3(x: float) -> float:
    return float(np.round(float(x), 3))


def _detect_repetition_hallucination(
    segments: List[Dict[str, Any]],
    min_repeats: int = 3,
) -> Optional[Dict[str, Any]]:
    """
    Detect Whisper hallucination loops where the same sentence is repeated 3+ times.

    Returns None if no hallucination detected, or a dict with:
        - repeated_text: the text that was repeated
        - count: number of consecutive repetitions
        - first_index: index of first occurrence
        - last_index: index of last occurrence

    Note: Only flags when the SAME SENTENCE is repeated. Does NOT flag if
    the same word appears in different sentences.
    """
    if len(segments) < min_repeats:
        return None

    # Track consecutive repetitions
    prev_text = None
    repeat_count = 0
    first_idx = 0

    for i, seg in enumerate(segments):
        text = str(seg.get("text", "")).strip()
        if not text:
            continue

        if text == prev_text:
            repeat_count += 1
        else:
            # Check if we had enough repetitions before reset
            if repeat_count >= min_repeats:
                return {
                    "repeated_text": prev_text,
                    "count": repeat_count,
                    "first_index": first_idx,
                    "last_index": i - 1,
                }
            prev_text = text
            repeat_count = 1
            first_idx = i

    # Check final sequence
    if repeat_count >= min_repeats:
        return {
            "repeated_text": prev_text,
            "count": repeat_count,
            "first_index": first_idx,
            "last_index": len(segments) - 1,
        }

    return None


def _detect_intra_segment_repetition(
    segments: List[Dict[str, Any]],
    min_word_repeats: int = 6,
    suspicious_word_duration: float = 5.0,
) -> List[Dict[str, Any]]:
    """
    Detect repeated words within individual segments (intra-segment hallucination).

    Checks each segment's word-level timestamps for:
    1. Consecutive runs of the same word (case-insensitive, punctuation-stripped) >= min_word_repeats (default 6)
    2. Any single word with duration > suspicious_word_duration seconds (gap-fill indicator)

    Returns a list of findings, each with:
        - segment_index: which segment
        - repeated_word: the word (lowercased, stripped)
        - count: consecutive occurrences
        - time_range: [start, end] of the repeated run
        - longest_word_duration: max duration of a single word instance
    """
    import re

    def _normalize_word(w: str) -> str:
        """Lowercase and strip non-alphanumeric chars for comparison."""
        return re.sub(r"[^a-z0-9]", "", w.lower())

    findings: List[Dict[str, Any]] = []

    for seg_idx, seg in enumerate(segments):
        words = seg.get("words")
        if not words or not isinstance(words, list):
            continue

        # Track consecutive runs of the same normalized word
        prev_norm = ""
        run_count = 0
        run_start_time: Optional[float] = None
        run_end_time: Optional[float] = None
        run_longest_dur = 0.0
        run_raw_word = ""

        def _emit_if_flagged() -> None:
            nonlocal run_count, run_start_time, run_end_time, run_longest_dur, run_raw_word
            if run_count >= min_word_repeats:
                finding: Dict[str, Any] = {
                    "segment_index": seg_idx,
                    "repeated_word": run_raw_word,
                    "count": run_count,
                    "time_range": [
                        run_start_time if run_start_time is not None else seg.get("start", 0.0),
                        run_end_time if run_end_time is not None else seg.get("end", 0.0),
                    ],
                    "longest_word_duration": round(run_longest_dur, 3),
                }
                findings.append(finding)

        for w in words:
            word_text = str(w.get("word", "")).strip()
            if not word_text:
                continue

            norm = _normalize_word(word_text)
            if not norm:
                continue

            # Compute word duration (only if both start and end exist)
            w_start = w.get("start")
            w_end = w.get("end")
            w_dur = 0.0
            if isinstance(w_start, (int, float)) and isinstance(w_end, (int, float)) and w_end > w_start:
                w_dur = float(w_end) - float(w_start)

            if norm == prev_norm:
                run_count += 1
                if isinstance(w_end, (int, float)):
                    run_end_time = float(w_end)
                run_longest_dur = max(run_longest_dur, w_dur)
            else:
                _emit_if_flagged()
                prev_norm = norm
                run_count = 1
                run_raw_word = norm
                run_start_time = float(w_start) if isinstance(w_start, (int, float)) else None
                run_end_time = float(w_end) if isinstance(w_end, (int, float)) else None
                run_longest_dur = w_dur

            # Also flag individual words with suspicious duration
            if w_dur > suspicious_word_duration:
                # Check if this word isn't already going to be captured by a run
                # We'll handle this at the end to avoid duplicates
                pass

        # Final run check
        _emit_if_flagged()

        # Check for suspicious individual word durations (even without consecutive repeats)
        for w in words:
            w_start = w.get("start")
            w_end = w.get("end")
            if isinstance(w_start, (int, float)) and isinstance(w_end, (int, float)) and w_end > w_start:
                w_dur = float(w_end) - float(w_start)
                if w_dur > suspicious_word_duration:
                    # Only flag if this word isn't already covered by a repetition finding
                    word_norm = _normalize_word(str(w.get("word", "")))
                    already_covered = any(
                        f["segment_index"] == seg_idx
                        and f["repeated_word"] == word_norm
                        for f in findings
                    )
                    if not already_covered:
                        findings.append({
                            "segment_index": seg_idx,
                            "repeated_word": word_norm,
                            "count": 1,
                            "time_range": [float(w_start), float(w_end)],
                            "longest_word_duration": round(w_dur, 3),
                        })

    return findings


def _classify_transcript_quality(
    segments: List[Dict[str, Any]],
    duration_sec: float,
    hallucination: Optional[Dict[str, Any]],
    intra_seg_issues: Optional[List[Dict[str, Any]]] = None,
) -> Dict[str, Any]:
    """
    Classify transcript quality and severity. Returns a dict with:
        - severity: "OK" | "WARNING" | "CRITICAL"
        - reasons: list of human-readable reason strings
        - total_words: int
        - wpm: float (words per minute)
        - hallucination_ratio: float (0.0-1.0, fraction of segments that are hallucinated)
    """
    total_words = sum(
        len(str(s.get("text", "")).split())
        for s in segments
        if str(s.get("text", "")).strip()
    )
    duration_min = duration_sec / 60.0 if duration_sec > 0 else 0.0
    wpm = total_words / duration_min if duration_min > 0 else 0.0
    seg_count = len(segments)

    hallucination_ratio = 0.0
    if hallucination and seg_count > 0:
        hallucinated_segs = hallucination["last_index"] - hallucination["first_index"] + 1
        hallucination_ratio = hallucinated_segs / seg_count

    severity = "OK"
    reasons: List[str] = []

    # CRITICAL: transcript is essentially empty
    if duration_sec > 30 and total_words < 50:
        severity = "CRITICAL"
        reasons.append(f"near-empty transcript ({total_words} words for {duration_sec:.0f}s audio)")

    # CRITICAL: extremely low WPM (less than 30 WPM for audio > 1 min)
    if duration_sec > 60 and wpm < 30:
        severity = "CRITICAL"
        reasons.append(f"extremely low word density ({wpm:.0f} WPM, expected 100-200)")

    # CRITICAL: hallucination covers >50% of segments
    if hallucination and hallucination_ratio > 0.5:
        severity = "CRITICAL"
        reasons.append(
            f"hallucination covers {hallucination_ratio:.0%} of segments "
            f"(\"{hallucination['repeated_text']}\" x{hallucination['count']})"
        )

    # WARNING: minor hallucination (detected but covers <=50% of segments)
    if hallucination and hallucination_ratio <= 0.5 and severity == "OK":
        severity = "WARNING"
        reasons.append(
            f"minor hallucination artifact "
            f"(\"{hallucination['repeated_text']}\" x{hallucination['count']}, "
            f"{hallucination_ratio:.0%} of segments)"
        )

    # WARNING: intra-segment word repetition detected
    if intra_seg_issues and severity == "OK":
        severity = "WARNING"
        affected = len(intra_seg_issues)
        worst = max(intra_seg_issues, key=lambda x: x["count"])
        reasons.append(
            f"intra-segment word repetition in {affected} segment(s): "
            f"\"{worst['repeated_word']}\" x{worst['count']}"
        )

    return {
        "severity": severity,
        "reasons": reasons,
        "total_words": total_words,
        "wpm": round(wpm, 1),
        "hallucination_ratio": round(hallucination_ratio, 3),
    }


def _auto_device(preferred: str = "") -> str:
    preferred = (preferred or "").strip().lower()
    if preferred in {"cuda", "cpu"}:
        if preferred == "cuda":
            if torch is not None and torch.cuda.is_available():
                return "cuda"
            return "cpu"
        return "cpu"
    if torch is not None and torch.cuda.is_available():
        return "cuda"
    return "cpu"


@contextmanager
def heartbeat(label: str, interval_sec: float = 15.0) -> Iterator[None]:
    stop = threading.Event()
    start = time.monotonic()

    def _runner() -> None:
        while not stop.wait(interval_sec):
            elapsed = int(time.monotonic() - start)
            log(f"[02.transcribe] … {label}: still running ({elapsed}s)")

    t = threading.Thread(target=_runner, daemon=True)
    t.start()
    try:
        yield
    finally:
        stop.set()
        t.join(timeout=1.0)


def cleanup_after_engine_run(engine: Optional["BaseEngine"]) -> None:
    try:
        del engine
    except Exception:
        pass
    import gc
    gc.collect()
    if torch is not None and torch.cuda.is_available():
        try:
            torch.cuda.synchronize()
        except Exception:
            pass
        for fn in ("empty_cache", "ipc_collect"):
            try:
                getattr(torch.cuda, fn)()
            except Exception:
                pass


# --------------------------
# Output formats
# --------------------------

def write_txt(path: Path, segments: List[Dict[str, Any]]) -> None:
    text = " ".join([str(s.get("text", "")).strip() for s in segments if str(s.get("text", "")).strip()]).strip()
    path.write_text(text + ("\n" if text else ""), encoding="utf-8")


def _write_all_outputs(out_json_path: Path, segments: List[Dict[str, Any]]) -> None:
    out_json_path.parent.mkdir(parents=True, exist_ok=True)
    full_text = " ".join([str(s.get("text", "")).strip() for s in segments if str(s.get("text", "")).strip()]).strip()
    out_json_path.write_text(
        json.dumps({"text": full_text, "segments": segments}, ensure_ascii=False, indent=2) + "\n",
        encoding="utf-8",
    )
    base = out_json_path.with_suffix("")
    write_txt(base.with_name(base.name + ".txt"), segments)


# --------------------------
# Audio loading
# --------------------------

def wav_duration_sec_fast(path: Path) -> Optional[float]:
    try:
        import wave
        with wave.open(str(path), "rb") as wf:
            frames = wf.getnframes()
            rate = wf.getframerate()
            if rate <= 0:
                return None
            return float(frames) / float(rate)
    except Exception:
        return None


def load_audio_mono(path: str) -> Tuple[np.ndarray, int]:
    if torchaudio is not None and torch is not None:
        try:
            waveform, sr = torchaudio.load(path)
            waveform = waveform.to(torch.float32)
            if waveform.ndim == 2 and waveform.shape[0] > 1:
                waveform = waveform.mean(dim=0, keepdim=True)
            if waveform.ndim == 2:
                waveform = waveform.squeeze(0)
            y = waveform.detach().cpu().numpy().astype(np.float32)
            return y, int(sr)
        except Exception:
            pass

    if sf is not None:
        y, sr = sf.read(path, dtype="float32", always_2d=True)
        if y.shape[1] > 1:
            y = y.mean(axis=1, keepdims=True)
        y = y[:, 0].astype(np.float32)
        return y, int(sr)

    if librosa is not None:
        y, sr = librosa.load(path, sr=None, mono=True)
        return y.astype(np.float32), int(sr)

    raise SystemExit("No audio loader available. Install soundfile or librosa (or torchaudio).")


# --------------------------
# Segment normalization
# --------------------------

def _normalize_word_list(words: Any, duration_sec: float) -> Optional[List[Dict[str, Any]]]:
    if not isinstance(words, list):
        return None
    out: List[Dict[str, Any]] = []
    for w in words:
        if not isinstance(w, dict):
            continue
        word = str(w.get("word", "")).strip()
        if not word:
            continue
        ww: Dict[str, Any] = {"word": word}
        try:
            ws = float(w.get("start", -1.0))
            we = float(w.get("end", -1.0))
        except Exception:
            ws, we = -1.0, -1.0
        if ws >= 0.0 and we > ws:
            ws = max(0.0, min(ws, duration_sec))
            we = max(0.0, min(we, duration_sec))
            ww["start"] = _r3(ws)
            ww["end"] = _r3(we)
        out.append(ww)
    return out or None


def _segments_from_any(segments_in: Any, duration_sec: float) -> List[Dict[str, Any]]:
    if not isinstance(segments_in, list):
        return []
    out: List[Dict[str, Any]] = []
    for seg in segments_in:
        if not isinstance(seg, dict):
            continue
        txt = str(seg.get("text", "") or "").strip()
        if not txt:
            continue
        try:
            s = float(seg.get("start", 0.0))
            e = float(seg.get("end", 0.0))
        except Exception:
            continue
        s = max(0.0, min(s, duration_sec))
        e = max(0.0, min(e, duration_sec))
        if e <= s:
            continue
        o: Dict[str, Any] = {"start": _r3(s), "end": _r3(e), "text": txt}
        if "words" in seg:
            wn = _normalize_word_list(seg.get("words"), duration_sec)
            if wn:
                o["words"] = wn
        out.append(o)
    out.sort(key=lambda x: (float(x["start"]), float(x["end"])))
    return out


# --------------------------
# Transcription Engine
# --------------------------

class BaseEngine:
    name: str = "base"
    model_name: str = ""
    language: str = "en"
    device: str = "cpu"
    decode_options: Dict[str, Any] = {}

    def transcribe(self, audio_path: str) -> Dict[str, Any]:
        raise NotImplementedError


class FasterWhisperEngine(BaseEngine):
    """Transcription engine using faster-whisper with word timestamps."""
    name = "faster"

    def __init__(
        self,
        model_name: str,
        language: str,
        device: str,
        compute_type: str,
        beam_size: int,
        temperature: float,
        condition_on_previous_text: bool = True,
        vad_filter: bool = False,
    ):
        self.model_name = str(model_name)
        self.language = (language or "en").strip()
        self.device = _auto_device(device)
        self.compute_type = (compute_type or "").strip().lower() or ("int8_float16" if self.device == "cuda" else "int8")
        self.decode_options = {
            "beam_size": int(beam_size),
            "temperature": float(temperature),
            "condition_on_previous_text": bool(condition_on_previous_text),
            "vad_filter": bool(vad_filter),
            "compute_type": self.compute_type,
        }
        try:
            from faster_whisper import WhisperModel  # type: ignore
        except Exception as e:
            raise SystemExit(f"Missing faster-whisper. Install: pip install -U faster-whisper ({type(e).__name__}: {e})")
        self._model = WhisperModel(self.model_name, device=self.device, compute_type=self.compute_type)
        log(f"[02.transcribe] Loaded model: {self.model_name} on device={self.device}")

    def transcribe(self, audio_path: str) -> Dict[str, Any]:
        segments_out: List[Dict[str, Any]] = []
        segments, _info = self._model.transcribe(
            audio_path,
            language=self.language,
            task="transcribe",
            beam_size=int(self.decode_options["beam_size"]),
            temperature=float(self.decode_options["temperature"]),
            condition_on_previous_text=bool(self.decode_options["condition_on_previous_text"]),
            vad_filter=bool(self.decode_options["vad_filter"]),
            word_timestamps=True,  # Always get word-level timestamps
        )
        parts: List[str] = []
        for seg in segments:
            txt = (seg.text or "").strip()
            if not txt:
                continue
            seg_dict: Dict[str, Any] = {"start": float(seg.start), "end": float(seg.end), "text": txt}
            # Capture word-level timestamps
            if hasattr(seg, "words") and seg.words:
                seg_dict["words"] = [
                    {"word": w.word, "start": float(w.start), "end": float(w.end)}
                    for w in seg.words if w.word
                ]
            segments_out.append(seg_dict)
            parts.append(txt)
        return {"text": " ".join(parts).strip(), "segments": segments_out}


# --------------------------
# Audio selection
# --------------------------

def _pick_best_audio_in_video_dir(video_dir: Path, prefer: str = "raw") -> Optional[Path]:
    raw = sorted(video_dir.glob("*.audio.asr.raw16k.wav"))
    legacy = sorted(video_dir.glob("*.wav"))

    if prefer == "legacy" and legacy:
        return legacy[0]

    if raw:
        return raw[0]
    if legacy:
        return legacy[0]
    return None


# --------------------------
# Core runner
# --------------------------

def transcribe_full_file_with_engine(
    audio_path: str,
    out_json: str,
    engine: BaseEngine,
    progress_interval: float,
) -> Optional[Dict[str, Any]]:
    """
    Transcribe audio file and write output.

    Returns:
        None if successful, or a dict with hallucination info if repetition detected.
    """
    t0 = time.monotonic()

    y, sr = load_audio_mono(audio_path)
    duration_sec = float(len(y) / float(sr)) if sr > 0 else 0.0

    out_json_path = Path(out_json)
    log(
        f"[02.transcribe] START {engine.name} ({getattr(engine, 'model_name', '')}) "
        f"| device={getattr(engine, 'device', '')} | audio={Path(audio_path).name} | dur={duration_sec:.1f}s"
    )

    with heartbeat(f"{engine.name} decode", interval_sec=float(progress_interval)):
        result = engine.transcribe(audio_path)

    segments = _segments_from_any(result.get("segments", []) or [], duration_sec)

    seg_n = len(segments)
    last_end = float(segments[-1]["end"]) if segments else 0.0
    if duration_sec > 60 and last_end < 0.90 * duration_sec:
        log(f"[02.transcribe] WARN: transcript ends early (last_end={last_end:.1f}s vs dur={duration_sec:.1f}s)")

    # Check for repetition hallucination (same sentence repeated 3+ times)
    hallucination = _detect_repetition_hallucination(segments, min_repeats=3)
    if hallucination:
        log(f"[02.transcribe] FLAGGED: Repetition hallucination detected!")
        log(f"[02.transcribe]   Text: \"{hallucination['repeated_text'][:50]}...\"")
        log(f"[02.transcribe]   Count: {hallucination['count']} consecutive repetitions")
        log(f"[02.transcribe]   Segments: {hallucination['first_index']} to {hallucination['last_index']}")

    # Check for intra-segment word repetition (e.g., "yeah yeah yeah yeah yeah" within one segment)
    intra_seg_issues = _detect_intra_segment_repetition(segments)
    if intra_seg_issues:
        log(f"[02.transcribe] FLAGGED: Intra-segment word repetition detected!")
        for issue in intra_seg_issues:
            log(f"[02.transcribe]   Seg {issue['segment_index']}: "
                f"\"{issue['repeated_word']}\" x{issue['count']} "
                f"({issue['time_range'][0]:.1f}s-{issue['time_range'][1]:.1f}s)")

    # Classify transcript quality and severity
    quality = _classify_transcript_quality(segments, duration_sec, hallucination, intra_seg_issues)

    if quality["severity"] == "CRITICAL":
        for reason in quality["reasons"]:
            log(f"[02.transcribe] CRITICAL: {reason}")
        log(f"[02.transcribe] SKIPPED: output NOT written (transcript is unusable)")
        log(f"[02.transcribe]   words={quality['total_words']} wpm={quality['wpm']} halluc_ratio={quality['hallucination_ratio']}")
    elif quality["severity"] == "WARNING":
        for reason in quality["reasons"]:
            log(f"[02.transcribe] WARNING: {reason}")
        _write_all_outputs(out_json_path, segments)
    else:
        _write_all_outputs(out_json_path, segments)

    elapsed = time.monotonic() - t0
    wrote = "SKIPPED" if quality["severity"] == "CRITICAL" else "WROTE"
    log(f"[02.transcribe] DONE: segments={seg_n} last_end={last_end:.1f}s elapsed={elapsed:.1f}s {wrote} -> {out_json_path.name}")

    # Return quality info (includes hallucination data if present)
    return {"hallucination": hallucination, "quality": quality, "intra_segment_repetition": intra_seg_issues}


# --------------------------
# Batch runner
# --------------------------

def batch_for_source(
    source_name: str,
    youtube_url: str,
    overwrite: bool,
    prefer_audio: str,
    engine: BaseEngine,
    progress_interval: float,
    manifest_ids: Optional[Set[str]] = None,
) -> int:
    root = repo_root()
    safe_source = safe_name(source_name)

    downloads_root = root / "data" / "01.download" / safe_source
    out_root = root / "data" / "02.transcribe" / safe_source

    if not downloads_root.exists():
        raise SystemExit(f"[02.transcribe] Missing downloads folder: {downloads_root}")

    video_id = extract_video_id(youtube_url)
    video_dirs = sorted([p for p in downloads_root.iterdir() if p.is_dir()])
    if video_id:
        video_dirs = [d for d in video_dirs if f"[{video_id}]" in d.name]
    if manifest_ids:
        video_dirs = manifest_filter_dirs(video_dirs, manifest_ids)

    if not video_dirs:
        log(f"[02.transcribe] No video folders found under: {downloads_root}")
        return 0

    processed = 0
    skipped = 0
    failed = 0
    flagged_videos: List[Dict[str, Any]] = []  # Track hallucination flags

    for video_dir in video_dirs:
        try:
            audio_path = _pick_best_audio_in_video_dir(video_dir, prefer=prefer_audio)
            if audio_path is None:
                log(f"[02.transcribe] WARN: No audio found in: {video_dir}")
                continue

            out_video_dir = out_root / video_dir.name
            out_video_dir.mkdir(parents=True, exist_ok=True)

            out_json = out_video_dir / f"{video_dir.name}.full.json"

            if out_json.exists() and not overwrite:
                skipped += 1
                continue

            log(f"[02.transcribe] VIDEO: {video_dir.name}")
            log(f"[02.transcribe] AUDIO: {audio_path.name}")

            result = transcribe_full_file_with_engine(
                audio_path=str(audio_path),
                out_json=str(out_json),
                engine=engine,
                progress_interval=progress_interval,
            )
            processed += 1

            hallucination = result.get("hallucination") if result else None
            intra_seg = result.get("intra_segment_repetition") if result else None
            quality = result.get("quality", {}) if result else {}
            severity = quality.get("severity", "OK")

            # Track flagged videos (both WARNING and CRITICAL)
            if hallucination or intra_seg or severity in ("WARNING", "CRITICAL"):
                flag_entry: Dict[str, Any] = {
                    "video": video_dir.name,
                    "source": safe_source,
                    "severity": severity,
                    "reasons": quality.get("reasons", []),
                    "total_words": quality.get("total_words", 0),
                    "wpm": quality.get("wpm", 0),
                    "timestamp": now_iso(),
                }
                if hallucination:
                    flag_entry["reason"] = "repetition_hallucination"
                    flag_entry["repeated_text"] = hallucination["repeated_text"]
                    flag_entry["repeat_count"] = hallucination["count"]
                    flag_entry["segment_range"] = [hallucination["first_index"], hallucination["last_index"]]
                if intra_seg:
                    flag_entry["reason"] = flag_entry.get("reason", "intra_segment_repetition")
                    flag_entry["intra_segment_issues"] = intra_seg
                flagged_videos.append(flag_entry)

        except Exception as e:
            failed += 1
            log(f"[02.transcribe] ERROR: Failed on folder: {video_dir.name}")
            log(f"[02.transcribe]        {type(e).__name__}: {e}")
            continue

    # Write flagged videos to flag file for manual review
    if flagged_videos:
        flag_file = out_root / ".flagged.json"
        existing: List[Dict[str, Any]] = []
        if flag_file.exists():
            try:
                existing = json.loads(flag_file.read_text(encoding="utf-8"))
            except Exception:
                pass
        existing.extend(flagged_videos)
        flag_file.parent.mkdir(parents=True, exist_ok=True)
        flag_file.write_text(json.dumps(existing, indent=2, ensure_ascii=False) + "\n", encoding="utf-8")
        log(f"[02.transcribe] FLAGGED: {len(flagged_videos)} videos written to {flag_file}")

    log(f"[02.transcribe] Done: processed={processed} skipped={skipped} failed={failed} flagged={len(flagged_videos)}")
    return processed


# --------------------------
# CLI
# --------------------------

if __name__ == "__main__":
    p = argparse.ArgumentParser(
        description="Raw transcription with faster-whisper (large-v3).",
        formatter_class=argparse.RawDescriptionHelpFormatter,
    )

    p.add_argument("source_name", nargs="?", help="Source name (folder under data/01.download).")
    p.add_argument("youtube_url", nargs="?", help="YouTube URL. Video URL filters by ID.")

    p.add_argument("--sources", nargs="?", const="docs/sources.txt", help="Process all sources from sources file.")
    p.add_argument("--manifest", help="Manifest file: only process videos listed (docs/pipeline/batches/P001.txt).")
    p.add_argument("--audio", help="Single-file input path.")
    p.add_argument("--out", help="Single-file output .full.json.")

    p.add_argument("--overwrite", action="store_true", help="Overwrite existing outputs.")
    p.add_argument("--prefer-audio", choices=["raw", "legacy"], default="raw")

    p.add_argument("--progress-interval", type=float, default=float(os.environ.get("TRANSCRIBE_PROGRESS_INTERVAL", "15")))

    # Model options
    p.add_argument("--model", default=os.environ.get("FASTER_WHISPER_MODEL", "large-v3"),
                   help="Whisper model (default: large-v3)")
    p.add_argument("--device", default=os.environ.get("FASTER_WHISPER_DEVICE", ""))
    p.add_argument("--compute-type", default=os.environ.get("FASTER_WHISPER_COMPUTE_TYPE", ""))
    p.add_argument("--beam-size", type=int, default=int(os.environ.get("WHISPER_BEAM_SIZE", "3")))
    p.add_argument("--temperature", type=float, default=float(os.environ.get("WHISPER_TEMPERATURE", "0.0")))
    p.add_argument("--language", default=os.environ.get("WHISPER_LANGUAGE", "en"))
    p.add_argument("--vad-filter", action="store_true", help="Enable Silero VAD filter (default OFF).")
    p.add_argument("--condition-on-prev", action="store_true", help="Enable condition_on_previous_text (better caps, risk of hallucination).")

    args = p.parse_args()

    device = _auto_device(args.device)
    compute_type = (args.compute_type or "").strip().lower() or ("int8_float16" if device == "cuda" else "int8")
    condition_on_prev = args.condition_on_prev

    log(f"[02.transcribe] Model: {args.model} | device={device} | condition_on_previous_text={condition_on_prev}")

    # Initialize engine
    engine = FasterWhisperEngine(
        model_name=args.model,
        language=args.language,
        device=device,
        compute_type=compute_type,
        beam_size=args.beam_size,
        temperature=args.temperature,
        condition_on_previous_text=condition_on_prev,
        vad_filter=args.vad_filter,
    )

    # Single-file mode
    if args.audio:
        if not args.out:
            raise SystemExit("--audio requires --out")
        out_path = Path(args.out)
        if out_path.suffix.lower() != ".json":
            out_path = out_path.with_suffix(".json")
        if not out_path.name.endswith(".full.json"):
            out_path = out_path.with_name(out_path.stem + ".full.json")

        if out_path.exists() and not args.overwrite:
            log(f"[02.transcribe] SKIP: exists ({out_path.name})")
            raise SystemExit(0)

        out_path.parent.mkdir(parents=True, exist_ok=True)
        transcribe_full_file_with_engine(
            audio_path=str(args.audio),
            out_json=str(out_path),
            engine=engine,
            progress_interval=args.progress_interval,
        )
        cleanup_after_engine_run(engine)
        raise SystemExit(0)

    # Manifest batch mode
    if args.manifest:
        manifest_path = Path(args.manifest)
        if not manifest_path.is_absolute():
            manifest_path = repo_root() / manifest_path
        if not manifest_path.exists():
            raise SystemExit(f"Manifest file not found: {manifest_path}")
        sources_map = load_manifest_sources(manifest_path)
        total = 0
        for source_name, vid_ids in sorted(sources_map.items()):
            log(f"[02.transcribe] Manifest: {source_name} ({len(vid_ids)} videos)")
            total += batch_for_source(
                source_name=source_name,
                youtube_url="",
                overwrite=bool(args.overwrite),
                prefer_audio=str(args.prefer_audio),
                engine=engine,
                progress_interval=args.progress_interval,
                manifest_ids=vid_ids,
            )
        cleanup_after_engine_run(engine)
        log(f"[02.transcribe] ✅ MANIFEST DONE: total_processed={total}")
        raise SystemExit(0)

    # Batch sources file
    if args.sources is not None:
        sources_path = Path(args.sources)
        if not sources_path.is_absolute():
            sources_path = repo_root() / sources_path
        if not sources_path.exists():
            raise SystemExit(f"Sources file not found: {sources_path}")
        total = 0
        for source_name, youtube_url in parse_sources_file(sources_path):
            total += batch_for_source(
                source_name=source_name,
                youtube_url=youtube_url,
                overwrite=bool(args.overwrite),
                prefer_audio=str(args.prefer_audio),
                engine=engine,
                progress_interval=args.progress_interval,
            )
        cleanup_after_engine_run(engine)
        log(f"[02.transcribe] ✅ ALL SOURCES DONE: total_processed={total}")
        raise SystemExit(0)

    # Normal batch mode
    if not args.source_name or not args.youtube_url:
        raise SystemExit(
            "Provide either --audio/--out, --manifest, or --sources [file], or:\n"
            "./scripts/training-data/02.transcribe <source_name> <youtube_url>"
        )

    batch_for_source(
        source_name=str(args.source_name),
        youtube_url=str(args.youtube_url),
        overwrite=bool(args.overwrite),
        prefer_audio=str(args.prefer_audio),
        engine=engine,
        progress_interval=args.progress_interval,
    )
    cleanup_after_engine_run(engine)
