#!/usr/bin/env python3
"""
scripts/training-data/02.transcribe

STEP 2 — RAW TRANSCRIPTION (large-v3 only)

Usage:
  A) Single source:  ./02.transcribe "daily_evolution" "https://youtube.com/watch?v=..."
  B) Batch sources:  ./02.transcribe --sources docs/sources.txt
  C) Single file:    ./02.transcribe --audio path/to/audio.wav --out output.full.json

Transcription Model (UPDATED 03-02-2026):
  large-v3 with condition_on_previous_text=False (default)
    - Prevents hallucination loops (repeated phrases)
    - Uses raw audio to capture quieter voices (e.g., women's responses in infield)
    - Trade-off: Minor capitalization inconsistencies on proper nouns

  To enable condition_on_previous_text (better capitalization, risk of hallucination):
    ./02.transcribe --audio <path> --out <output> --condition-on-prev

Note: This script ONLY does transcription. Alignment and diarization are
      handled by subsequent scripts (03.align, 04.diarize).

Input:  data/01.download/<source>/<video>/*.audio.asr.raw16k.wav
Output: data/02.transcribe/<source>/<video>/<video>.full.json + .txt
"""

from __future__ import annotations

import argparse
import json
import os
import re
import shlex
import shutil
import subprocess
import sys
import threading
import time
from contextlib import contextmanager
from dataclasses import dataclass
from datetime import datetime
from pathlib import Path
from typing import Any, Callable, Dict, Iterator, List, Optional, Tuple

import numpy as np

# --------------------------
# Torch import
# --------------------------

try:
    import torch  # type: ignore
except Exception:
    torch = None

# Optional audio loaders
try:
    import torchaudio  # type: ignore
except Exception:
    torchaudio = None

try:
    import soundfile as sf  # type: ignore
except Exception:
    sf = None

try:
    import librosa  # type: ignore
except Exception:
    librosa = None


# --------------------------
# Small helpers
# --------------------------

def log(msg: str) -> None:
    print(msg, flush=True)


def repo_root() -> Path:
    return Path(__file__).resolve().parents[2]


def now_iso() -> str:
    return datetime.utcnow().replace(microsecond=0).isoformat() + "Z"


def safe_name(name: str) -> str:
    cleaned = re.sub(r"[^A-Za-z0-9._-]+", "_", (name or "").strip())
    return cleaned.strip("_") or "source"


def extract_video_id(url: str) -> Optional[str]:
    url = (url or "").strip()
    if not url:
        return None
    m = re.search(r"[?&]v=([^&]+)", url)
    if m:
        return m.group(1)
    m = re.search(r"youtu\.be/([^?&/]+)", url)
    if m:
        return m.group(1)
    return None


def parse_sources_file(path: Path) -> List[Tuple[str, str]]:
    out: List[Tuple[str, str]] = []
    for raw in path.read_text(encoding="utf-8").splitlines():
        line = raw.strip()
        if not line or line.startswith("#"):
            continue
        if "|" in line:
            name, url = line.split("|", 1)
            name = name.strip()
            url = url.strip()
            if name and url:
                out.append((name, url))
            continue
        parts = shlex.split(line)
        if len(parts) >= 2:
            out.append((parts[0], parts[1]))
    return out


def _r3(x: float) -> float:
    return float(np.round(float(x), 3))


def _detect_repetition_hallucination(
    segments: List[Dict[str, Any]],
    min_repeats: int = 3,
) -> Optional[Dict[str, Any]]:
    """
    Detect Whisper hallucination loops where the same sentence is repeated 3+ times.

    Returns None if no hallucination detected, or a dict with:
        - repeated_text: the text that was repeated
        - count: number of consecutive repetitions
        - first_index: index of first occurrence
        - last_index: index of last occurrence

    Note: Only flags when the SAME SENTENCE is repeated. Does NOT flag if
    the same word appears in different sentences.
    """
    if len(segments) < min_repeats:
        return None

    # Track consecutive repetitions
    prev_text = None
    repeat_count = 0
    first_idx = 0

    for i, seg in enumerate(segments):
        text = str(seg.get("text", "")).strip()
        if not text:
            continue

        if text == prev_text:
            repeat_count += 1
        else:
            # Check if we had enough repetitions before reset
            if repeat_count >= min_repeats:
                return {
                    "repeated_text": prev_text,
                    "count": repeat_count,
                    "first_index": first_idx,
                    "last_index": i - 1,
                }
            prev_text = text
            repeat_count = 1
            first_idx = i

    # Check final sequence
    if repeat_count >= min_repeats:
        return {
            "repeated_text": prev_text,
            "count": repeat_count,
            "first_index": first_idx,
            "last_index": len(segments) - 1,
        }

    return None


def _auto_device(preferred: str = "") -> str:
    preferred = (preferred or "").strip().lower()
    if preferred in {"cuda", "cpu"}:
        if preferred == "cuda":
            if torch is not None and torch.cuda.is_available():
                return "cuda"
            return "cpu"
        return "cpu"
    if torch is not None and torch.cuda.is_available():
        return "cuda"
    return "cpu"


@contextmanager
def heartbeat(label: str, interval_sec: float = 15.0) -> Iterator[None]:
    stop = threading.Event()
    start = time.monotonic()

    def _runner() -> None:
        while not stop.wait(interval_sec):
            elapsed = int(time.monotonic() - start)
            log(f"[02.transcribe] … {label}: still running ({elapsed}s)")

    t = threading.Thread(target=_runner, daemon=True)
    t.start()
    try:
        yield
    finally:
        stop.set()
        t.join(timeout=1.0)


def cleanup_after_engine_run(engine: Optional["BaseEngine"]) -> None:
    try:
        del engine
    except Exception:
        pass
    import gc
    gc.collect()
    if torch is not None and torch.cuda.is_available():
        try:
            torch.cuda.synchronize()
        except Exception:
            pass
        for fn in ("empty_cache", "ipc_collect"):
            try:
                getattr(torch.cuda, fn)()
            except Exception:
                pass


# --------------------------
# Output formats
# --------------------------

def write_txt(path: Path, segments: List[Dict[str, Any]]) -> None:
    text = " ".join([str(s.get("text", "")).strip() for s in segments if str(s.get("text", "")).strip()]).strip()
    path.write_text(text + ("\n" if text else ""), encoding="utf-8")


def _write_all_outputs(out_json_path: Path, segments: List[Dict[str, Any]]) -> None:
    out_json_path.parent.mkdir(parents=True, exist_ok=True)
    full_text = " ".join([str(s.get("text", "")).strip() for s in segments if str(s.get("text", "")).strip()]).strip()
    out_json_path.write_text(
        json.dumps({"text": full_text, "segments": segments}, ensure_ascii=False, indent=2) + "\n",
        encoding="utf-8",
    )
    base = out_json_path.with_suffix("")
    write_txt(base.with_name(base.name + ".txt"), segments)


# --------------------------
# Audio loading
# --------------------------

def wav_duration_sec_fast(path: Path) -> Optional[float]:
    try:
        import wave
        with wave.open(str(path), "rb") as wf:
            frames = wf.getnframes()
            rate = wf.getframerate()
            if rate <= 0:
                return None
            return float(frames) / float(rate)
    except Exception:
        return None


def load_audio_mono(path: str) -> Tuple[np.ndarray, int]:
    if torchaudio is not None and torch is not None:
        try:
            waveform, sr = torchaudio.load(path)
            waveform = waveform.to(torch.float32)
            if waveform.ndim == 2 and waveform.shape[0] > 1:
                waveform = waveform.mean(dim=0, keepdim=True)
            if waveform.ndim == 2:
                waveform = waveform.squeeze(0)
            y = waveform.detach().cpu().numpy().astype(np.float32)
            return y, int(sr)
        except Exception:
            pass

    if sf is not None:
        y, sr = sf.read(path, dtype="float32", always_2d=True)
        if y.shape[1] > 1:
            y = y.mean(axis=1, keepdims=True)
        y = y[:, 0].astype(np.float32)
        return y, int(sr)

    if librosa is not None:
        y, sr = librosa.load(path, sr=None, mono=True)
        return y.astype(np.float32), int(sr)

    raise SystemExit("No audio loader available. Install soundfile or librosa (or torchaudio).")


# --------------------------
# Segment normalization
# --------------------------

def _normalize_word_list(words: Any, duration_sec: float) -> Optional[List[Dict[str, Any]]]:
    if not isinstance(words, list):
        return None
    out: List[Dict[str, Any]] = []
    for w in words:
        if not isinstance(w, dict):
            continue
        word = str(w.get("word", "")).strip()
        if not word:
            continue
        ww: Dict[str, Any] = {"word": word}
        try:
            ws = float(w.get("start", -1.0))
            we = float(w.get("end", -1.0))
        except Exception:
            ws, we = -1.0, -1.0
        if ws >= 0.0 and we > ws:
            ws = max(0.0, min(ws, duration_sec))
            we = max(0.0, min(we, duration_sec))
            ww["start"] = _r3(ws)
            ww["end"] = _r3(we)
        out.append(ww)
    return out or None


def _segments_from_any(segments_in: Any, duration_sec: float) -> List[Dict[str, Any]]:
    if not isinstance(segments_in, list):
        return []
    out: List[Dict[str, Any]] = []
    for seg in segments_in:
        if not isinstance(seg, dict):
            continue
        txt = str(seg.get("text", "") or "").strip()
        if not txt:
            continue
        try:
            s = float(seg.get("start", 0.0))
            e = float(seg.get("end", 0.0))
        except Exception:
            continue
        s = max(0.0, min(s, duration_sec))
        e = max(0.0, min(e, duration_sec))
        if e <= s:
            continue
        o: Dict[str, Any] = {"start": _r3(s), "end": _r3(e), "text": txt}
        if "words" in seg:
            wn = _normalize_word_list(seg.get("words"), duration_sec)
            if wn:
                o["words"] = wn
        out.append(o)
    out.sort(key=lambda x: (float(x["start"]), float(x["end"])))
    return out


# --------------------------
# Transcription Engine
# --------------------------

class BaseEngine:
    name: str = "base"
    model_name: str = ""
    language: str = "en"
    device: str = "cpu"
    decode_options: Dict[str, Any] = {}

    def transcribe(self, audio_path: str) -> Dict[str, Any]:
        raise NotImplementedError


class FasterWhisperEngine(BaseEngine):
    """Transcription engine using faster-whisper with word timestamps."""
    name = "faster"

    def __init__(
        self,
        model_name: str,
        language: str,
        device: str,
        compute_type: str,
        beam_size: int,
        temperature: float,
        condition_on_previous_text: bool = True,
        vad_filter: bool = False,
    ):
        self.model_name = str(model_name)
        self.language = (language or "en").strip()
        self.device = _auto_device(device)
        self.compute_type = (compute_type or "").strip().lower() or ("int8_float16" if self.device == "cuda" else "int8")
        self.decode_options = {
            "beam_size": int(beam_size),
            "temperature": float(temperature),
            "condition_on_previous_text": bool(condition_on_previous_text),
            "vad_filter": bool(vad_filter),
            "compute_type": self.compute_type,
        }
        try:
            from faster_whisper import WhisperModel  # type: ignore
        except Exception as e:
            raise SystemExit(f"Missing faster-whisper. Install: pip install -U faster-whisper ({type(e).__name__}: {e})")
        self._model = WhisperModel(self.model_name, device=self.device, compute_type=self.compute_type)
        log(f"[02.transcribe] Loaded model: {self.model_name} on device={self.device}")

    def transcribe(self, audio_path: str) -> Dict[str, Any]:
        segments_out: List[Dict[str, Any]] = []
        segments, _info = self._model.transcribe(
            audio_path,
            language=self.language,
            task="transcribe",
            beam_size=int(self.decode_options["beam_size"]),
            temperature=float(self.decode_options["temperature"]),
            condition_on_previous_text=bool(self.decode_options["condition_on_previous_text"]),
            vad_filter=bool(self.decode_options["vad_filter"]),
            word_timestamps=True,  # Always get word-level timestamps
        )
        parts: List[str] = []
        for seg in segments:
            txt = (seg.text or "").strip()
            if not txt:
                continue
            seg_dict: Dict[str, Any] = {"start": float(seg.start), "end": float(seg.end), "text": txt}
            # Capture word-level timestamps
            if hasattr(seg, "words") and seg.words:
                seg_dict["words"] = [
                    {"word": w.word, "start": float(w.start), "end": float(w.end)}
                    for w in seg.words if w.word
                ]
            segments_out.append(seg_dict)
            parts.append(txt)
        return {"text": " ".join(parts).strip(), "segments": segments_out}


# --------------------------
# Audio selection
# --------------------------

def _pick_best_audio_in_video_dir(video_dir: Path, prefer: str = "raw") -> Optional[Path]:
    raw = sorted(video_dir.glob("*.audio.asr.raw16k.wav"))
    legacy = sorted(video_dir.glob("*.wav"))

    if prefer == "legacy" and legacy:
        return legacy[0]

    if raw:
        return raw[0]
    if legacy:
        return legacy[0]
    return None


# --------------------------
# Core runner
# --------------------------

def transcribe_full_file_with_engine(
    audio_path: str,
    out_json: str,
    engine: BaseEngine,
    progress_interval: float,
) -> Optional[Dict[str, Any]]:
    """
    Transcribe audio file and write output.

    Returns:
        None if successful, or a dict with hallucination info if repetition detected.
    """
    t0 = time.monotonic()

    y, sr = load_audio_mono(audio_path)
    duration_sec = float(len(y) / float(sr)) if sr > 0 else 0.0

    out_json_path = Path(out_json)
    log(
        f"[02.transcribe] START {engine.name} ({getattr(engine, 'model_name', '')}) "
        f"| device={getattr(engine, 'device', '')} | audio={Path(audio_path).name} | dur={duration_sec:.1f}s"
    )

    with heartbeat(f"{engine.name} decode", interval_sec=float(progress_interval)):
        result = engine.transcribe(audio_path)

    segments = _segments_from_any(result.get("segments", []) or [], duration_sec)

    seg_n = len(segments)
    last_end = float(segments[-1]["end"]) if segments else 0.0
    if duration_sec > 60 and last_end < 0.90 * duration_sec:
        log(f"[02.transcribe] WARN: transcript ends early (last_end={last_end:.1f}s vs dur={duration_sec:.1f}s)")

    # Check for repetition hallucination (same sentence repeated 3+ times)
    hallucination = _detect_repetition_hallucination(segments, min_repeats=3)
    if hallucination:
        log(f"[02.transcribe] FLAGGED: Repetition hallucination detected!")
        log(f"[02.transcribe]   Text: \"{hallucination['repeated_text'][:50]}...\"")
        log(f"[02.transcribe]   Count: {hallucination['count']} consecutive repetitions")
        log(f"[02.transcribe]   Segments: {hallucination['first_index']} to {hallucination['last_index']}")

    _write_all_outputs(out_json_path, segments)

    elapsed = time.monotonic() - t0
    log(f"[02.transcribe] DONE: segments={seg_n} last_end={last_end:.1f}s elapsed={elapsed:.1f}s -> {out_json_path.name}")

    return hallucination


# --------------------------
# Batch runner
# --------------------------

def batch_for_source(
    source_name: str,
    youtube_url: str,
    overwrite: bool,
    prefer_audio: str,
    engine: BaseEngine,
    progress_interval: float,
) -> int:
    root = repo_root()
    safe_source = safe_name(source_name)

    downloads_root = root / "data" / "01.download" / safe_source
    out_root = root / "data" / "02.transcribe" / safe_source

    if not downloads_root.exists():
        raise SystemExit(f"[02.transcribe] Missing downloads folder: {downloads_root}")

    video_id = extract_video_id(youtube_url)
    video_dirs = sorted([p for p in downloads_root.iterdir() if p.is_dir()])
    if video_id:
        video_dirs = [d for d in video_dirs if f"[{video_id}]" in d.name]

    if not video_dirs:
        log(f"[02.transcribe] No video folders found under: {downloads_root}")
        return 0

    processed = 0
    skipped = 0
    failed = 0
    flagged_videos: List[Dict[str, Any]] = []  # Track hallucination flags

    for video_dir in video_dirs:
        try:
            audio_path = _pick_best_audio_in_video_dir(video_dir, prefer=prefer_audio)
            if audio_path is None:
                log(f"[02.transcribe] WARN: No audio found in: {video_dir}")
                continue

            out_video_dir = out_root / video_dir.name
            out_video_dir.mkdir(parents=True, exist_ok=True)

            out_json = out_video_dir / f"{video_dir.name}.full.json"

            if out_json.exists() and not overwrite:
                skipped += 1
                continue

            log(f"[02.transcribe] VIDEO: {video_dir.name}")
            log(f"[02.transcribe] AUDIO: {audio_path.name}")

            hallucination = transcribe_full_file_with_engine(
                audio_path=str(audio_path),
                out_json=str(out_json),
                engine=engine,
                progress_interval=progress_interval,
            )
            processed += 1

            # Track hallucination flags
            if hallucination:
                flagged_videos.append({
                    "video": video_dir.name,
                    "source": safe_source,
                    "reason": "repetition_hallucination",
                    "repeated_text": hallucination["repeated_text"],
                    "repeat_count": hallucination["count"],
                    "segment_range": [hallucination["first_index"], hallucination["last_index"]],
                    "timestamp": now_iso(),
                })

        except Exception as e:
            failed += 1
            log(f"[02.transcribe] ERROR: Failed on folder: {video_dir.name}")
            log(f"[02.transcribe]        {type(e).__name__}: {e}")
            continue

    # Write flagged videos to flag file for manual review
    if flagged_videos:
        flag_file = out_root / ".flagged.json"
        existing: List[Dict[str, Any]] = []
        if flag_file.exists():
            try:
                existing = json.loads(flag_file.read_text(encoding="utf-8"))
            except Exception:
                pass
        existing.extend(flagged_videos)
        flag_file.parent.mkdir(parents=True, exist_ok=True)
        flag_file.write_text(json.dumps(existing, indent=2, ensure_ascii=False) + "\n", encoding="utf-8")
        log(f"[02.transcribe] FLAGGED: {len(flagged_videos)} videos written to {flag_file}")

    log(f"[02.transcribe] Done: processed={processed} skipped={skipped} failed={failed} flagged={len(flagged_videos)}")
    return processed


# --------------------------
# CLI
# --------------------------

if __name__ == "__main__":
    p = argparse.ArgumentParser(
        description="Raw transcription with faster-whisper (large-v3).",
        formatter_class=argparse.RawDescriptionHelpFormatter,
    )

    p.add_argument("source_name", nargs="?", help="Source name (folder under data/01.download).")
    p.add_argument("youtube_url", nargs="?", help="YouTube URL. Video URL filters by ID.")

    p.add_argument("--sources", nargs="?", const="docs/sources.txt", help="Process all sources from sources file.")
    p.add_argument("--audio", help="Single-file input path.")
    p.add_argument("--out", help="Single-file output .full.json.")

    p.add_argument("--overwrite", action="store_true", help="Overwrite existing outputs.")
    p.add_argument("--prefer-audio", choices=["raw", "legacy"], default="raw")

    p.add_argument("--progress-interval", type=float, default=float(os.environ.get("TRANSCRIBE_PROGRESS_INTERVAL", "15")))

    # Model options
    p.add_argument("--model", default=os.environ.get("FASTER_WHISPER_MODEL", "large-v3"),
                   help="Whisper model (default: large-v3)")
    p.add_argument("--device", default=os.environ.get("FASTER_WHISPER_DEVICE", ""))
    p.add_argument("--compute-type", default=os.environ.get("FASTER_WHISPER_COMPUTE_TYPE", ""))
    p.add_argument("--beam-size", type=int, default=int(os.environ.get("WHISPER_BEAM_SIZE", "3")))
    p.add_argument("--temperature", type=float, default=float(os.environ.get("WHISPER_TEMPERATURE", "0.0")))
    p.add_argument("--language", default=os.environ.get("WHISPER_LANGUAGE", "en"))
    p.add_argument("--vad-filter", action="store_true", help="Enable Silero VAD filter (default OFF).")
    p.add_argument("--condition-on-prev", action="store_true", help="Enable condition_on_previous_text (better caps, risk of hallucination).")

    args = p.parse_args()

    device = _auto_device(args.device)
    compute_type = (args.compute_type or "").strip().lower() or ("int8_float16" if device == "cuda" else "int8")
    condition_on_prev = args.condition_on_prev

    log(f"[02.transcribe] Model: {args.model} | device={device} | condition_on_previous_text={condition_on_prev}")

    # Initialize engine
    engine = FasterWhisperEngine(
        model_name=args.model,
        language=args.language,
        device=device,
        compute_type=compute_type,
        beam_size=args.beam_size,
        temperature=args.temperature,
        condition_on_previous_text=condition_on_prev,
        vad_filter=args.vad_filter,
    )

    # Single-file mode
    if args.audio:
        if not args.out:
            raise SystemExit("--audio requires --out")
        out_path = Path(args.out)
        if out_path.suffix.lower() != ".json":
            out_path = out_path.with_suffix(".json")
        if not out_path.name.endswith(".full.json"):
            out_path = out_path.with_name(out_path.stem + ".full.json")

        if out_path.exists() and not args.overwrite:
            log(f"[02.transcribe] SKIP: exists ({out_path.name})")
            raise SystemExit(0)

        out_path.parent.mkdir(parents=True, exist_ok=True)
        transcribe_full_file_with_engine(
            audio_path=str(args.audio),
            out_json=str(out_path),
            engine=engine,
            progress_interval=args.progress_interval,
        )
        cleanup_after_engine_run(engine)
        raise SystemExit(0)

    # Batch sources file
    if args.sources is not None:
        sources_path = Path(args.sources)
        if not sources_path.is_absolute():
            sources_path = repo_root() / sources_path
        if not sources_path.exists():
            raise SystemExit(f"Sources file not found: {sources_path}")
        total = 0
        for source_name, youtube_url in parse_sources_file(sources_path):
            total += batch_for_source(
                source_name=source_name,
                youtube_url=youtube_url,
                overwrite=bool(args.overwrite),
                prefer_audio=str(args.prefer_audio),
                engine=engine,
                progress_interval=args.progress_interval,
            )
        cleanup_after_engine_run(engine)
        log(f"[02.transcribe] ✅ ALL SOURCES DONE: total_processed={total}")
        raise SystemExit(0)

    # Normal batch mode
    if not args.source_name or not args.youtube_url:
        raise SystemExit(
            "Provide either --audio/--out, or --sources [file], or:\n"
            "./scripts/training-data/02.transcribe <source_name> <youtube_url>"
        )

    batch_for_source(
        source_name=str(args.source_name),
        youtube_url=str(args.youtube_url),
        overwrite=bool(args.overwrite),
        prefer_audio=str(args.prefer_audio),
        engine=engine,
        progress_interval=args.progress_interval,
    )
    cleanup_after_engine_run(engine)
