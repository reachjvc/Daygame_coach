#!/usr/bin/env python3
"""
scripts/training-data/06.conversations

Conversation Detection using Claude Code CLI

Three-pass architecture:
1. Video Type Classification - infield, talking_head, podcast, compilation
2. Speaker Labeling - SPEAKER_XX -> coach, target, voiceover, other
3. Conversation Boundaries - segment_type + conversation_id

Reads:
  - Audio feature files:
      data/05.audio-features/<source>/<video>/*.audio_features.json

Writes:
  - Conversation files:
      data/06.conversations/<source>/<video>/*.conversations.json

Use:

  A) Test videos:
     ./scripts/training-data/06.conversations --test

  B) Single file:
     ./scripts/training-data/06.conversations --input data/test/05.audio-features/video.audio_features.json

  C) Batch from sources file:
     ./scripts/training-data/06.conversations --sources

Requirements:
  - Claude Code CLI installed and authenticated (claude command available)
"""

from __future__ import annotations

import argparse
import hashlib
import json
import re
import shlex
import subprocess
import time
from dataclasses import dataclass, asdict
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple


# ---------------------------
# Configuration
# ---------------------------

SCHEMA_VERSION = "2.0.0"
PIPELINE_VERSION = "06.conversations-cli-v1"
PROMPT_VERSION = "2.0.0"

# Claude CLI binary path - try common locations
CLAUDE_BINARY_PATHS = [
    "claude",  # If in PATH
    Path.home() / ".vscode-server/extensions/anthropic.claude-code-2.1.17-linux-x64/resources/native-binary/claude",
    Path.home() / ".vscode/extensions/anthropic.claude-code-2.1.17-linux-x64/resources/native-binary/claude",
    "/usr/local/bin/claude",
]


# ---------------------------
# Prompts
# ---------------------------

VIDEO_TYPE_PROMPT = """You are classifying daygame coaching videos. Determine the PRIMARY content type.

VIDEO TYPES:

1. "infield" - Coach DOING live approaches on the street
   - Live interaction with women (not hypothetical)
   - Real-time responses from targets
   - Street/shopping/park environment implied
   - Nervous energy, real rejection/acceptance
   - Incomplete thoughts, interruptions, ambient noise

2. "talking_head" - Coach EXPLAINING concepts to camera
   - Educational/instructional tone
   - No second party responding
   - Abstract examples, theory discussion
   - "Guys", "everyone", "you should"
   - Clean, complete sentences

3. "podcast" - Multiple speakers DISCUSSING topics
   - Back-and-forth dialogue about theory
   - Named co-hosts or guests
   - Interview-style questions
   - Both speakers have long turns

4. "compilation" - Mixed content types
   - Shifts between infield and commentary
   - Multiple approaches with breakdowns between
   - "As you saw..." followed by explanation

VIDEO TITLE: "{title}"

SAMPLE SEGMENTS:
{samples}

OUTPUT: Return ONLY a JSON object with this exact structure:
{{"type": "infield|talking_head|podcast|compilation", "confidence": 0.0-1.0, "reasoning": "brief explanation"}}
"""

SPEAKER_LABELING_PROMPT = """You are labeling speakers in a daygame video based on speech patterns.

SPEAKER ROLES:

1. "coach" - The person teaching/demonstrating
   - Opens conversations (excuse me, quick question...)
   - Asks personal questions (name, origin, occupation)
   - Gives compliments
   - Longer, confident statements
   - Commentary to camera between approaches
   - May reference teaching ("guys", "as you can see")

2. "target" - Women being approached
   - Responds to questions (short answers initially)
   - Asked about herself (not asking)
   - Gives name when asked
   - May laugh, show surprise, hesitation
   - Typically shorter turns than coach

3. "voiceover" - Post-production narration
   - Instructional tone disconnected from live action
   - No back-and-forth dialogue
   - Perfect sentences (not fragmented)
   - "Notice how he..." or "Watch what happens..."

4. "other" - Background voices, friends, staff
   - Brief interjections
   - Not involved in approach

5. "unknown" - Cannot determine with confidence
   - Mark as unknown if unsure
   - Flag for manual review

CRITICAL RULES:
1. The person who delivers approach OPENERS ("excuse me", "I just saw you", "you looked really cute") is ALWAYS the coach, NEVER the target
2. The person giving SHORT RESPONSES to questions is typically the target
3. If unsure, mark as "unknown" with low confidence - DO NOT GUESS
4. Consider speech patterns across ALL samples, not just one or two

HANDLING DIARIZATION CONFUSION:
If one speaker ID shows BOTH coach AND target patterns (opens AND gives short responses),
this indicates pyannote merged two speakers. In this case:
- Label as "unknown" with confidence 0.3
- Add reasoning "mixed_diarization_error"

SPEAKERS TO LABEL: {speakers}

FULL TRANSCRIPT BY SPEAKER:
{transcript}

OUTPUT: Return ONLY a JSON object with this exact structure:
{{"speaker_labels": {{"SPEAKER_XX": {{"role": "coach|target|voiceover|other|unknown", "confidence": 0.0-1.0, "reasoning": "brief explanation"}}}}}}
"""

CONVERSATION_BOUNDARY_PROMPT = """You are detecting conversation boundaries in infield daygame footage.

SEGMENT TYPES:

1. "approach" - Part of live interaction with a woman
   - Includes: opener, small talk, flirting, number close
   - Gets non-zero conversation_id

2. "commentary" - Coach talking to camera (not to woman)
   - Pre-approach setup, post-approach breakdown
   - conversation_id: 0

3. "transition" - Brief moment between content
   - Walking, repositioning, audio gaps
   - conversation_id: 0

NEW CONVERSATION STARTS WHEN:
- Direct address to new person (excuse me, sorry, quick question)
- Change from commentary to interpersonal dialogue
- Location/context shift implied
- Previous approach clearly ended (goodbye, number, rejection)

SAME CONVERSATION CONTINUES WHEN:
- Same conversational thread
- Questions followed by relevant answers
- Continuous interaction without camera break

APPROACH ENDS WHEN:
- Number exchange ("text you", "send you a message")
- Explicit goodbye
- Rejection ("I have a boyfriend", walking away)
- Coach pivots to camera commentary

RULES:
- When uncertain: default to commentary (safer)
- conversation_id must be sequential (1, 2, 3...)
- Segments with same conversation_id must be contiguous
- Return EXACTLY {segment_count} segments

SPEAKER ROLES FOR CONTEXT:
{speaker_roles}

SEGMENTS TO CLASSIFY (id: speaker_role: text):
{segments}

OUTPUT: Return ONLY a JSON object with this exact structure:
{{"segments": [{{"id": 0, "segment_type": "approach|commentary|transition", "conversation_id": 0, "is_conversation_start": false}}, ...]}}
"""


# ---------------------------
# State Management
# ---------------------------

@dataclass
class ProcessingState:
    version: int
    completed_files: List[str]
    in_progress: Optional[str]
    failures: List[Dict[str, str]]


def load_state(state_path: Path) -> ProcessingState:
    """Load processing state from file."""
    if state_path.exists():
        try:
            data = json.loads(state_path.read_text())
            return ProcessingState(
                version=data.get("version", 1),
                completed_files=data.get("completed_files", []),
                in_progress=data.get("in_progress"),
                failures=data.get("failures", []),
            )
        except (json.JSONDecodeError, KeyError):
            pass
    return ProcessingState(version=1, completed_files=[], in_progress=None, failures=[])


def save_state(state_path: Path, state: ProcessingState) -> None:
    """Save processing state to file."""
    state_path.parent.mkdir(parents=True, exist_ok=True)
    state_path.write_text(json.dumps(asdict(state), indent=2))


# ---------------------------
# Claude CLI Interface
# ---------------------------

def find_claude_binary() -> Optional[str]:
    """Find the Claude CLI binary."""
    for path in CLAUDE_BINARY_PATHS:
        path = Path(path)
        if path.exists() and path.is_file():
            return str(path)
        # Also check if it's in PATH
        if str(path) == "claude":
            try:
                result = subprocess.run(["which", "claude"], capture_output=True, text=True)
                if result.returncode == 0:
                    return "claude"
            except Exception:
                pass
    return None


def call_claude(prompt: str, retries: int = 3, timeout: int = 300) -> Optional[str]:
    """Call Claude Code CLI with retry logic."""
    claude_bin = find_claude_binary()
    if not claude_bin:
        print("[06.conversations] Error: Claude CLI binary not found")
        return None

    for attempt in range(retries):
        try:
            result = subprocess.run(
                [claude_bin, "-p", prompt, "--output-format", "text"],
                capture_output=True,
                text=True,
                timeout=timeout
            )
            if result.returncode == 0:
                return result.stdout.strip()
            else:
                if attempt < retries - 1:
                    wait = 2 ** attempt
                    print(f"[06.conversations] Claude CLI error, retrying in {wait}s...")
                    print(f"[06.conversations]   stderr: {result.stderr[:200]}")
                    time.sleep(wait)
                    continue
                print(f"[06.conversations] Claude CLI error: {result.stderr[:500]}")
        except subprocess.TimeoutExpired:
            if attempt < retries - 1:
                print(f"[06.conversations] Timeout, retrying...")
                time.sleep(2 ** attempt)
                continue
            print(f"[06.conversations] Claude CLI timeout after {timeout}s")
        except FileNotFoundError:
            print("[06.conversations] Error: 'claude' command not found. Install Claude Code CLI.")
            return None
    return None


def parse_json_response(response: str) -> Optional[Dict]:
    """Parse JSON object from LLM response."""
    if not response:
        return None

    try:
        # Try to find JSON in code block
        code_block_match = re.search(r"```(?:json)?\s*(\{[\s\S]*?\})\s*```", response)
        if code_block_match:
            return json.loads(code_block_match.group(1))

        # Try to find raw JSON object
        start = response.find("{")
        if start != -1:
            bracket_count = 0
            for i, char in enumerate(response[start:], start):
                if char == "{":
                    bracket_count += 1
                elif char == "}":
                    bracket_count -= 1
                    if bracket_count == 0:
                        json_str = response[start:i + 1]
                        return json.loads(json_str)
    except (json.JSONDecodeError, ValueError) as e:
        print(f"[06.conversations] JSON parse error: {e}")
        print(f"[06.conversations] Response preview: {response[:500]}...")

    return None


# ---------------------------
# Pass 1: Video Type Classification
# ---------------------------

def classify_video_type(title: str, segments: List[Dict]) -> Dict[str, Any]:
    """Classify video type using Claude CLI."""

    # Sample segments: first 5, middle 5, last 5
    sample_indices = []
    for i in range(min(5, len(segments))):
        sample_indices.append(i)

    if len(segments) > 10:
        mid = len(segments) // 2
        for i in range(mid - 2, mid + 3):
            if 0 <= i < len(segments):
                sample_indices.append(i)

    if len(segments) > 15:
        for i in range(len(segments) - 5, len(segments)):
            sample_indices.append(i)

    unique_indices = sorted(set(sample_indices))[:15]

    samples = "\n".join([
        f"[{i}] {segments[i].get('text', '')[:100]}"
        for i in unique_indices
    ])

    prompt = VIDEO_TYPE_PROMPT.format(title=title, samples=samples)

    print("[06.conversations] Pass 1: Classifying video type...")
    response = call_claude(prompt, timeout=120)

    result = parse_json_response(response)
    if result and "type" in result:
        print(f"[06.conversations]   Type: {result['type']} ({result.get('confidence', 0) * 100:.0f}%)")
        return result

    # Fallback
    print("[06.conversations]   WARNING: Could not classify, defaulting to 'compilation'")
    return {"type": "compilation", "confidence": 0.5, "reasoning": "Classification failed, defaulting"}


# ---------------------------
# Pass 2: Speaker Labeling
# ---------------------------

def label_speakers(segments: List[Dict], video_type: str) -> Dict[str, Dict[str, Any]]:
    """Label speakers using Claude CLI."""

    # For talking_head, single speaker is coach
    speakers = list(set(seg.get("pyannote_speaker", "UNKNOWN") for seg in segments))

    if video_type == "talking_head":
        print("[06.conversations] Pass 2: Skipping speaker labeling for talking_head")
        return {
            speaker: {"role": "coach", "confidence": 0.95, "reasoning": "Single speaker in talking_head video"}
            for speaker in speakers
        }

    # Build full transcript by speaker (not just first 50!)
    speaker_transcripts: Dict[str, List[str]] = {s: [] for s in speakers}
    for i, seg in enumerate(segments):
        speaker = seg.get("pyannote_speaker", "UNKNOWN")
        text = seg.get("text", "").strip()
        if text:
            speaker_transcripts[speaker].append(f"[{i}] \"{text}\"")

    transcript_text = "\n\n".join([
        f"=== {speaker} ({len(lines)} utterances) ===\n" + "\n".join(lines)
        for speaker, lines in speaker_transcripts.items()
    ])

    prompt = SPEAKER_LABELING_PROMPT.format(
        speakers=", ".join(speakers),
        transcript=transcript_text
    )

    print(f"[06.conversations] Pass 2: Labeling {len(speakers)} speakers...")
    response = call_claude(prompt, timeout=180)

    result = parse_json_response(response)
    if result and "speaker_labels" in result:
        labels = result["speaker_labels"]

        # Ensure all speakers have labels
        for speaker in speakers:
            if speaker not in labels:
                labels[speaker] = {"role": "unknown", "confidence": 0.3, "reasoning": "Not labeled by LLM"}

        # Log results
        for speaker, label in labels.items():
            conf = label.get("confidence", 0) * 100
            print(f"[06.conversations]   {speaker}: {label.get('role')} ({conf:.0f}%) - {label.get('reasoning', '')[:50]}")

        return labels

    # Fallback - all unknown
    print("[06.conversations]   WARNING: Speaker labeling failed, marking all as unknown")
    return {
        speaker: {"role": "unknown", "confidence": 0, "reasoning": "LLM labeling failed"}
        for speaker in speakers
    }


# ---------------------------
# Pass 3: Conversation Boundaries
# ---------------------------

def detect_conversations(
    segments: List[Dict],
    speaker_labels: Dict[str, Dict[str, Any]],
    video_type: str
) -> List[Dict[str, Any]]:
    """Detect conversation boundaries using Claude CLI."""

    # For talking_head/podcast, all is commentary
    if video_type in ("talking_head", "podcast"):
        print("[06.conversations] Pass 3: Skipping boundary detection for non-infield content")
        return [
            {"id": i, "segment_type": "commentary", "conversation_id": 0, "is_conversation_start": False}
            for i in range(len(segments))
        ]

    # Build segment list with speaker roles
    speaker_roles_text = "\n".join([
        f"  {speaker}: {label.get('role', 'unknown')}"
        for speaker, label in speaker_labels.items()
    ])

    segments_text = "\n".join([
        f"[{i}] {speaker_labels.get(seg.get('pyannote_speaker', 'UNKNOWN'), {}).get('role', 'unknown').upper()}: {seg.get('text', '')[:80]}"
        for i, seg in enumerate(segments)
    ])

    prompt = CONVERSATION_BOUNDARY_PROMPT.format(
        segment_count=len(segments),
        speaker_roles=speaker_roles_text,
        segments=segments_text
    )

    print(f"[06.conversations] Pass 3: Detecting boundaries in {len(segments)} segments...")
    response = call_claude(prompt, timeout=300)

    result = parse_json_response(response)
    if result and "segments" in result:
        classifications = result["segments"]

        # Validate count
        if len(classifications) != len(segments):
            print(f"[06.conversations]   WARNING: Got {len(classifications)} classifications for {len(segments)} segments")
            # Pad or truncate
            while len(classifications) < len(segments):
                classifications.append({
                    "id": len(classifications),
                    "segment_type": "commentary",
                    "conversation_id": 0,
                    "is_conversation_start": False
                })
            classifications = classifications[:len(segments)]

        # Count conversations
        conv_ids = set(c.get("conversation_id", 0) for c in classifications if c.get("conversation_id", 0) > 0)
        print(f"[06.conversations]   Found {len(conv_ids)} conversations")

        return classifications

    # Fallback - all commentary
    print("[06.conversations]   WARNING: Boundary detection failed, marking all as commentary")
    return [
        {"id": i, "segment_type": "commentary", "conversation_id": 0, "is_conversation_start": False}
        for i in range(len(segments))
    ]


# ---------------------------
# Validation
# ---------------------------

def validate_results(
    video_type: Dict[str, Any],
    speaker_labels: Dict[str, Dict[str, Any]],
    classifications: List[Dict[str, Any]]
) -> List[str]:
    """Validate results and return list of review flags."""
    flags = []

    # Check for unknown speakers
    unknown_speakers = [
        (speaker, label)
        for speaker, label in speaker_labels.items()
        if label.get("role") == "unknown" or label.get("confidence", 0) < 0.5
    ]

    if unknown_speakers:
        unknown_ratio = len(unknown_speakers) / len(speaker_labels)
        if unknown_ratio > 0.5:
            flags.append(f"speakers_unknown_{len(unknown_speakers)}_of_{len(speaker_labels)}")
            print(f"[06.conversations] FLAG: {len(unknown_speakers)}/{len(speaker_labels)} speakers unknown - NEEDS MANUAL REVIEW")

    # Check for 0 conversations in infield/compilation
    conv_ids = set(c.get("conversation_id", 0) for c in classifications if c.get("conversation_id", 0) > 0)
    vtype = video_type.get("type", "")

    if vtype in ("infield", "compilation") and len(conv_ids) == 0:
        flags.append("zero_conversations_in_infield")
        print(f"[06.conversations] FLAG: 0 conversations detected in {vtype} video - NEEDS MANUAL REVIEW")

    return flags


# ---------------------------
# Main Processing
# ---------------------------

def extract_video_title(filename: str) -> str:
    """Extract video title from filename."""
    name = Path(filename).stem
    name = re.sub(r"\.(audio_features|conversations)$", "", name)
    match = re.match(r"^(.+?)\s*\[", name)
    return match.group(1).strip() if match else name


def extract_video_id(filename: str) -> str:
    """Extract video ID from filename."""
    match = re.search(r"\[([^\]]+)\]", filename)
    return match.group(1) if match else Path(filename).stem


def compute_checksum(data: Any) -> str:
    """Compute checksum of data."""
    return hashlib.sha256(json.dumps(data, sort_keys=True).encode()).hexdigest()[:16]


def process_file(input_path: Path, output_path: Path, dry_run: bool = False) -> Dict[str, Any]:
    """Process a single audio_features.json file."""

    print(f"\n[06.conversations] Processing: {input_path.name}")

    # Load input
    with input_path.open("r", encoding="utf-8") as f:
        data = json.load(f)

    segments = data.get("segments", [])
    video_title = extract_video_title(str(input_path))
    video_id = extract_video_id(str(input_path))

    print(f"[06.conversations]   Video: \"{video_title}\" [{video_id}]")
    print(f"[06.conversations]   Segments: {len(segments)}")

    if not segments:
        print("[06.conversations]   No segments found")
        return {"video_type": None, "conversations": 0, "flags": ["no_segments"]}

    if dry_run:
        print("[06.conversations]   [DRY RUN] Would process this file")
        return {"video_type": None, "conversations": 0, "flags": []}

    start_time = time.time()
    llm_calls = 0

    # Pass 1: Video Type Classification
    video_type = classify_video_type(video_title, segments)
    llm_calls += 1

    # Pass 2: Speaker Labeling
    speaker_labels = label_speakers(segments, video_type.get("type", "compilation"))
    if video_type.get("type") != "talking_head":
        llm_calls += 1

    # Pass 3: Conversation Boundaries
    classifications = detect_conversations(segments, speaker_labels, video_type.get("type", "compilation"))
    if video_type.get("type") not in ("talking_head", "podcast"):
        llm_calls += 1

    # Validate
    review_flags = validate_results(video_type, speaker_labels, classifications)

    # Build output segments
    output_segments = []
    for i, seg in enumerate(segments):
        classification = classifications[i] if i < len(classifications) else {
            "segment_type": "commentary",
            "conversation_id": 0,
            "is_conversation_start": False
        }

        speaker_id = seg.get("pyannote_speaker", "UNKNOWN")
        speaker_role = speaker_labels.get(speaker_id, {}).get("role", "unknown")

        output_segments.append({
            "id": i,
            "start": seg.get("start", 0),
            "end": seg.get("end", 0),
            "text": seg.get("text", ""),
            "speaker_id": speaker_id,
            "speaker_role": speaker_role,
            "segment_type": classification.get("segment_type", "commentary"),
            "conversation_id": classification.get("conversation_id", 0),
            "is_conversation_start": classification.get("is_conversation_start", False),
        })

    # Build conversation summaries
    conversations = []
    conv_segments: Dict[int, List[Dict]] = {}
    for seg in output_segments:
        conv_id = seg["conversation_id"]
        if conv_id > 0:
            if conv_id not in conv_segments:
                conv_segments[conv_id] = []
            conv_segments[conv_id].append(seg)

    for conv_id, segs in sorted(conv_segments.items()):
        conversations.append({
            "conversation_id": conv_id,
            "segment_ids": [s["id"] for s in segs],
            "start_time": segs[0]["start"],
            "end_time": segs[-1]["end"],
        })

    elapsed = time.time() - start_time

    # Build output
    output = {
        "video_id": video_id,
        "source_file": str(input_path),
        "processed_at": time.strftime("%Y-%m-%dT%H:%M:%SZ"),
        "video_type": {
            "type": video_type.get("type"),
            "confidence": video_type.get("confidence"),
            "method": "claude_cli",
            "reasoning": video_type.get("reasoning"),
        },
        "speaker_labels": speaker_labels,
        "segments": output_segments,
        "conversations": conversations,
        "review_flags": review_flags if review_flags else None,
        "metadata": {
            "pipeline_version": PIPELINE_VERSION,
            "prompt_version": PROMPT_VERSION,
            "schema_version": SCHEMA_VERSION,
            "input_checksum": compute_checksum(data),
            "llm_calls": llm_calls,
            "processing_time_sec": elapsed,
        },
    }

    # Write output
    output_path.parent.mkdir(parents=True, exist_ok=True)
    with output_path.open("w", encoding="utf-8") as f:
        json.dump(output, f, indent=2, ensure_ascii=False)

    # Summary
    type_counts: Dict[str, int] = {}
    for seg in output_segments:
        st = seg["segment_type"]
        type_counts[st] = type_counts.get(st, 0) + 1

    print(f"[06.conversations] Results:")
    print(f"  Video type: {video_type.get('type')}")
    print(f"  Conversations: {len(conversations)}")
    print(f"  Segment types: {type_counts}")
    print(f"  LLM calls: {llm_calls}")
    print(f"  Time: {elapsed:.1f}s")
    print(f"  Output: {output_path}")

    return {
        "video_type": video_type.get("type"),
        "conversations": len(conversations),
        "flags": review_flags,
    }


# ---------------------------
# Path helpers
# ---------------------------

def repo_root() -> Path:
    return Path(__file__).resolve().parents[2]


def input_root() -> Path:
    return repo_root() / "data" / "05.audio-features"


def output_root() -> Path:
    return repo_root() / "data" / "06.conversations"


def test_input_root() -> Path:
    return repo_root() / "data" / "test" / "05.audio-features"


def test_output_root() -> Path:
    return repo_root() / "data" / "test" / "06.conversations"


def compute_output_path(input_path: Path, output_dir: Path) -> Path:
    """Compute output path from input path."""
    stem = input_path.stem
    if stem.endswith(".audio_features"):
        stem = stem[:-len(".audio_features")]
    return output_dir / f"{stem}.conversations.json"


def parse_sources_file(path: Path) -> List[Tuple[str, str]]:
    """Parse sources.txt file."""
    sources: List[Tuple[str, str]] = []
    for raw in path.read_text(encoding="utf-8").splitlines():
        line = raw.strip()
        if not line or line.startswith("#"):
            continue
        if "|" in line:
            name, url = line.split("|", 1)
            sources.append((name.strip(), url.strip()))
            continue
        parts = shlex.split(line)
        if len(parts) >= 2:
            sources.append((parts[0], parts[1]))
    return sources


def find_input_files(in_dir: Path) -> List[Path]:
    """Find all audio_features JSON files in directory."""
    return sorted(in_dir.rglob("*.audio_features.json"))


# ---------------------------
# CLI
# ---------------------------

def main() -> None:
    parser = argparse.ArgumentParser(
        description="Detect conversations in daygame videos using Claude CLI"
    )
    parser.add_argument(
        "--input",
        help="Input .audio_features.json file or directory"
    )
    parser.add_argument(
        "--output",
        help="Output directory (defaults to data/06.conversations/)"
    )
    parser.add_argument(
        "--test",
        action="store_true",
        help="Process test videos (data/test/05.audio-features/)"
    )
    parser.add_argument(
        "--sources",
        nargs="?",
        const="docs/sources.txt",
        help="Process all sources from sources.txt file"
    )
    parser.add_argument(
        "--dry-run",
        action="store_true",
        help="Preview what would be processed"
    )
    parser.add_argument(
        "--overwrite",
        action="store_true",
        help="Overwrite existing output files"
    )

    args = parser.parse_args()

    # Test Claude CLI availability
    claude_bin = find_claude_binary()
    if not claude_bin:
        print("[06.conversations] Error: Claude CLI binary not found")
        print("[06.conversations] Searched paths:")
        for p in CLAUDE_BINARY_PATHS:
            print(f"  - {p}")
        print("[06.conversations] Install Claude Code CLI: https://claude.ai/code")
        return

    try:
        result = subprocess.run(
            [claude_bin, "--version"],
            capture_output=True,
            text=True,
            timeout=10
        )
        if result.returncode != 0:
            print("[06.conversations] Warning: Claude CLI not responding properly")
        else:
            print(f"[06.conversations] Using Claude CLI: {claude_bin}")
    except subprocess.TimeoutExpired:
        print("[06.conversations] Warning: Claude CLI slow to respond")

    # Determine input/output paths
    if args.test:
        in_dir = test_input_root()
        out_dir = test_output_root()
    elif args.input:
        input_path = Path(args.input)
        if not input_path.exists():
            input_path = repo_root() / args.input
        if not input_path.exists():
            raise SystemExit(f"Input not found: {args.input}")

        if input_path.is_file():
            # Single file mode
            out_dir = Path(args.output) if args.output else output_root()
            output_path = compute_output_path(input_path, out_dir)

            if output_path.exists() and not args.overwrite:
                print(f"[06.conversations] Output exists, skipping: {output_path}")
                return

            result = process_file(input_path, output_path, dry_run=args.dry_run)
            print(f"\n[06.conversations] Done. Found {result['conversations']} conversations")
            return

        in_dir = input_path
        out_dir = Path(args.output) if args.output else output_root()
    elif args.sources:
        sources_path = repo_root() / args.sources
        if not sources_path.exists():
            raise SystemExit(f"Sources file not found: {sources_path}")

        total_convs = 0
        total_files = 0

        for src_name, _ in parse_sources_file(sources_path):
            src_in_dir = input_root() / src_name
            if not src_in_dir.exists():
                print(f"[06.conversations] Skipping {src_name}: no 05.audio-features output")
                continue

            src_out_dir = output_root() / src_name
            files = find_input_files(src_in_dir)

            for input_file in files:
                output_file = compute_output_path(input_file, src_out_dir)
                if output_file.exists() and not args.overwrite:
                    continue
                result = process_file(input_file, output_file, dry_run=args.dry_run)
                total_convs += result["conversations"]
                total_files += 1

        print(f"\n[06.conversations] Done. Processed {total_files} files, found {total_convs} conversations")
        return
    else:
        raise SystemExit("Provide --input, --test, or --sources")

    # Directory mode
    files = find_input_files(in_dir)
    if not files:
        print(f"[06.conversations] No .audio_features.json files found in: {in_dir}")
        return

    print(f"[06.conversations] Input : {in_dir}")
    print(f"[06.conversations] Output: {out_dir}")
    print(f"[06.conversations] Files : {len(files)}")

    # Load state for checkpointing
    state_path = out_dir / ".conversations_state.json"
    state = load_state(state_path)

    total_convs = 0
    processed = 0
    skipped = 0

    for input_file in files:
        file_key = str(input_file.relative_to(in_dir))

        # Skip if already completed
        if file_key in state.completed_files and not args.overwrite:
            skipped += 1
            continue

        output_file = compute_output_path(input_file, out_dir)

        if output_file.exists() and not args.overwrite:
            skipped += 1
            state.completed_files.append(file_key)
            save_state(state_path, state)
            continue

        # Mark as in progress
        state.in_progress = file_key
        save_state(state_path, state)

        try:
            result = process_file(input_file, output_file, dry_run=args.dry_run)
            total_convs += result["conversations"]
            processed += 1

            # Mark as completed
            if not args.dry_run:
                state.completed_files.append(file_key)
                state.in_progress = None
                save_state(state_path, state)
        except Exception as e:
            print(f"[06.conversations] Error processing {input_file}: {e}")
            state.failures.append({"file": file_key, "error": str(e)})
            state.in_progress = None
            save_state(state_path, state)

    print(f"\n[06.conversations] Done.")
    print(f"  Processed: {processed}")
    print(f"  Skipped  : {skipped}")
    print(f"  Conversations found: {total_convs}")


if __name__ == "__main__":
    main()
