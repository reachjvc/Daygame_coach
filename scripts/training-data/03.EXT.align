#!/usr/bin/env python3
"""
scripts/training-data/03.EXT.align

STEP 3 — SENTENCE-LEVEL ALIGNMENT (whisperx.align)

Usage:
  A) Single source:  ./03.EXT.align "daily_evolution" "https://youtube.com/watch?v=..."
  B) Batch sources:  ./03.EXT.align --sources docs/pipeline/sources.txt
  C) Single file:    ./03.EXT.align --input path/to/transcription.full.json --audio path/to/audio.wav --out output.full.json

Takes raw transcription from 02.EXT.transcribe and aligns to sentence-level segments
using whisperx.align. This improves segment boundaries for downstream diarization.

Input:  data/02.EXT.transcribe/<source>/<video>/<video>.full.json
Output: data/03.EXT.align/<source>/<video>/<video>.full.json
"""

from __future__ import annotations

import argparse
import json
import os
import re
import shlex
import sys
import threading
import time
from contextlib import contextmanager
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, Iterator, List, Optional, Set, Tuple

import numpy as np

from batch.manifest_parser import load_manifest, load_manifest_sources, manifest_filter_dirs

# --------------------------
# Torch import
# --------------------------

try:
    import torch  # type: ignore
except Exception:
    torch = None

# Optional audio loaders
try:
    import torchaudio  # type: ignore
except Exception:
    torchaudio = None

try:
    import soundfile as sf  # type: ignore
except Exception:
    sf = None

try:
    import librosa  # type: ignore
except Exception:
    librosa = None


# --------------------------
# Small helpers
# --------------------------

def log(msg: str) -> None:
    print(msg, flush=True)


def repo_root() -> Path:
    return Path(__file__).resolve().parents[2]


def now_iso() -> str:
    return datetime.utcnow().replace(microsecond=0).isoformat() + "Z"


def safe_name(name: str) -> str:
    cleaned = re.sub(r"[^A-Za-z0-9._-]+", "_", (name or "").strip())
    return cleaned.strip("_") or "source"


def extract_video_id(url: str) -> Optional[str]:
    url = (url or "").strip()
    if not url:
        return None
    m = re.search(r"[?&]v=([^&]+)", url)
    if m:
        return m.group(1)
    m = re.search(r"youtu\.be/([^?&/]+)", url)
    if m:
        return m.group(1)
    return None


def parse_sources_file(path: Path) -> List[Tuple[str, str]]:
    out: List[Tuple[str, str]] = []
    for raw in path.read_text(encoding="utf-8").splitlines():
        line = raw.strip()
        if not line or line.startswith("#"):
            continue
        if "|" in line:
            name, url = line.split("|", 1)
            name = name.strip()
            url = url.strip()
            if name and url:
                out.append((name, url))
            continue
        parts = shlex.split(line)
        if len(parts) >= 2:
            out.append((parts[0], parts[1]))
    return out


def _r3(x: float) -> float:
    return float(np.round(float(x), 3))


def _detect_repetition_hallucination(
    segments: List[Dict[str, Any]],
    min_repeats: int = 3,
) -> Optional[Dict[str, Any]]:
    """
    Detect Whisper hallucination loops where the same sentence is repeated 3+ times.

    Returns None if no hallucination detected, or a dict with:
        - repeated_text: the text that was repeated
        - count: number of consecutive repetitions
        - first_index: index of first occurrence
        - last_index: index of last occurrence

    Note: Only flags when the SAME SENTENCE is repeated. Does NOT flag if
    the same word appears in different sentences.
    """
    if len(segments) < min_repeats:
        return None

    # Track consecutive repetitions
    prev_text = None
    repeat_count = 0
    first_idx = 0

    for i, seg in enumerate(segments):
        text = str(seg.get("text", "")).strip()
        if not text:
            continue

        if text == prev_text:
            repeat_count += 1
        else:
            # Check if we had enough repetitions before reset
            if repeat_count >= min_repeats:
                return {
                    "repeated_text": prev_text,
                    "count": repeat_count,
                    "first_index": first_idx,
                    "last_index": i - 1,
                }
            prev_text = text
            repeat_count = 1
            first_idx = i

    # Check final sequence
    if repeat_count >= min_repeats:
        return {
            "repeated_text": prev_text,
            "count": repeat_count,
            "first_index": first_idx,
            "last_index": len(segments) - 1,
        }

    return None


def _load_flagged_videos(flag_file: Path) -> set:
    """Load CRITICAL-severity video names from a .flagged.json file.

    Only CRITICAL videos are skipped. WARNING-severity entries (including
    entries that predate the severity field) are allowed through.
    """
    if not flag_file.exists():
        return set()
    try:
        entries = json.loads(flag_file.read_text(encoding="utf-8"))
        return {
            str(e.get("video", ""))
            for e in entries
            if e.get("video") and e.get("severity", "WARNING") == "CRITICAL"
        }
    except Exception:
        return set()


def _auto_device(preferred: str = "") -> str:
    preferred = (preferred or "").strip().lower()
    if preferred in {"cuda", "cpu"}:
        if preferred == "cuda":
            if torch is not None and torch.cuda.is_available():
                return "cuda"
            return "cpu"
        return "cpu"
    if torch is not None and torch.cuda.is_available():
        return "cuda"
    return "cpu"


@contextmanager
def heartbeat(label: str, interval_sec: float = 15.0) -> Iterator[None]:
    stop = threading.Event()
    start = time.monotonic()

    def _runner() -> None:
        while not stop.wait(interval_sec):
            elapsed = int(time.monotonic() - start)
            log(f"[03.EXT.align] … {label}: still running ({elapsed}s)")

    t = threading.Thread(target=_runner, daemon=True)
    t.start()
    try:
        yield
    finally:
        stop.set()
        t.join(timeout=1.0)


# --------------------------
# Audio loading
# --------------------------

def load_audio_mono(path: str) -> Tuple[np.ndarray, int]:
    if torchaudio is not None and torch is not None:
        try:
            waveform, sr = torchaudio.load(path)
            waveform = waveform.to(torch.float32)
            if waveform.ndim == 2 and waveform.shape[0] > 1:
                waveform = waveform.mean(dim=0, keepdim=True)
            if waveform.ndim == 2:
                waveform = waveform.squeeze(0)
            y = waveform.detach().cpu().numpy().astype(np.float32)
            return y, int(sr)
        except Exception:
            pass

    if sf is not None:
        y, sr = sf.read(path, dtype="float32", always_2d=True)
        if y.shape[1] > 1:
            y = y.mean(axis=1, keepdims=True)
        y = y[:, 0].astype(np.float32)
        return y, int(sr)

    if librosa is not None:
        y, sr = librosa.load(path, sr=None, mono=True)
        return y.astype(np.float32), int(sr)

    raise SystemExit("No audio loader available. Install soundfile or librosa (or torchaudio).")


def resample_to_16k(y: np.ndarray, sr: int) -> np.ndarray:
    if sr == 16000:
        return y.astype(np.float32)
    if torchaudio is not None and torch is not None:
        try:
            yt = torch.from_numpy(y).to(torch.float32)
            yt = torchaudio.functional.resample(yt, orig_freq=sr, new_freq=16000)
            return yt.detach().cpu().numpy().astype(np.float32)
        except Exception:
            pass
    if librosa is not None:
        return librosa.resample(y.astype(np.float32), orig_sr=sr, target_sr=16000).astype(np.float32)
    raise SystemExit("Need torchaudio or librosa to resample to 16k.")


def load_audio_16k_for_whisperx(audio_path: str) -> np.ndarray:
    try:
        import whisperx  # type: ignore
        return whisperx.load_audio(audio_path)
    except Exception:
        y, sr = load_audio_mono(audio_path)
        return resample_to_16k(y, sr)


# --------------------------
# Segment normalization
# --------------------------

def _normalize_word_list(words: Any, duration_sec: float) -> Optional[List[Dict[str, Any]]]:
    if not isinstance(words, list):
        return None
    out: List[Dict[str, Any]] = []
    for w in words:
        if not isinstance(w, dict):
            continue
        word = str(w.get("word", "")).strip()
        if not word:
            continue
        ww: Dict[str, Any] = {"word": word}
        try:
            ws = float(w.get("start", -1.0))
            we = float(w.get("end", -1.0))
        except Exception:
            ws, we = -1.0, -1.0
        if ws >= 0.0 and we > ws:
            ws = max(0.0, min(ws, duration_sec))
            we = max(0.0, min(we, duration_sec))
            ww["start"] = _r3(ws)
            ww["end"] = _r3(we)
        if "speaker" in w:
            ww["speaker"] = str(w.get("speaker"))
        out.append(ww)
    return out or None


def _segments_from_any(segments_in: Any, duration_sec: float) -> List[Dict[str, Any]]:
    if not isinstance(segments_in, list):
        return []
    out: List[Dict[str, Any]] = []
    for seg in segments_in:
        if not isinstance(seg, dict):
            continue
        txt = str(seg.get("text", "") or "").strip()
        if not txt:
            continue
        try:
            s = float(seg.get("start", 0.0))
            e = float(seg.get("end", 0.0))
        except Exception:
            continue
        s = max(0.0, min(s, duration_sec))
        e = max(0.0, min(e, duration_sec))
        if e <= s:
            continue
        o: Dict[str, Any] = {"start": _r3(s), "end": _r3(e), "text": txt}
        if "speaker" in seg:
            o["speaker"] = str(seg.get("speaker"))
        if "words" in seg:
            wn = _normalize_word_list(seg.get("words"), duration_sec)
            if wn:
                o["words"] = wn
        out.append(o)
    out.sort(key=lambda x: (float(x["start"]), float(x["end"])))
    return out


def _preprocess_segments_for_align(
    segments: List[Dict[str, Any]],
) -> Tuple[List[Dict[str, Any]], List[int]]:
    """
    Preprocess segments before whisperx.align to prevent ZeroDivisionError.
    Filters out segments without words array.

    Returns:
        Tuple of (filtered_segments, removed_indices)
    """
    filtered = []
    removed_indices = []

    for i, seg in enumerate(segments):
        words = seg.get("words", [])

        # Skip segments without words (causes ZeroDivisionError in whisperx.align)
        if not words:
            removed_indices.append(i)
            continue

        filtered.append(seg)

    return filtered, removed_indices


# --------------------------
# Alignment engine
# --------------------------

class AlignEngine:
    """Sentence-level alignment using whisperx.align."""

    def __init__(self, language: str = "en", device: str = ""):
        self.language = (language or "en").strip()
        self.device = _auto_device(device)

        try:
            import whisperx  # type: ignore
        except Exception as e:
            raise SystemExit(f"Missing whisperx. Install: pip install -U whisperx ({type(e).__name__}: {e})")

        self._whisperx = whisperx
        self._align_model, self._align_meta = self._whisperx.load_align_model(
            language_code=self.language,
            device=self.device,
        )
        log(f"[03.EXT.align] Loaded alignment model for language={self.language} on device={self.device}")

    def align(self, segments: List[Dict[str, Any]], audio16k: np.ndarray) -> List[Dict[str, Any]]:
        """Align segments to sentence-level boundaries.

        Preprocessing: Removes segments without words (causes ZeroDivisionError).
        Failures are flagged for manual review.
        """
        # Preprocess to avoid ZeroDivisionError from wordless segments
        filtered_segments, removed = _preprocess_segments_for_align(segments)

        if removed:
            log(f"[03.EXT.align] Filtered {len(removed)} problematic segments (indices: {removed[:10]}{'...' if len(removed) > 10 else ''})")

        if not filtered_segments:
            log("[03.EXT.align] WARN: No valid segments after filtering")
            return segments  # Return original if nothing left to align

        try:
            aligned = self._whisperx.align(
                filtered_segments,
                self._align_model,
                self._align_meta,
                audio16k,
                self.device,
                return_char_alignments=False,
            )
            return aligned.get("segments", []) or []
        except TypeError:
            # Older whisperx versions have different signature
            aligned = self._whisperx.align(filtered_segments, self._align_model, self._align_meta, audio16k, self.device)
            return aligned.get("segments", []) or []
        # ZeroDivisionError intentionally NOT caught - let it propagate for flagging


# --------------------------
# Output writing
# --------------------------

def write_txt(path: Path, segments: List[Dict[str, Any]]) -> None:
    text = " ".join([str(s.get("text", "")).strip() for s in segments if str(s.get("text", "")).strip()]).strip()
    path.write_text(text + ("\n" if text else ""), encoding="utf-8")


def _write_all_outputs(out_json_path: Path, segments: List[Dict[str, Any]]) -> None:
    out_json_path.parent.mkdir(parents=True, exist_ok=True)
    full_text = " ".join([str(s.get("text", "")).strip() for s in segments if str(s.get("text", "")).strip()]).strip()
    out_json_path.write_text(
        json.dumps({"text": full_text, "segments": segments}, ensure_ascii=False, indent=2) + "\n",
        encoding="utf-8",
    )
    base = out_json_path.with_suffix("")
    write_txt(base.with_name(base.name + ".txt"), segments)


# --------------------------
# Audio selection
# --------------------------

def _pick_best_audio_in_video_dir(video_dir: Path) -> Optional[Path]:
    clean = sorted(video_dir.glob("*.audio.asr.clean16k.wav"))
    if clean:
        return clean[0]
    return None


# --------------------------
# Core processing
# --------------------------

def align_transcription(
    transcription_json: Path,
    audio_path: Path,
    out_json: Path,
    engine: AlignEngine,
    progress_interval: float = 15.0,
) -> Optional[Dict[str, Any]]:
    """Align a transcription to sentence-level segments.

    Returns:
        None if successful, or a dict with hallucination info if repetition detected.
    """
    t0 = time.monotonic()

    # Load transcription
    data = json.loads(transcription_json.read_text(encoding="utf-8"))
    segments = data.get("segments", [])

    if not segments:
        log(f"[03.EXT.align] WARN: No segments in {transcription_json.name}")
        _write_all_outputs(out_json, [])
        return None

    log(f"[03.EXT.align] Input: {len(segments)} segments from {transcription_json.name}")

    # Check for repetition hallucination in input (same sentence repeated 3+ times)
    hallucination = _detect_repetition_hallucination(segments, min_repeats=3)
    if hallucination:
        log(f"[03.EXT.align] FLAGGED: Repetition hallucination detected in input!")
        log(f"[03.EXT.align]   Text: \"{hallucination['repeated_text'][:50]}...\"")
        log(f"[03.EXT.align]   Count: {hallucination['count']} consecutive repetitions")
        log(f"[03.EXT.align]   Segments: {hallucination['first_index']} to {hallucination['last_index']}")

    # Load audio
    audio16k = load_audio_16k_for_whisperx(str(audio_path))
    duration_sec = len(audio16k) / 16000.0
    log(f"[03.EXT.align] Audio: {audio_path.name} ({duration_sec:.1f}s)")

    # Align
    with heartbeat("alignment", interval_sec=progress_interval):
        aligned_segments = engine.align(segments, audio16k)

    # Normalize
    aligned_segments = _segments_from_any(aligned_segments, duration_sec)

    log(f"[03.EXT.align] Output: {len(aligned_segments)} aligned segments")

    # Write output
    _write_all_outputs(out_json, aligned_segments)

    elapsed = time.monotonic() - t0
    log(f"[03.EXT.align] DONE: {out_json.name} ({elapsed:.1f}s)")

    return hallucination


# --------------------------
# Batch processing
# --------------------------

def batch_for_source(
    source_name: str,
    youtube_url: str,
    overwrite: bool,
    engine: AlignEngine,
    progress_interval: float,
    manifest_ids: Optional[Set[str]] = None,
) -> int:
    root = repo_root()
    safe_source = safe_name(source_name)

    # Input from 02.EXT.transcribe
    transcribe_root = root / "data" / "02.EXT.transcribe" / safe_source
    # Audio from 01.download
    downloads_root = root / "data" / "01.download" / safe_source
    # Output to 03.EXT.align
    out_root = root / "data" / "03.EXT.align" / safe_source

    if not transcribe_root.exists():
        log(f"[03.EXT.align] No transcriptions found: {transcribe_root}")
        return 0

    if not downloads_root.exists():
        log(f"[03.EXT.align] No downloads found: {downloads_root}")
        return 0

    # Load flagged videos from 02.EXT.transcribe — skip these entirely
    flagged = _load_flagged_videos(transcribe_root / ".flagged.json")
    if flagged:
        log(f"[03.EXT.align] CRITICAL-flagged videos (will skip): {len(flagged)}")

    video_id = extract_video_id(youtube_url)
    video_dirs = sorted([p for p in transcribe_root.iterdir() if p.is_dir()])
    if video_id:
        video_dirs = [d for d in video_dirs if f"[{video_id}]" in d.name]
    if manifest_ids:
        video_dirs = manifest_filter_dirs(video_dirs, manifest_ids)

    if not video_dirs:
        log(f"[03.EXT.align] No video folders found under: {transcribe_root}")
        return 0

    processed = 0
    skipped = 0
    failed = 0
    failed_videos: List[Dict[str, Any]] = []  # Track failures for flagging
    flagged_videos: List[Dict[str, Any]] = []  # Track hallucination flags

    for video_dir in video_dirs:
        try:
            # Skip flagged videos (hallucination, quality issues in 02.EXT.transcribe)
            if video_dir.name in flagged:
                log(f"[03.EXT.align] SKIP (flagged): {video_dir.name}")
                skipped += 1
                continue

            # Find transcription JSON
            transcription_files = sorted(video_dir.glob("*.full.json"))
            # Filter out engine-specific files (*.full.whisperx.json, etc.)
            transcription_files = [f for f in transcription_files if not any(
                f.name.endswith(f".full.{eng}.json") for eng in ["whisperx", "faster", "whisper"]
            )]

            if not transcription_files:
                log(f"[03.EXT.align] WARN: No transcription found in: {video_dir}")
                continue

            transcription_json = transcription_files[0]

            # Find corresponding audio
            download_video_dir = downloads_root / video_dir.name
            if not download_video_dir.exists():
                log(f"[03.EXT.align] WARN: No download folder for: {video_dir.name}")
                continue

            audio_path = _pick_best_audio_in_video_dir(download_video_dir)
            if audio_path is None:
                log(f"[03.EXT.align] WARN: No clean16k audio found for: {video_dir.name}")
                continue

            # Output
            out_video_dir = out_root / video_dir.name
            out_video_dir.mkdir(parents=True, exist_ok=True)
            out_json = out_video_dir / f"{video_dir.name}.full.json"

            if out_json.exists() and not overwrite:
                skipped += 1
                continue

            log(f"[03.EXT.align] VIDEO: {video_dir.name}")
            hallucination = align_transcription(
                transcription_json=transcription_json,
                audio_path=audio_path,
                out_json=out_json,
                engine=engine,
                progress_interval=progress_interval,
            )
            processed += 1

            # Track hallucination flags
            if hallucination:
                flagged_videos.append({
                    "video": video_dir.name,
                    "source": safe_source,
                    "reason": "repetition_hallucination",
                    "repeated_text": hallucination["repeated_text"],
                    "repeat_count": hallucination["count"],
                    "segment_range": [hallucination["first_index"], hallucination["last_index"]],
                    "timestamp": now_iso(),
                })

        except Exception as e:
            failed += 1
            log(f"[03.EXT.align] ERROR: Failed on folder: {video_dir.name}")
            log(f"[03.EXT.align]        {type(e).__name__}: {e}")
            failed_videos.append({
                "video": video_dir.name,
                "source": safe_source,
                "error": f"{type(e).__name__}: {e}",
                "timestamp": now_iso(),
            })
            continue

    # Write failures to flag file for manual review
    if failed_videos:
        flag_file = out_root / ".failed.json"
        existing: List[Dict[str, Any]] = []
        if flag_file.exists():
            try:
                existing = json.loads(flag_file.read_text(encoding="utf-8"))
            except Exception:
                pass
        existing.extend(failed_videos)
        flag_file.parent.mkdir(parents=True, exist_ok=True)
        flag_file.write_text(json.dumps(existing, indent=2, ensure_ascii=False) + "\n", encoding="utf-8")
        log(f"[03.EXT.align] FAILED: {len(failed_videos)} videos written to {flag_file}")

    # Write hallucination flags to flag file for manual review
    if flagged_videos:
        flagged_file = out_root / ".flagged.json"
        existing_flagged: List[Dict[str, Any]] = []
        if flagged_file.exists():
            try:
                existing_flagged = json.loads(flagged_file.read_text(encoding="utf-8"))
            except Exception:
                pass
        existing_flagged.extend(flagged_videos)
        flagged_file.parent.mkdir(parents=True, exist_ok=True)
        flagged_file.write_text(json.dumps(existing_flagged, indent=2, ensure_ascii=False) + "\n", encoding="utf-8")
        log(f"[03.EXT.align] FLAGGED: {len(flagged_videos)} videos with hallucination written to {flagged_file}")

    log(f"[03.EXT.align] Done: processed={processed} skipped={skipped} failed={failed} flagged={len(flagged_videos)}")
    return processed


# --------------------------
# CLI
# --------------------------

if __name__ == "__main__":
    p = argparse.ArgumentParser(
        description="Sentence-level alignment using whisperx.align.",
        formatter_class=argparse.RawDescriptionHelpFormatter,
    )

    p.add_argument("source_name", nargs="?", help="Source name (folder under data/02.EXT.transcribe).")
    p.add_argument("youtube_url", nargs="?", help="YouTube URL. Video URL filters by ID.")

    p.add_argument("--sources", nargs="?", const="docs/pipeline/sources.txt", help="Process all sources from sources file.")
    p.add_argument("--manifest", help="Manifest file: only process videos listed (docs/pipeline/batches/P001.txt).")
    p.add_argument("--input", help="Single-file input transcription JSON.")
    p.add_argument("--audio", help="Single-file input audio path.")
    p.add_argument("--out", help="Single-file output .full.json.")

    p.add_argument("--overwrite", action="store_true", help="Overwrite existing outputs.")

    p.add_argument("--language", default=os.environ.get("WHISPER_LANGUAGE", "en"))
    p.add_argument("--device", default=os.environ.get("WHISPERX_DEVICE", ""))

    p.add_argument("--progress-interval", type=float, default=float(os.environ.get("TRANSCRIBE_PROGRESS_INTERVAL", "15")))

    args = p.parse_args()

    # Initialize engine
    engine = AlignEngine(language=args.language, device=args.device)

    # Single-file mode
    if args.input:
        if not args.audio:
            raise SystemExit("--input requires --audio")
        if not args.out:
            raise SystemExit("--input requires --out")

        input_path = Path(args.input)
        audio_path = Path(args.audio)
        out_path = Path(args.out)

        if not input_path.exists():
            raise SystemExit(f"Input not found: {input_path}")
        if not audio_path.exists():
            raise SystemExit(f"Audio not found: {audio_path}")

        if out_path.exists() and not args.overwrite:
            log(f"[03.EXT.align] SKIP: exists ({out_path.name})")
            raise SystemExit(0)

        out_path.parent.mkdir(parents=True, exist_ok=True)
        align_transcription(
            transcription_json=input_path,
            audio_path=audio_path,
            out_json=out_path,
            engine=engine,
            progress_interval=args.progress_interval,
        )
        raise SystemExit(0)

    # Manifest batch mode
    if args.manifest:
        manifest_path = Path(args.manifest)
        if not manifest_path.is_absolute():
            manifest_path = repo_root() / manifest_path
        if not manifest_path.exists():
            raise SystemExit(f"Manifest file not found: {manifest_path}")
        sources_map = load_manifest_sources(manifest_path)
        total = 0
        for source_name, vid_ids in sorted(sources_map.items()):
            log(f"[03.EXT.align] Manifest: {source_name} ({len(vid_ids)} videos)")
            total += batch_for_source(
                source_name=source_name,
                youtube_url="",
                overwrite=bool(args.overwrite),
                engine=engine,
                progress_interval=args.progress_interval,
                manifest_ids=vid_ids,
            )
        log(f"[03.EXT.align] ✅ MANIFEST DONE: total_processed={total}")
        raise SystemExit(0)

    # Batch sources file
    if args.sources is not None:
        sources_path = Path(args.sources)
        if not sources_path.is_absolute():
            sources_path = repo_root() / sources_path
        if not sources_path.exists():
            raise SystemExit(f"Sources file not found: {sources_path}")
        total = 0
        for source_name, youtube_url in parse_sources_file(sources_path):
            total += batch_for_source(
                source_name=source_name,
                youtube_url=youtube_url,
                overwrite=bool(args.overwrite),
                engine=engine,
                progress_interval=args.progress_interval,
            )
        log(f"[03.EXT.align] ✅ ALL SOURCES DONE: total_processed={total}")
        raise SystemExit(0)

    # Normal batch mode
    if not args.source_name or not args.youtube_url:
        raise SystemExit(
            "Provide either --input/--audio/--out, --manifest, or --sources [file], or:\n"
            "./scripts/training-data/03.EXT.align <source_name> <youtube_url>"
        )

    batch_for_source(
        source_name=str(args.source_name),
        youtube_url=str(args.youtube_url),
        overwrite=bool(args.overwrite),
        engine=engine,
        progress_interval=args.progress_interval,
    )
