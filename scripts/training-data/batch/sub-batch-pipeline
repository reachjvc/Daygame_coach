#!/usr/bin/env bash
#
# scripts/training-data/batch/sub-batch-pipeline
#
# Pipeline orchestrator for sub-batches. Runs stages with automatic
# post-stage validation and quarantine-and-continue on failures.
#
# Usage:
#   A) Run full pipeline (stages 06â†’09 with validation):
#      ./sub-batch-pipeline P001.1 --run
#      ./sub-batch-pipeline P001.1 --run --from 07    # resume from stage 07
#
#   B) Run all sub-batches in a batch:
#      ./sub-batch-pipeline P001 --run-all
#      ./sub-batch-pipeline P001 --run-all --count 3  # next 3 incomplete
#
#   C) Run a single stage (with post-stage validation):
#      ./sub-batch-pipeline P001.1 --stage 06
#
#   D) List / status / view / validate / approve:
#      ./sub-batch-pipeline --list
#      ./sub-batch-pipeline P001.1 --status
#      ./sub-batch-pipeline P001 --status
#      ./sub-batch-pipeline P001.1 --view
#      ./sub-batch-pipeline P001.1 --validate
#      ./sub-batch-pipeline P001.1 --approve

set -euo pipefail

SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
REPO_ROOT="$(cd "${SCRIPT_DIR}/../../.." && pwd)"
cd "$REPO_ROOT"

BATCHES_DIR="$REPO_ROOT/docs/pipeline/batches"
WAIVERS_DIR="$REPO_ROOT/docs/pipeline/waivers"
QUARANTINE_DIR="$REPO_ROOT/data/validation/quarantine"
DATA_DIR="$REPO_ROOT/data"
VIEWS_DIR="$REPO_ROOT/batch-views"

# Stage definitions (all stages for view generation)
declare -A STAGE_DIRS=(
  ["01"]="01.download"
  ["02"]="02.EXT.transcribe"
  ["03"]="03.EXT.align"
  ["04"]="04.EXT.diarize"
  ["05"]="05.EXT.audio-features"
  ["06"]="06.LLM.video-type"
  ["06b"]="06b.LLM.verify"
  ["06c"]="06c.DET.patched"
  ["06d"]="06d.DET.sanitized"
  ["06e"]="06e.LLM.quality-check"
  ["06f"]="06f.DET.damage-map"
  ["06g"]="06g.LLM.damage-adjudicator"
  ["06h"]="06h.DET.confidence-propagation"
  ["07"]="07.LLM.content"
  ["08"]="08.DET.taxonomy-validation"
  ["09"]="09.EXT.chunks"
)
# Stages with nested source/video folder structure
NESTED_STAGES=("01" "02" "03" "04" "05")
# Stages that often write source-scoped flat outputs (data/<stage>/<source>/*.json) in --manifest mode
FLAT_STAGES=("06" "06b" "06c" "06d" "06e" "06f" "06g" "06h" "07" "08" "09")

declare -A STAGE_SCRIPTS=(
  ["06"]="$REPO_ROOT/scripts/training-data/06.LLM.video-type"
  ["06b"]="$REPO_ROOT/scripts/training-data/06b.LLM.verify"
  ["06c"]="$REPO_ROOT/scripts/training-data/06c.DET.patch"
  ["06d"]="$REPO_ROOT/scripts/training-data/06d.DET.sanitize"
  ["06e"]="$REPO_ROOT/scripts/training-data/06e.LLM.quality-check"
  ["06f"]="$REPO_ROOT/scripts/training-data/06f.DET.damage-map"
  ["06g"]="$REPO_ROOT/scripts/training-data/06g.LLM.damage-adjudicator"
  ["06h"]="$REPO_ROOT/scripts/training-data/06h.DET.confidence-propagation"
  ["07"]="$REPO_ROOT/scripts/training-data/07.LLM.content"
  ["08"]="$REPO_ROOT/scripts/training-data/08.DET.taxonomy-validation"
  ["09"]="$REPO_ROOT/scripts/training-data/09.EXT.chunk-embed.ts"
)
declare -A STAGE_PATTERNS=(
  ["01"]="*.wav"
  ["02"]="*.full.json"
  ["03"]="*.full.json"
  ["04"]="*.full.json"
  ["05"]="*.audio_features.json"
  ["06"]="*.conversations.json"
  ["06b"]="*.verification.json"
  ["06c"]="*.conversations.json"
  ["06d"]="*.conversations.json"
  ["06e"]="*.quality-check.json"
  ["06f"]="*.damage-map.json"
  ["06g"]="*.damage-adjudication.json"
  ["06h"]="*.confidence.report.json"
  ["07"]="*.enriched.json"
  ["08"]="*.report.json"
  ["09"]="*.chunks.json"
)
# Stages that can be run via this pipeline
RUNNABLE_STAGES=("06" "06b" "06c" "06d" "06e" "06f" "06g" "06h" "07" "08" "09")
# Ordered stage sequence for --from-stage
STAGE_ORDER=("06" "06b" "06c" "06d" "06e" "06f" "06g" "06h" "07" "08" "09")
# All stages for view generation
ALL_STAGES=("01" "02" "03" "04" "05" "06" "06b" "06c" "06d" "06e" "06f" "06g" "06h" "07" "08" "09")

# Modes
MODE=""
DRY_RUN=false
STAGE=""
START_STAGE=""
SUB_BATCH_ID=""
BATCH_ID=""
RUN_ALL_COUNT=0
VALIDATE_SOURCE=""
VALIDATE_WAIVER_FILE=""
VALIDATE_QUARANTINE_FILE=""
RUN_QUARANTINE_FILE=""
CONFIDENCE_BAND_HIGH_THRESHOLD_OVERRIDE=""
CONFIDENCE_BAND_MEDIUM_THRESHOLD_OVERRIDE=""
MIN_CHUNK_CONFIDENCE_OVERRIDE=""
PARALLEL=""

# Config-driven defaults (loaded from pipeline.config.json)
CFG_QUARANTINE_LEVEL="error"
CFG_MAX_WARNING_CHECKS="3"
CFG_MAX_WARNING_CHECKS_BY_TYPE=()
CFG_BLOCK_WARNING_CHECKS=()
CFG_MAX_WARNING_CHECKS_BY_CLASS=()
CFG_BLOCK_WARNING_CLASSES=()
CFG_REVIEW_WARNING_CLASS_BUDGET_BY_CONTENT_TYPE=()

load_pipeline_config() {
  local config_file="$SCRIPT_DIR/pipeline.config.json"
  if [[ ! -f "$config_file" ]]; then
    echo "WARNING: Pipeline config not found: $config_file (using defaults)"
    return 0
  fi

  eval "$(python3 - "$config_file" <<'PYEOF'
import json
import sys

with open(sys.argv[1]) as f:
    cfg = json.load(f)

v = cfg.get("validation", {})
r = v.get("readiness", {})

print(f'CFG_QUARANTINE_LEVEL="{v.get("quarantine_level", "error")}"')
print(f'CFG_MAX_WARNING_CHECKS="{r.get("max_warning_checks", 3)}"')

by_type = r.get("max_warning_checks_by_type", {})
if by_type:
    items = " ".join(f'"{k}={v}"' for k, v in by_type.items())
    print(f'CFG_MAX_WARNING_CHECKS_BY_TYPE=({items})')
else:
    print('CFG_MAX_WARNING_CHECKS_BY_TYPE=()')

blocks = r.get("block_warning_checks", [])
if blocks:
    items = " ".join(f'"{b}"' for b in blocks)
    print(f'CFG_BLOCK_WARNING_CHECKS=({items})')
else:
    print('CFG_BLOCK_WARNING_CHECKS=()')

by_class = r.get("max_warning_checks_by_class", {})
if by_class:
    items = " ".join(f'"{k}={v}"' for k, v in by_class.items())
    print(f'CFG_MAX_WARNING_CHECKS_BY_CLASS=({items})')
else:
    print('CFG_MAX_WARNING_CHECKS_BY_CLASS=()')

block_classes = r.get("block_warning_classes", [])
if block_classes:
    items = " ".join(f'"{b}"' for b in block_classes)
    print(f'CFG_BLOCK_WARNING_CLASSES=({items})')
else:
    print('CFG_BLOCK_WARNING_CLASSES=()')

review_by_type = r.get("review_warning_class_budget_by_content_type", {})
if isinstance(review_by_type, dict) and review_by_type:
    items = []
    for video_type, class_map in review_by_type.items():
        if not isinstance(video_type, str) or not isinstance(class_map, dict):
            continue
        for warning_class, limit in class_map.items():
            items.append(f'"{video_type}:{warning_class}={limit}"')
    if items:
        print(f'CFG_REVIEW_WARNING_CLASS_BUDGET_BY_CONTENT_TYPE=({" ".join(items)})')
    else:
        print('CFG_REVIEW_WARNING_CLASS_BUDGET_BY_CONTENT_TYPE=()')
else:
    print('CFG_REVIEW_WARNING_CLASS_BUDGET_BY_CONTENT_TYPE=()')

PYEOF
  )"
}

# Load config at startup
load_pipeline_config

usage() {
  cat <<'EOF'
Usage:
  ./sub-batch-pipeline P001.1 --run              Run stages 06â†’09 with auto-validation
  ./sub-batch-pipeline P001.1 --run --from 07    Resume from stage 07
  ./sub-batch-pipeline P001.1 --run --parallel 10  Conveyor-belt parallel mode
  ./sub-batch-pipeline P001 --run-all            Run all incomplete sub-batches
  ./sub-batch-pipeline P001 --run-all --count 3  Run next 3 incomplete sub-batches
  ./sub-batch-pipeline --list                    List all batches and sub-batches
  ./sub-batch-pipeline P001.1 --status           Per-stage status
  ./sub-batch-pipeline P001 --status             Batch-level status
  ./sub-batch-pipeline P001.1 --stage 06         Run single stage (with post-stage validation)
  ./sub-batch-pipeline P001.1 --view             Create symlink view
  ./sub-batch-pipeline P001.1 --validate         Run end-of-pipeline validation only
  ./sub-batch-pipeline P001.1 --approve          Mark as approved

Options:
  --dry-run                Preview without running
  --from <stage>           With --run, start from this stage (default: 06)
  --parallel <n>           Conveyor-belt mode: videos run in parallel, n max LLM calls
  --count <n>              With --run-all, limit to n sub-batches
  --source <name>          With --validate, only validate one source
  --quarantine-file <path> Override auto-detected quarantine file
  --waiver-file <path>     Override auto-detected waiver file
  --confidence-band-high-threshold <0..1>
                            Stage 06h high confidence band threshold override
  --confidence-band-medium-threshold <0..1>
                            Stage 06h medium confidence band threshold override
  --min-chunk-confidence <0..1>
                            Stage 09 chunk confidence floor override

Config: scripts/training-data/batch/pipeline.config.json
  All validation thresholds and policies are configured there.
EOF
}

error() {
  echo "ERROR: $*" >&2
  exit 1
}

stage_supports_quarantine() {
  local stage="$1"
  case "$stage" in
    06c|06d|06e|06f|06g|06h|07|08|09) return 0 ;;
    *) return 1 ;;
  esac
}

stage_supports_overwrite() {
  local stage="$1"
  case "$stage" in
    06|06b|06c|06d|06e|06f|06g|06h|07) return 0 ;;
    *) return 1 ;;
  esac
}

require_probability_01() {
  local raw="$1"
  python3 - "$raw" <<'PYEOF'
import sys
raw = sys.argv[1]
try:
    value = float(raw)
except Exception:
    raise SystemExit(1)
if not (0.0 <= value <= 1.0):
    raise SystemExit(1)
print(f"{value:.4f}".rstrip("0").rstrip("."))
PYEOF
}

run_stage_contract_preflight() {
  local stage="$1"
  local manifest="$2"
  local quarantine_file="${3:-}"

  local -a cmd=(
    python3
    "$REPO_ROOT/scripts/training-data/validation/validate_stage_contract.py"
    --manifest "$manifest"
    --stage "$stage"
  )
  if [[ -n "$quarantine_file" && -f "$quarantine_file" ]]; then
    cmd+=(--quarantine-file "$quarantine_file")
  fi

  echo "--- Preflight contract: stage $stage ---"
  "${cmd[@]}"
}

# Extract batch ID from sub-batch ID (P001.1 -> P001)
get_batch_id() {
  local sub_id="$1"
  echo "${sub_id%.*}"
}

# Check if argument looks like a sub-batch (has .N suffix)
is_sub_batch() {
  [[ "$1" =~ \.[0-9]+$ ]]
}

# Extract video ID from filename/path
extract_video_id() {
  local name="$1"
  if [[ "$name" =~ \[([a-zA-Z0-9_-]{11})\] ]]; then
    echo "${BASH_REMATCH[1]}"
  fi
}

# Load manifest video IDs
load_manifest_ids() {
  local manifest="$1"
  grep -v "^#" "$manifest" | while read -r line; do
    extract_video_id "$line"
  done
}

# List all batches
list_batches() {
  echo ""
  echo "=== All Batches ==="
  echo ""

  for status_file in "$BATCHES_DIR"/*.status.json; do
    [[ -f "$status_file" ]] || continue
    python3 - "$status_file" <<'PYEOF'
import json
import sys

with open(sys.argv[1]) as f:
    data = json.load(f)

batch_id = data["batch_id"]
total = data["total_videos"]
sub_count = len(data["sub_batches"])

# Count statuses
counts = {"approved": 0, "pending_review": 0, "in_progress": 0, "not_started": 0}
for info in data["sub_batches"].values():
    counts[info["status"]] = counts.get(info["status"], 0) + 1

print(f"{batch_id}: {total} videos in {sub_count} sub-batches")
print(f"  approved: {counts['approved']}, pending: {counts['pending_review']}, in_progress: {counts['in_progress']}, not_started: {counts['not_started']}")
PYEOF
  done

  # List batches without status files (not yet split)
  for batch_file in "$BATCHES_DIR"/P*.txt; do
    [[ -f "$batch_file" ]] || continue
    batch_name=$(basename "$batch_file" .txt)
    # Skip sub-batch files (have dots)
    [[ "$batch_name" =~ \. ]] && continue
    status_file="$BATCHES_DIR/${batch_name}.status.json"
    if [[ ! -f "$status_file" ]]; then
      count=$(grep -v "^#" "$batch_file" | grep -c . || echo 0)
      echo "$batch_name: $count videos (not split yet - run sub-batch-create)"
    fi
  done
}

# Show per-stage status for a sub-batch
show_sub_batch_status() {
  local sub_id="$1"
  local batch_id
  batch_id=$(get_batch_id "$sub_id")

  local manifest="$BATCHES_DIR/${sub_id}.txt"
  if [[ ! -f "$manifest" ]]; then
    error "Sub-batch manifest not found: $manifest"
  fi

  # Get video IDs from manifest
  local -a video_ids=()
  while IFS='|' read -r source folder || [[ -n "$source" ]]; do
    [[ "$source" =~ ^# ]] && continue
    [[ -z "$source" ]] && continue
    # Trim leading/trailing whitespace without collapsing internal spaces
    source="${source#"${source%%[![:space:]]*}"}"
    source="${source%"${source##*[![:space:]]}"}"
    folder="${folder#"${folder%%[![:space:]]*}"}"
    folder="${folder%"${folder##*[![:space:]]}"}"
    vid_id=$(extract_video_id "$folder")
    [[ -n "$vid_id" ]] && video_ids+=("$vid_id:$source:$folder")
  done < "$manifest"

  local total=${#video_ids[@]}

  echo ""
  echo "=== Sub-batch $sub_id Status ($total videos) ==="
  echo ""

  for stage in "${RUNNABLE_STAGES[@]}"; do
    local stage_dir="${STAGE_DIRS[$stage]}"
    local pattern="${STAGE_PATTERNS[$stage]}"
    local found=0
    local missing=()

    for entry in "${video_ids[@]}"; do
      IFS=':' read -r vid_id source folder <<< "$entry"

      # Check if output exists for this video
      local found_file=false

      # Search in stage directory
      if [[ -d "$DATA_DIR/$stage_dir" ]]; then
        while IFS= read -r -d '' f; do
          found_file=true
          break
        # Follow symlinks when the stage dir itself is a symlink (common in worktree-based experiments).
        done < <(find -L "$DATA_DIR/$stage_dir" -name "*${vid_id}*" -name "$pattern" -print0 2>/dev/null)
      fi

      if [[ "$found_file" == true ]]; then
        ((++found))
      else
        missing+=("$folder")
      fi
    done

    local status_icon="â¬œ"
    if [[ $found -eq $total ]]; then
      status_icon="âœ…"
    elif [[ $found -gt 0 ]]; then
      status_icon="ðŸ”¶"
    fi

    printf "  %s Stage %s: %d/%d\n" "$status_icon" "$stage" "$found" "$total"

    # Show missing files if partially complete
    if [[ $found -gt 0 && $found -lt $total && ${#missing[@]} -le 5 ]]; then
      for m in "${missing[@]}"; do
        echo "      missing: $m"
      done
    elif [[ ${#missing[@]} -gt 5 ]]; then
      echo "      (${#missing[@]} missing)"
    fi
  done

  echo ""

  # Show current status from status file
  local status_file="$BATCHES_DIR/${batch_id}.status.json"
  if [[ -f "$status_file" ]]; then
    python3 - "$status_file" "$sub_id" <<'PYEOF'
import json
import sys

with open(sys.argv[1]) as f:
    data = json.load(f)

sub_id = sys.argv[2]
info = data["sub_batches"].get(sub_id, {})
status = info.get("status", "unknown")
print(f"Overall status: {status}")
PYEOF
  fi
}

# Show batch-level status (all sub-batches)
show_batch_status() {
  local batch_id="$1"
  local status_file="$BATCHES_DIR/${batch_id}.status.json"

  if [[ ! -f "$status_file" ]]; then
    error "Status file not found: $status_file (run sub-batch-create first)"
  fi

  python3 - "$status_file" <<'PYEOF'
import json
import sys

with open(sys.argv[1]) as f:
    data = json.load(f)

print(f"\nBatch {data['batch_id']} Status")
print(f"Total videos: {data['total_videos']}")
print(f"Sub-batch size: {data['sub_batch_size']}")
print()

status_symbols = {
    "approved": "\u2705",
    "pending_review": "\U0001F50D",
    "in_progress": "\u23F3",
    "not_started": "\u2B1C",
}

for sub_id in sorted(data["sub_batches"].keys(), key=lambda x: int(x.split(".")[-1])):
    info = data["sub_batches"][sub_id]
    status = info["status"]
    symbol = status_symbols.get(status, "?")
    count = info.get("video_count", "?")
    print(f"  {symbol} {sub_id}: {status} ({count} videos)")

# Summary
counts = {}
for info in data["sub_batches"].values():
    s = info["status"]
    counts[s] = counts.get(s, 0) + info.get("video_count", 0)

print()
print("Summary:")
for status in ["approved", "pending_review", "in_progress", "not_started"]:
    if status in counts:
        print(f"  {status}: {counts[status]} videos")
PYEOF
}

# Run a single stage
run_stage() {
  local sub_id="$1"
  local stage="$2"
  local batch_id
  batch_id=$(get_batch_id "$sub_id")

  local manifest="$BATCHES_DIR/${sub_id}.txt"
  if [[ ! -f "$manifest" ]]; then
    error "Sub-batch manifest not found: $manifest"
  fi

  local script="${STAGE_SCRIPTS[$stage]:-}"
  if [[ -z "$script" ]]; then
    error "Unknown stage: $stage (valid: ${RUNNABLE_STAGES[*]})"
  fi

  local stage_dir="${STAGE_DIRS[$stage]}"
  local -a stage_args=(--manifest "$manifest")
  if stage_supports_overwrite "$stage"; then
    # Single strict path: always refresh outputs for stages that expose
    # --overwrite to avoid stale downstream artifacts after upstream changes.
    stage_args+=(--overwrite)
  fi
  local quarantine_file=""
  local auto_quarantine="$QUARANTINE_DIR/${sub_id}.json"

  echo ""
  echo "=== Running Stage $stage for $sub_id ==="
  echo ""

  quarantine_file="$RUN_QUARANTINE_FILE"
  if [[ -z "$quarantine_file" ]]; then
    if [[ -f "$auto_quarantine" ]]; then
      quarantine_file="$auto_quarantine"
    fi
  fi

  if [[ "$stage" == "06h" ]]; then
    if [[ -n "$CONFIDENCE_BAND_HIGH_THRESHOLD_OVERRIDE" ]]; then
      stage_args+=(--confidence-band-high-threshold "$CONFIDENCE_BAND_HIGH_THRESHOLD_OVERRIDE")
    fi
    if [[ -n "$CONFIDENCE_BAND_MEDIUM_THRESHOLD_OVERRIDE" ]]; then
      stage_args+=(--confidence-band-medium-threshold "$CONFIDENCE_BAND_MEDIUM_THRESHOLD_OVERRIDE")
    fi
  fi

  if [[ "$stage" == "09" ]]; then
    if [[ -n "$MIN_CHUNK_CONFIDENCE_OVERRIDE" ]]; then
      stage_args+=(--min-chunk-confidence "$MIN_CHUNK_CONFIDENCE_OVERRIDE")
    fi
  fi

  if [[ "$DRY_RUN" == true ]]; then
    if stage_supports_quarantine "$stage" && [[ -n "$quarantine_file" ]]; then
      stage_args+=(--quarantine-file "$quarantine_file")
    fi
    echo "[DRY RUN] Would run: $script ${stage_args[*]}"
    return 0
  fi

  if ! run_stage_contract_preflight "$stage" "$manifest" "$quarantine_file"; then
    echo "Stage $stage: CONTRACT PRECHECK FAILED"
    return 1
  fi

  # Update status to in_progress
  local status_file="$BATCHES_DIR/${batch_id}.status.json"
  if [[ -f "$status_file" ]]; then
    python3 - "$status_file" "$sub_id" "$stage" <<'PYEOF'
import json
import sys

status_file, sub_id, stage = sys.argv[1:4]

with open(status_file) as f:
    data = json.load(f)

if sub_id in data["sub_batches"]:
    data["sub_batches"][sub_id]["status"] = "in_progress"
    data["sub_batches"][sub_id]["current_stage"] = stage

with open(status_file, "w") as f:
    json.dump(data, f, indent=2)
    f.write("\n")
PYEOF
  fi

  if stage_supports_quarantine "$stage"; then
    if [[ -n "$quarantine_file" && -f "$quarantine_file" ]]; then
      stage_args+=(--quarantine-file "$quarantine_file")
      echo "Using quarantine file: $quarantine_file"
    elif [[ -n "$quarantine_file" ]]; then
      echo "WARNING: Quarantine file not found, ignoring: $quarantine_file"
    fi
  fi

  # Build execution command (TypeScript stages need tsx loader)
  local -a exec_cmd
  if [[ "$script" == *.ts ]]; then
    local tsx_loader="$REPO_ROOT/node_modules/tsx/dist/loader.mjs"
    if [[ ! -f "$tsx_loader" ]]; then
      echo "ERROR: tsx loader not found: $tsx_loader"
      return 1
    fi
    exec_cmd=(node --import "$tsx_loader" "$script")
  else
    exec_cmd=("$script")
  fi

  # Run the stage
  if "${exec_cmd[@]}" "${stage_args[@]}"; then
    echo ""
    echo "Stage $stage: COMPLETE"
  else
    local stage_rc=$?
    echo ""
    echo "Stage $stage: FAILED (exit code $stage_rc)"
    # For stage 08, try to quarantine per-video failures instead of hard-failing
    if [[ "$stage" == "08" ]]; then
      local report_file
      report_file=$(find "$DATA_DIR/08.DET.taxonomy-validation" -name "*.report.json" -newer "$manifest" -print -quit 2>/dev/null)
      if [[ -n "$report_file" ]]; then
        echo "--- Checking for per-video quarantine from Stage 08 report ---"
        python3 "$SCRIPT_DIR/quarantine_updater.py" \
          --quarantine-file "$auto_quarantine" \
          --stage08-report "$report_file" 2>&1 || true
      fi
    fi
    return 1
  fi

  # Create symlink view
  create_stage_view "$sub_id" "$stage"

  # --- Post-stage validation hooks ---
  run_post_stage_validation "$sub_id" "$stage" "$manifest" "$auto_quarantine"
  local validate_rc=$?
  if [[ $validate_rc -eq 2 ]]; then
    return 2  # all videos quarantined
  fi

  return 0
}

# Count non-comment lines in manifest
count_manifest_videos() {
  grep -v "^#" "$1" | grep -c . || echo 0
}

# Post-stage validation hooks
run_post_stage_validation() {
  local sub_id="$1"
  local stage="$2"
  local manifest="$3"
  local quarantine_file="$4"

  case "$stage" in
    06b)
      echo ""
      echo "--- Post-stage validation: checking 06b verdicts ---"
      python3 "$SCRIPT_DIR/quarantine_updater.py" \
        --quarantine-file "$quarantine_file" \
        --stage06b-dir "$DATA_DIR/06b.LLM.verify" \
        --manifest "$manifest" 2>&1 || true
      ;;
    07)
      echo ""
      echo "--- Post-stage validation: cross-stage (06/06c vs 07) ---"
      local cross_stage_json
      cross_stage_json=$(python3 "$REPO_ROOT/scripts/training-data/validation/validate_cross_stage.py" \
        --manifest "$manifest" --json 2>/dev/null) || true
      if [[ -n "$cross_stage_json" ]]; then
        echo "$cross_stage_json" | python3 "$SCRIPT_DIR/quarantine_updater.py" \
          --quarantine-file "$quarantine_file" \
          --stage 07 2>&1 || true
        # Show summary
        echo "$cross_stage_json" | python3 -c "
import json, sys
data = json.load(sys.stdin)
s = data.get('summary', {})
errors = s.get('errors', 0)
warnings = s.get('warnings', 0)
total = s.get('total_checks', 0)
status = 'PASS' if errors == 0 else 'FAIL'
print(f'  Cross-stage: {status} ({total} checks, {errors} errors, {warnings} warnings)')
" 2>/dev/null || true
      fi
      ;;
    09)
      echo ""
      echo "--- Post-stage validation: chunk integrity ---"
      local chunks_json
      chunks_json=$(python3 "$REPO_ROOT/scripts/training-data/validation/validate_chunks.py" \
        --manifest "$manifest" --json 2>/dev/null) || true
      if [[ -n "$chunks_json" ]]; then
        echo "$chunks_json" | python3 "$SCRIPT_DIR/quarantine_updater.py" \
          --quarantine-file "$quarantine_file" \
          --stage 09 2>&1 || true
        # Show summary
        echo "$chunks_json" | python3 -c "
import json, sys
data = json.load(sys.stdin)
errors = data.get('issues_summary', {}).get('errors', 0)
warnings = data.get('issues_summary', {}).get('warnings', 0)
files = data.get('processed_files', 0)
status = 'PASS' if errors == 0 else 'FAIL'
print(f'  Chunks: {status} ({files} files, {errors} errors, {warnings} warnings)')
" 2>/dev/null || true
      fi
      ;;
  esac

  # Check if all videos are now quarantined
  if [[ -f "$quarantine_file" ]]; then
    local manifest_count
    manifest_count=$(count_manifest_videos "$manifest")
    local all_quarantined
    all_quarantined=$(python3 -c "
import json, sys
q = json.load(open(sys.argv[1]))
m_count = int(sys.argv[2])
print('yes' if len(q.get('quarantined_video_ids',[])) >= m_count else 'no')
" "$quarantine_file" "$manifest_count" 2>/dev/null) || all_quarantined="no"
    if [[ "$all_quarantined" == "yes" ]]; then
      echo ""
      echo "ALL videos in $sub_id are quarantined. Skipping remaining stages."
      return 2
    fi
  fi

  return 0
}

# Run pipeline: stages sequentially with post-stage validation + end-of-run validation
run_pipeline() {
  local sub_id="$1"
  local start_stage="${2:-06}"

  # Parallel mode: delegate to pipeline-runner (conveyor-belt orchestrator)
  if [[ -n "$PARALLEL" ]]; then
    local runner_args=("$sub_id" --parallel "$PARALLEL" --from "$start_stage")
    if [[ -n "$RUN_QUARANTINE_FILE" ]]; then
      runner_args+=(--quarantine-file "$RUN_QUARANTINE_FILE")
    fi
    if [[ "$DRY_RUN" == true ]]; then
      runner_args+=(--dry-run)
    fi
    if [[ -n "$CONFIDENCE_BAND_HIGH_THRESHOLD_OVERRIDE" ]]; then
      runner_args+=(--confidence-band-high-threshold "$CONFIDENCE_BAND_HIGH_THRESHOLD_OVERRIDE")
    fi
    if [[ -n "$CONFIDENCE_BAND_MEDIUM_THRESHOLD_OVERRIDE" ]]; then
      runner_args+=(--confidence-band-medium-threshold "$CONFIDENCE_BAND_MEDIUM_THRESHOLD_OVERRIDE")
    fi
    if [[ -n "$MIN_CHUNK_CONFIDENCE_OVERRIDE" ]]; then
      runner_args+=(--min-chunk-confidence "$MIN_CHUNK_CONFIDENCE_OVERRIDE")
    fi
    "$REPO_ROOT/.venv/bin/python" -u "$SCRIPT_DIR/pipeline-runner" "${runner_args[@]}"
    return $?
  fi

  # --- Sequential mode (original behavior) ---

  # Find start index in STAGE_ORDER
  local start_idx=-1
  for i in "${!STAGE_ORDER[@]}"; do
    if [[ "${STAGE_ORDER[$i]}" == "$start_stage" ]]; then
      start_idx=$i
      break
    fi
  done

  if [[ $start_idx -eq -1 ]]; then
    error "Unknown stage: $start_stage (valid: ${STAGE_ORDER[*]})"
  fi

  local remaining=("${STAGE_ORDER[@]:$start_idx}")
  local manifest="$BATCHES_DIR/${sub_id}.txt"
  local manifest_count
  manifest_count=$(count_manifest_videos "$manifest")

  echo ""
  echo "========================================"
  echo "  Pipeline: $sub_id"
  echo "  Stages: ${remaining[*]}"
  echo "  Videos: $manifest_count"
  echo "  Config: $SCRIPT_DIR/pipeline.config.json"
  echo "========================================"

  local pipeline_failed=false
  for stage in "${remaining[@]}"; do
    run_stage "$sub_id" "$stage"
    local rc=$?
    if [[ $rc -eq 2 ]]; then
      echo ""
      echo "Sub-batch $sub_id: all videos quarantined at stage $stage"
      break
    elif [[ $rc -ne 0 ]]; then
      echo ""
      echo "PIPELINE HALTED at stage $stage (non-zero exit)"
      pipeline_failed=true
      break
    fi
  done

  echo ""
  echo "=== End-of-run validation: $sub_id ==="
  echo ""
  validate_sub_batch "$sub_id"

  echo ""
  print_pipeline_summary "$sub_id"

  if [[ "$pipeline_failed" == true ]]; then
    return 1
  fi
  return 0
}

# Run all incomplete sub-batches in a batch
run_all_sub_batches() {
  local batch_id="$1"
  local count="${2:-0}"
  local status_file="$BATCHES_DIR/${batch_id}.status.json"

  if [[ ! -f "$status_file" ]]; then
    error "Status file not found: $status_file (run sub-batch-create first)"
  fi

  local sub_batches
  sub_batches=$(python3 - "$status_file" "$count" <<'PYEOF'
import json
import sys

with open(sys.argv[1]) as f:
    data = json.load(f)

count = int(sys.argv[2])
found = 0

for sub_id in sorted(data["sub_batches"].keys(), key=lambda x: int(x.split(".")[-1])):
    info = data["sub_batches"][sub_id]
    if info["status"] in ("not_started", "in_progress"):
        print(sub_id)
        found += 1
        if count > 0 and found >= count:
            break
PYEOF
  )

  if [[ -z "$sub_batches" ]]; then
    echo "No incomplete sub-batches found in $batch_id"
    return 0
  fi

  local total_subs
  total_subs=$(echo "$sub_batches" | wc -l)
  echo ""
  echo "========================================"
  echo "  Batch run: $batch_id ($total_subs sub-batches)"
  echo "  Sub-batches: $(echo $sub_batches | tr '\n' ' ')"
  echo "========================================"

  local pass_count=0
  local fail_count=0

  for sub_id in $sub_batches; do
    run_pipeline "$sub_id" "${START_STAGE:-06}"
    local rc=$?
    if [[ $rc -eq 0 ]]; then
      ((++pass_count))
    else
      ((++fail_count))
      echo ""
      echo "Sub-batch $sub_id had failures. Continuing to next sub-batch."
    fi
  done

  echo ""
  echo "========================================"
  echo "  Batch $batch_id complete"
  echo "  Passed: $pass_count / $total_subs"
  if [[ $fail_count -gt 0 ]]; then
    echo "  Failed: $fail_count / $total_subs"
  fi
  echo "========================================"
  echo ""
  show_batch_status "$batch_id"
}

# Legacy alias
run_from_stage() {
  run_pipeline "$1" "$2"
}

# Check if stage is nested (has source/video folder structure)
is_nested_stage() {
  local stage="$1"
  for s in "${NESTED_STAGES[@]}"; do
    [[ "$s" == "$stage" ]] && return 0
  done
  return 1
}

# Create symlink view for a stage
create_stage_view() {
  local sub_id="$1"
  local stage="$2"
  local batch_id
  batch_id=$(get_batch_id "$sub_id")

  local stage_dir="${STAGE_DIRS[$stage]:-}"
  [[ -z "$stage_dir" ]] && return 0

  local pattern="${STAGE_PATTERNS[$stage]}"
  local manifest="$BATCHES_DIR/${sub_id}.txt"
  local view_base="$VIEWS_DIR/$batch_id/$sub_id/$stage_dir"
  local stage_data_root="$DATA_DIR/$stage_dir"
  if [[ -L "$stage_data_root" ]]; then
    stage_data_root=$(realpath "$stage_data_root")
  fi

  # Clear existing view for this stage
  rm -rf "$view_base" 2>/dev/null || true

  local link_count=0
  local pending_count=0

  # Process each video from manifest
  while IFS='|' read -r source folder || [[ -n "$source" ]]; do
    [[ "$source" =~ ^# ]] && continue
    [[ -z "$source" ]] && continue
    # Trim leading/trailing whitespace without collapsing internal spaces
    source="${source#"${source%%[![:space:]]*}"}"
    source="${source%"${source##*[![:space:]]}"}"
    folder="${folder#"${folder%%[![:space:]]*}"}"
    folder="${folder%"${folder##*[![:space:]]}"}"
    vid_id=$(extract_video_id "$folder")
    [[ -z "$vid_id" ]] && continue

    # Create source/video folder structure in view
    local video_view_dir="$view_base/$source/$folder"
    mkdir -p "$video_view_dir"

    local found_file=false

	    if is_nested_stage "$stage"; then
	      # Nested stage: data/<stage>/<source>/<video>/
	      local src_video_dir="$stage_data_root/$source/$folder"
	      if [[ ! -d "$src_video_dir" && -d "$stage_data_root/$source" ]]; then
	        # Folder names in manifests are display-only and can drift (unicode punctuation, title edits).
	        # Resolve nested stage dirs by video_id when the exact folder is missing.
	        while IFS= read -r -d '' d; do
	          local bn
	          bn=$(basename "$d")
	          local alt_id
	          alt_id=$(extract_video_id "$bn")
	          if [[ -n "$alt_id" && "$alt_id" == "$vid_id" ]]; then
	            src_video_dir="$d"
	            break
	          fi
	        done < <(find "$stage_data_root/$source" -mindepth 1 -maxdepth 1 -type d -print0 2>/dev/null)
	      fi
	      if [[ -d "$src_video_dir" ]]; then
	        # Symlink all matching files in the video folder
	        while IFS= read -r -d '' f; do
	          found_file=true
          local link_name=$(basename "$f")
          local rel_target
          rel_target=$(realpath --relative-to="$video_view_dir" "$f")
          ln -sf "$rel_target" "$video_view_dir/$link_name" 2>/dev/null || true
          ((++link_count))
        done < <(find "$src_video_dir" -maxdepth 1 -type f -name "$pattern" -print0 2>/dev/null)
      fi
    else
      # Source-scoped stage layout:
      # - Source/video layout: data/<stage>/<source>/<video>/
      # - Source-flat layout:  data/<stage>/<source>/*.json

      # 1) Source-video layout (if present)
      local src_video_dir="$stage_data_root/$source/$folder"
      if [[ -d "$src_video_dir" ]]; then
        while IFS= read -r -d '' f; do
          found_file=true
          local link_name=$(basename "$f")
          local rel_target
          rel_target=$(realpath --relative-to="$video_view_dir" "$f")
          ln -sf "$rel_target" "$video_view_dir/$link_name" 2>/dev/null || true
          ((++link_count))
        done < <(find "$src_video_dir" -maxdepth 1 -type f -name "$pattern" -print0 2>/dev/null)
      fi

      # 2) Source-flat layout (if present)
      local src_source_dir="$stage_data_root/$source"
      if [[ "$found_file" == false && -d "$src_source_dir" ]]; then
        while IFS= read -r -d '' f; do
          found_file=true
          local link_name=$(basename "$f")
          local rel_target
          rel_target=$(realpath --relative-to="$video_view_dir" "$f")
          ln -sf "$rel_target" "$video_view_dir/$link_name" 2>/dev/null || true
          ((++link_count))
        done < <(find "$src_source_dir" -maxdepth 1 -type f -name "*${vid_id}*" -name "$pattern" -print0 2>/dev/null)
      fi

    fi

    # Create .pending marker if no files found
    if [[ "$found_file" == false ]]; then
      touch "$video_view_dir/.pending"
      ((++pending_count))
    fi
  done < "$manifest"

  echo "  $stage_dir: $link_count files, $pending_count pending"
}

# Create full symlink view for all stages
create_full_view() {
  local sub_id="$1"
  local batch_id
  batch_id=$(get_batch_id "$sub_id")

  echo ""
  echo "=== Creating View for $sub_id ==="
  echo ""

  for stage in "${ALL_STAGES[@]}"; do
    create_stage_view "$sub_id" "$stage"
  done

  echo ""
  echo "Browse: $VIEWS_DIR/$batch_id/$sub_id/"
}

# Update status
update_status() {
  local batch_id="$1"
  local sub_id="$2"
  local new_status="$3"
  local status_file="$BATCHES_DIR/${batch_id}.status.json"

  local timestamp
  timestamp=$(date -u +"%Y-%m-%dT%H:%M:%SZ")

  python3 - "$status_file" "$sub_id" "$new_status" "$timestamp" <<'PYEOF'
import json
import sys

status_file, sub_id, new_status, timestamp = sys.argv[1:5]

with open(status_file) as f:
    data = json.load(f)

if sub_id not in data["sub_batches"]:
    print(f"ERROR: Sub-batch {sub_id} not found", file=sys.stderr)
    sys.exit(1)

data["sub_batches"][sub_id]["status"] = new_status
if new_status == "approved":
    data["sub_batches"][sub_id]["approved_at"] = timestamp
elif new_status == "pending_review":
    data["sub_batches"][sub_id]["completed_at"] = timestamp

with open(status_file, "w") as f:
    json.dump(data, f, indent=2)
    f.write("\n")
PYEOF
}

# Approve sub-batch
approve_sub_batch() {
  local sub_id="$1"
  local batch_id
  batch_id=$(get_batch_id "$sub_id")

  if [[ "$DRY_RUN" == true ]]; then
    echo "[DRY RUN] Would mark $sub_id as approved"
    return 0
  fi

  update_status "$batch_id" "$sub_id" "approved"

  # Remove symlink view folder
  local view_dir="$VIEWS_DIR/$batch_id/$sub_id"
  if [[ -d "$view_dir" ]]; then
    echo "Removing view folder: $view_dir"
    rm -rf "$view_dir"
  fi

  # Also clean old-style view folder if exists
  local old_view_dir="$DATA_DIR/$sub_id"
  if [[ -d "$old_view_dir" ]]; then
    rm -rf "$old_view_dir"
  fi

  echo "Approved: $sub_id"
}

validate_sub_batch() {
  local sub_id="$1"
  local manifest="$BATCHES_DIR/${sub_id}.txt"
  if [[ ! -f "$manifest" ]]; then
    error "Sub-batch manifest not found: $manifest"
  fi

  local rc=0
  local -a validate_args=(--manifest "$manifest")
  local -a confidence_trace_args=(--manifest "$manifest" --strict-missing)
  local -a batch_report_args=()
  local -a stage_report_check_args=()
  local waiver_file="$VALIDATE_WAIVER_FILE"
  local quarantine_file="$VALIDATE_QUARANTINE_FILE"

  # Always-on: deep checks, stage reports, quarantine emission
  validate_args+=(--quarantine-level "$CFG_QUARANTINE_LEVEL")
  validate_args+=(--check-stage05-audio --check-stage08-report --check-stage09-chunks)
  validate_args+=(--emit-stage-reports --emit-quarantine)

  local stage_reports_dir="$REPO_ROOT/data/validation/stage_reports/$sub_id"

  if [[ -n "$VALIDATE_SOURCE" ]]; then
    validate_args+=(--source "$VALIDATE_SOURCE")
    confidence_trace_args+=(--source "$VALIDATE_SOURCE")
    batch_report_args+=(--source "$VALIDATE_SOURCE")
    stage_report_check_args+=(--source "$VALIDATE_SOURCE")
    stage_reports_dir="$REPO_ROOT/data/validation/stage_reports/${sub_id}.${VALIDATE_SOURCE}"
  else
    batch_report_args+=(--all)
  fi

  # Auto-detect waivers + quarantine
  if [[ -z "$waiver_file" ]]; then
    local auto_waiver="$WAIVERS_DIR/${sub_id}.json"
    if [[ -f "$auto_waiver" ]]; then
      waiver_file="$auto_waiver"
      echo "Using auto-detected waiver file: $waiver_file"
    fi
  fi
  if [[ -z "$quarantine_file" ]]; then
    local auto_quarantine="$QUARANTINE_DIR/${sub_id}.json"
    if [[ -f "$auto_quarantine" ]]; then
      quarantine_file="$auto_quarantine"
      echo "Using auto-detected quarantine file: $quarantine_file"
    fi
  fi

  if [[ -n "$waiver_file" ]]; then
    validate_args+=(--waiver-file "$waiver_file")
  fi
  if [[ -n "$quarantine_file" ]]; then
    validate_args+=(--quarantine-file "$quarantine_file")
    confidence_trace_args+=(--quarantine-file "$quarantine_file")
  fi

  validate_args+=(--stage-reports-dir "$stage_reports_dir")

  echo ""
  echo "=== Validation: $sub_id ==="
  [[ -n "$quarantine_file" ]] && echo "  Quarantine: $quarantine_file"
  [[ -n "$waiver_file" ]] && echo "  Waivers: $waiver_file"
  echo "  Confidence trace policy: strict-only"
  echo ""

  # 1. Manifest + cross-stage validation
  python3 "$REPO_ROOT/scripts/training-data/validation/validate_manifest.py" "${validate_args[@]}" || rc=$?

  # 2. Stage report contract validation + readiness
  echo ""
  echo "=== Confidence Trace Validation ==="
  echo ""
  python3 "$REPO_ROOT/scripts/training-data/validation/validate_confidence_trace.py" \
    "${confidence_trace_args[@]}" || rc=$?

  # 3. Stage report contract validation + readiness
  if [[ -n "$CFG_MAX_WARNING_CHECKS" ]]; then
    stage_report_check_args+=(--max-warning-checks "$CFG_MAX_WARNING_CHECKS")
  fi
  for rule in "${CFG_MAX_WARNING_CHECKS_BY_TYPE[@]}"; do
    stage_report_check_args+=(--max-warning-check "$rule")
  done
  for rule in "${CFG_MAX_WARNING_CHECKS_BY_CLASS[@]}"; do
    stage_report_check_args+=(--max-warning-class "$rule")
  done
  for chk in "${CFG_BLOCK_WARNING_CHECKS[@]}"; do
    stage_report_check_args+=(--block-warning-check "$chk")
  done
  for cls in "${CFG_BLOCK_WARNING_CLASSES[@]}"; do
    stage_report_check_args+=(--block-warning-class "$cls")
  done
  for rule in "${CFG_REVIEW_WARNING_CLASS_BUDGET_BY_CONTENT_TYPE[@]}"; do
    stage_report_check_args+=(--review-warning-class-budget-by-content-type "$rule")
  done

  echo ""
  echo "=== Stage Report Validation ==="
  echo ""
  python3 "$REPO_ROOT/scripts/training-data/validation/validate_stage_report.py" \
    --dir "$stage_reports_dir" \
    --manifest "$manifest" \
    "${stage_report_check_args[@]}" \
    --emit-canonical-gate \
    --emit-readiness-summary || rc=$?

  # 4. Batch report (statistics + drift detection, no semantic gate in auto mode)
  echo ""
  echo "=== Batch Report ==="
  echo ""
  python3 "$REPO_ROOT/scripts/training-data/validation/batch_report.py" \
    "${batch_report_args[@]}" --manifest "$manifest" --batch-id "$sub_id" --no-write || rc=$?

  return "$rc"
}

# Print pipeline summary
print_pipeline_summary() {
  local sub_id="$1"
  local quarantine_file="$QUARANTINE_DIR/${sub_id}.json"
  local canonical_gate_file="$REPO_ROOT/data/validation/gates/${sub_id}.gate.json"
  local manifest="$BATCHES_DIR/${sub_id}.txt"

  python3 - "$manifest" "$quarantine_file" "$sub_id" "$canonical_gate_file" <<'PYEOF'
import json
import sys
from pathlib import Path

manifest_path = Path(sys.argv[1])
quarantine_path = Path(sys.argv[2])
sub_id = sys.argv[3]

# Count manifest videos
total = 0
for line in manifest_path.read_text(encoding="utf-8").splitlines():
    line = line.strip()
    if line and not line.startswith("#"):
        total += 1

# Load quarantine
quarantined = []
if quarantine_path.exists():
    try:
        data = json.loads(quarantine_path.read_text(encoding="utf-8"))
        quarantined = data.get("videos", [])
    except (json.JSONDecodeError, KeyError):
        pass

q_count = len(quarantined)
p_count = total - q_count

print()
print("=" * 56)
print(f"  Pipeline Summary: {sub_id}")
print("=" * 56)
print(f"  Videos: {total} total, {p_count} passed, {q_count} quarantined")

if quarantined:
    print()
    print("  Quarantined:")
    for v in quarantined:
        vid = v.get("video_id", "???")
        checks = ", ".join(v.get("checks", ["unknown"]))
        print(f"    {vid}  {checks}")

print()
if quarantine_path.exists():
    print(f"  Quarantine: {quarantine_path}")
if Path(sys.argv[4]).exists():
    print(f"  Canonical Gate: {sys.argv[4]}")
print(f"  Reports:    data/validation/stage_reports/{sub_id}/")
print("=" * 56)
print()
PYEOF
}

# Parse arguments
while [[ $# -gt 0 ]]; do
  case "$1" in
    -h|--help)
      usage
      exit 0
      ;;
    --list)
      MODE="list"
      shift
      ;;
    --dry-run)
      DRY_RUN=true
      shift
      ;;
    --status)
      MODE="status"
      shift
      ;;
    --view)
      MODE="view"
      shift
      ;;
    --approve)
      MODE="approve"
      shift
      ;;
    --validate)
      MODE="validate"
      shift
      ;;
    --run)
      MODE="run_pipeline"
      shift
      ;;
    --run-all)
      MODE="run_all"
      shift
      ;;
    --from)
      [[ $# -lt 2 ]] && error "--from requires a stage"
      START_STAGE="$2"
      shift 2
      ;;
    --from=*)
      START_STAGE="${1#--from=}"
      shift
      ;;
    --count)
      [[ $# -lt 2 ]] && error "--count requires a number"
      RUN_ALL_COUNT="$2"
      shift 2
      ;;
    --count=*)
      RUN_ALL_COUNT="${1#--count=}"
      shift
      ;;
    --stage)
      MODE="run_stage"
      STAGE="$2"
      shift 2
      ;;
    --stage=*)
      MODE="run_stage"
      STAGE="${1#--stage=}"
      shift
      ;;
    --from-stage)
      # Legacy alias for --run --from
      MODE="run_pipeline"
      START_STAGE="$2"
      shift 2
      ;;
    --from-stage=*)
      MODE="run_pipeline"
      START_STAGE="${1#--from-stage=}"
      shift
      ;;
    --source)
      [[ $# -lt 2 ]] && error "--source requires a name"
      VALIDATE_SOURCE="$2"
      shift 2
      ;;
    --source=*)
      VALIDATE_SOURCE="${1#--source=}"
      shift
      ;;
    --quarantine-file)
      [[ $# -lt 2 ]] && error "--quarantine-file requires a path"
      RUN_QUARANTINE_FILE="$2"
      VALIDATE_QUARANTINE_FILE="$2"
      shift 2
      ;;
    --quarantine-file=*)
      RUN_QUARANTINE_FILE="${1#--quarantine-file=}"
      VALIDATE_QUARANTINE_FILE="${1#--quarantine-file=}"
      shift
      ;;
    --waiver-file)
      [[ $# -lt 2 ]] && error "--waiver-file requires a path"
      VALIDATE_WAIVER_FILE="$2"
      shift 2
      ;;
    --confidence-band-high-threshold)
      [[ $# -lt 2 ]] && error "--confidence-band-high-threshold requires a value in [0,1]"
      CONFIDENCE_BAND_HIGH_THRESHOLD_OVERRIDE="$(require_probability_01 "$2")" || error "--confidence-band-high-threshold must be a number in [0,1]"
      shift 2
      ;;
    --confidence-band-high-threshold=*)
      CONFIDENCE_BAND_HIGH_THRESHOLD_OVERRIDE="$(require_probability_01 "${1#--confidence-band-high-threshold=}")" || error "--confidence-band-high-threshold must be a number in [0,1]"
      shift
      ;;
    --confidence-band-medium-threshold)
      [[ $# -lt 2 ]] && error "--confidence-band-medium-threshold requires a value in [0,1]"
      CONFIDENCE_BAND_MEDIUM_THRESHOLD_OVERRIDE="$(require_probability_01 "$2")" || error "--confidence-band-medium-threshold must be a number in [0,1]"
      shift 2
      ;;
    --confidence-band-medium-threshold=*)
      CONFIDENCE_BAND_MEDIUM_THRESHOLD_OVERRIDE="$(require_probability_01 "${1#--confidence-band-medium-threshold=}")" || error "--confidence-band-medium-threshold must be a number in [0,1]"
      shift
      ;;
    --min-chunk-confidence)
      [[ $# -lt 2 ]] && error "--min-chunk-confidence requires a value in [0,1]"
      MIN_CHUNK_CONFIDENCE_OVERRIDE="$(require_probability_01 "$2")" || error "--min-chunk-confidence must be a number in [0,1]"
      shift 2
      ;;
    --min-chunk-confidence=*)
      MIN_CHUNK_CONFIDENCE_OVERRIDE="$(require_probability_01 "${1#--min-chunk-confidence=}")" || error "--min-chunk-confidence must be a number in [0,1]"
      shift
      ;;
    --waiver-file=*)
      VALIDATE_WAIVER_FILE="${1#--waiver-file=}"
      shift
      ;;
    --parallel|-j)
      [[ $# -lt 2 ]] && error "--parallel requires a number"
      PARALLEL="$2"
      shift 2
      ;;
    --parallel=*)
      PARALLEL="${1#--parallel=}"
      shift
      ;;
    # Legacy flags â€” warn and ignore
    --quality-gate|--validate-deep|--check-stage08-report|--check-stage09-chunks|--check-stage05-audio|--check-stage10|--emit-stage-reports|--emit-quarantine)
      echo "WARNING: $1 is deprecated (validation is now always-on via pipeline.config.json)"
      shift
      ;;
    -*)
      error "Unknown option: $1 (see --help)"
      ;;
    *)
      if [[ -z "$SUB_BATCH_ID" ]]; then
        SUB_BATCH_ID="$1"
      else
        error "Unexpected argument: $1"
      fi
      shift
      ;;
  esac
done

# Handle --list (no sub-batch needed)
if [[ "$MODE" == "list" ]]; then
  list_batches
  exit 0
fi

# Require sub-batch ID for other modes
if [[ -z "$SUB_BATCH_ID" ]]; then
  usage
  exit 1
fi

# Determine if this is a batch ID or sub-batch ID
if is_sub_batch "$SUB_BATCH_ID"; then
  BATCH_ID=$(get_batch_id "$SUB_BATCH_ID")
else
  BATCH_ID="$SUB_BATCH_ID"
fi

if [[ -n "$CONFIDENCE_BAND_HIGH_THRESHOLD_OVERRIDE" && -n "$CONFIDENCE_BAND_MEDIUM_THRESHOLD_OVERRIDE" ]]; then
  python3 - "$CONFIDENCE_BAND_HIGH_THRESHOLD_OVERRIDE" "$CONFIDENCE_BAND_MEDIUM_THRESHOLD_OVERRIDE" <<'PYEOF' || error "confidence band thresholds invalid: medium threshold cannot exceed high threshold"
import sys
high = float(sys.argv[1])
medium = float(sys.argv[2])
if medium > high:
    raise SystemExit(1)
PYEOF
fi


# Execute based on mode
case "$MODE" in
  status)
    if is_sub_batch "$SUB_BATCH_ID"; then
      show_sub_batch_status "$SUB_BATCH_ID"
    else
      show_batch_status "$BATCH_ID"
    fi
    ;;
  run_stage)
    if ! is_sub_batch "$SUB_BATCH_ID"; then
      error "--stage requires a sub-batch ID (e.g., P001.1)"
    fi
    run_stage "$SUB_BATCH_ID" "$STAGE"
    ;;
  run_pipeline)
    if ! is_sub_batch "$SUB_BATCH_ID"; then
      error "--run requires a sub-batch ID (e.g., P001.1)"
    fi
    run_pipeline "$SUB_BATCH_ID" "${START_STAGE:-06}"
    ;;
  run_all)
    if is_sub_batch "$SUB_BATCH_ID"; then
      error "--run-all requires a batch ID (e.g., P001), not a sub-batch"
    fi
    run_all_sub_batches "$BATCH_ID" "$RUN_ALL_COUNT"
    ;;
  view)
    if ! is_sub_batch "$SUB_BATCH_ID"; then
      error "--view requires a sub-batch ID (e.g., P001.1)"
    fi
    create_full_view "$SUB_BATCH_ID"
    ;;
  approve)
    if ! is_sub_batch "$SUB_BATCH_ID"; then
      error "--approve requires a sub-batch ID (e.g., P001.1)"
    fi
    approve_sub_batch "$SUB_BATCH_ID"
    ;;
  validate)
    if ! is_sub_batch "$SUB_BATCH_ID"; then
      error "--validate requires a sub-batch ID (e.g., P001.1)"
    fi
    validate_sub_batch "$SUB_BATCH_ID"
    ;;
  *)
    # Default: show status
    if is_sub_batch "$SUB_BATCH_ID"; then
      show_sub_batch_status "$SUB_BATCH_ID"
    else
      show_batch_status "$BATCH_ID"
    fi
    ;;
esac
