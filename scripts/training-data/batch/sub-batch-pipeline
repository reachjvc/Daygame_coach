#!/usr/bin/env bash
#
# scripts/training-data/batch/sub-batch-pipeline
#
# Pipeline orchestrator for sub-batches. Runs stages with automatic
# post-stage validation and quarantine-and-continue on failures.
#
# Usage:
#   A) Run full pipeline (stages 06â†’09 with validation):
#      ./sub-batch-pipeline P001.1 --run
#      ./sub-batch-pipeline P001.1 --run --from 07    # resume from stage 07
#
#   B) Run all sub-batches in a batch:
#      ./sub-batch-pipeline P001 --run-all
#      ./sub-batch-pipeline P001 --run-all --count 3  # next 3 incomplete
#
#   C) Run a single stage (with post-stage validation):
#      ./sub-batch-pipeline P001.1 --stage 06
#
#   D) List / status / view / validate / approve:
#      ./sub-batch-pipeline --list
#      ./sub-batch-pipeline P001.1 --status
#      ./sub-batch-pipeline P001 --status
#      ./sub-batch-pipeline P001.1 --view
#      ./sub-batch-pipeline P001.1 --validate
#      ./sub-batch-pipeline P001.1 --approve

set -euo pipefail

SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
REPO_ROOT="$(cd "${SCRIPT_DIR}/../../.." && pwd)"
cd "$REPO_ROOT"

BATCHES_DIR="$REPO_ROOT/docs/pipeline/batches"
WAIVERS_DIR="$REPO_ROOT/docs/pipeline/waivers"
QUARANTINE_DIR="$REPO_ROOT/data/validation/quarantine"
DATA_DIR="$REPO_ROOT/data"
VIEWS_DIR="$REPO_ROOT/batch-views"

# Stage definitions (all stages for view generation)
declare -A STAGE_DIRS=(
  ["01"]="01.download"
  ["02"]="02.EXT.transcribe"
  ["03"]="03.EXT.align"
  ["04"]="04.EXT.diarize"
  ["05"]="05.EXT.audio-features"
  ["06"]="06.LLM.video-type"
  ["06b"]="06b.LLM.verify"
  ["06c"]="06c.DET.patched"
  ["06d"]="06d.DET.sanitized"
  ["06e"]="06e.LLM.quality-check"
  ["06f"]="06f.DET.damage-map"
  ["06g"]="06g.LLM.damage-adjudicator"
  ["06h"]="06h.DET.confidence-propagation"
  ["07"]="07.LLM.content"
  ["08"]="08.DET.taxonomy-validation"
  ["09"]="09.EXT.chunks"
)
# Stages with nested source/video folder structure
NESTED_STAGES=("01" "02" "03" "04" "05")
# Stages that often write source-scoped flat outputs (data/<stage>/<source>/*.json) in --manifest mode
FLAT_STAGES=("06" "06b" "06c" "06d" "06e" "06f" "06g" "06h" "07" "08" "09")

declare -A STAGE_SCRIPTS=(
  ["06"]="$REPO_ROOT/scripts/training-data/06.LLM.video-type"
  ["06b"]="$REPO_ROOT/scripts/training-data/06b.LLM.verify"
  ["06c"]="$REPO_ROOT/scripts/training-data/06c.DET.patch"
  ["06d"]="$REPO_ROOT/scripts/training-data/06d.DET.sanitize"
  ["06e"]="$REPO_ROOT/scripts/training-data/06e.LLM.quality-check"
  ["06f"]="$REPO_ROOT/scripts/training-data/06f.DET.damage-map"
  ["06g"]="$REPO_ROOT/scripts/training-data/06g.LLM.damage-adjudicator"
  ["06h"]="$REPO_ROOT/scripts/training-data/06h.DET.confidence-propagation"
  ["07"]="$REPO_ROOT/scripts/training-data/07.LLM.content"
  ["08"]="$REPO_ROOT/scripts/training-data/08.DET.taxonomy-validation"
  ["09"]="$REPO_ROOT/scripts/training-data/09.EXT.chunk-embed.ts"
)
declare -A STAGE_PATTERNS=(
  ["01"]="*.wav"
  ["02"]="*.full.json"
  ["03"]="*.full.json"
  ["04"]="*.full.json"
  ["05"]="*.audio_features.json"
  ["06"]="*.conversations.json"
  ["06b"]="*.verification.json"
  ["06c"]="*.conversations.json"
  ["06d"]="*.conversations.json"
  ["06e"]="*.quality-check.json"
  ["06f"]="*.damage-map.json"
  ["06g"]="*.damage-adjudication.json"
  ["06h"]="*.confidence.report.json"
  ["07"]="*.enriched.json"
  ["08"]="*.report.json"
  ["09"]="*.chunks.json"
)
# Stages that can be run via this pipeline
RUNNABLE_STAGES=("06" "06b" "06c" "06d" "06e" "06f" "06g" "06h" "07" "08" "09")
# Ordered stage sequence for --from-stage
STAGE_ORDER=("06" "06b" "06c" "06d" "06e" "06f" "06g" "06h" "07" "08" "09")
# All stages for view generation
ALL_STAGES=("01" "02" "03" "04" "05" "06" "06b" "06c" "06d" "06e" "06f" "06g" "06h" "07" "08" "09")

# Modes
MODE=""
DRY_RUN=false
STAGE=""
START_STAGE=""
SUB_BATCH_ID=""
BATCH_ID=""
RUN_ALL_COUNT=0
VALIDATE_SOURCE=""
VALIDATE_WAIVER_FILE=""
VALIDATE_QUARANTINE_FILE=""
RUN_QUARANTINE_FILE=""

# Config-driven defaults (loaded from pipeline.config.json)
CFG_STAGE07_GATE_POLICY="approve_only"
CFG_QUARANTINE_LEVEL="error"
CFG_ALLOW_REVIEW_INGEST=false
CFG_MAX_WARNING_CHECKS="3"
CFG_MAX_WARNING_CHECKS_BY_TYPE=()
CFG_BLOCK_WARNING_CHECKS=()

load_pipeline_config() {
  local config_file="$SCRIPT_DIR/pipeline.config.json"
  if [[ ! -f "$config_file" ]]; then
    echo "WARNING: Pipeline config not found: $config_file (using defaults)"
    return 0
  fi

  eval "$(python3 - "$config_file" <<'PYEOF'
import json
import sys

with open(sys.argv[1]) as f:
    cfg = json.load(f)

v = cfg.get("validation", {})
r = v.get("readiness", {})

print(f'CFG_STAGE07_GATE_POLICY="{v.get("stage07_gate_policy", "approve_only")}"')
print(f'CFG_QUARANTINE_LEVEL="{v.get("quarantine_level", "error")}"')
print(f'CFG_ALLOW_REVIEW_INGEST={"true" if r.get("allow_review_ingest") else "false"}')
print(f'CFG_MAX_WARNING_CHECKS="{r.get("max_warning_checks", 3)}"')

by_type = r.get("max_warning_checks_by_type", {})
if by_type:
    items = " ".join(f'"{k}={v}"' for k, v in by_type.items())
    print(f'CFG_MAX_WARNING_CHECKS_BY_TYPE=({items})')
else:
    print('CFG_MAX_WARNING_CHECKS_BY_TYPE=()')

blocks = r.get("block_warning_checks", [])
if blocks:
    items = " ".join(f'"{b}"' for b in blocks)
    print(f'CFG_BLOCK_WARNING_CHECKS=({items})')
else:
    print('CFG_BLOCK_WARNING_CHECKS=()')
PYEOF
  )"
}

# Load config at startup
load_pipeline_config

usage() {
  cat <<'EOF'
Usage:
  ./sub-batch-pipeline P001.1 --run              Run stages 06â†’09 with auto-validation
  ./sub-batch-pipeline P001.1 --run --from 07    Resume from stage 07
  ./sub-batch-pipeline P001 --run-all            Run all incomplete sub-batches
  ./sub-batch-pipeline P001 --run-all --count 3  Run next 3 incomplete sub-batches
  ./sub-batch-pipeline --list                    List all batches and sub-batches
  ./sub-batch-pipeline P001.1 --status           Per-stage status
  ./sub-batch-pipeline P001 --status             Batch-level status
  ./sub-batch-pipeline P001.1 --stage 06         Run single stage (with post-stage validation)
  ./sub-batch-pipeline P001.1 --view             Create symlink view
  ./sub-batch-pipeline P001.1 --validate         Run end-of-pipeline validation only
  ./sub-batch-pipeline P001.1 --approve          Mark as approved

Options:
  --dry-run                Preview without running
  --from <stage>           With --run, start from this stage (default: 06)
  --count <n>              With --run-all, limit to n sub-batches
  --source <name>          With --validate, only validate one source
  --quarantine-file <path> Override auto-detected quarantine file
  --waiver-file <path>     Override auto-detected waiver file

Config: scripts/training-data/batch/pipeline.config.json
  All validation thresholds and policies are configured there.
EOF
}

error() {
  echo "ERROR: $*" >&2
  exit 1
}

# Extract batch ID from sub-batch ID (P001.1 -> P001)
get_batch_id() {
  local sub_id="$1"
  echo "${sub_id%.*}"
}

# Check if argument looks like a sub-batch (has .N suffix)
is_sub_batch() {
  [[ "$1" =~ \.[0-9]+$ ]]
}

# Extract video ID from filename/path
extract_video_id() {
  local name="$1"
  if [[ "$name" =~ \[([a-zA-Z0-9_-]{11})\] ]]; then
    echo "${BASH_REMATCH[1]}"
  fi
}

# Load manifest video IDs
load_manifest_ids() {
  local manifest="$1"
  grep -v "^#" "$manifest" | while read -r line; do
    extract_video_id "$line"
  done
}

# Build quarantine JSON from Stage 06b.LLM.reverify REJECT verdicts for a manifest.
emit_reverify_quarantine() {
  local manifest="$1"
  local out_path="$2"
  local reverify_root="$REPO_ROOT/data/06b.LLM.reverify"

  python3 - "$manifest" "$out_path" "$reverify_root" <<'PYEOF'
import json
import re
import sys
import time
from pathlib import Path
from typing import Dict, List, Optional, Tuple

manifest_path = Path(sys.argv[1])
out_path = Path(sys.argv[2])
reverify_root = Path(sys.argv[3])

video_re = re.compile(r"\[([A-Za-z0-9_-]{11})\]")

def parse_manifest(path: Path) -> List[Tuple[str, str]]:
    entries: List[Tuple[str, str]] = []
    for raw in path.read_text(encoding="utf-8").splitlines():
        line = raw.strip()
        if not line or line.startswith("#"):
            continue
        parts = line.split("|", 1)
        if len(parts) != 2:
            continue
        source = parts[0].strip()
        folder = parts[1].strip()
        match = video_re.search(folder)
        if not match:
            continue
        entries.append((source, match.group(1)))
    return entries

def pick_verification_path(candidates: List[Path], source: str) -> Optional[Path]:
    if not candidates:
        return None
    source_tag = f"/{source}/"
    for path in candidates:
        normalized = str(path).replace("\\", "/")
        if source_tag in normalized:
            return path
    return candidates[0]

def index_verification_paths(paths) -> Dict[str, List[Path]]:
    idx: Dict[str, List[Path]] = {}
    for path in sorted(paths):
        match = video_re.search(path.name)
        if not match:
            continue
        idx.setdefault(match.group(1), []).append(path)
    return idx

entries = parse_manifest(manifest_path)
source_index: Dict[str, Dict[str, List[Path]]] = {}
root_flat_index: Dict[str, List[Path]] = {}
if reverify_root.exists():
    for source in sorted({src for src, _ in entries}):
        source_dir = reverify_root / source
        if source_dir.exists():
            source_index[source] = index_verification_paths(source_dir.rglob("*.verification.json"))
        else:
            source_index[source] = {}
    root_flat_index = index_verification_paths(reverify_root.glob("*.verification.json"))

reject_video_ids: List[str] = []
reject_videos: List[Dict[str, object]] = []
for source, video_id in entries:
    candidates = source_index.get(source, {}).get(video_id, [])
    if not candidates:
        candidates = root_flat_index.get(video_id, [])
    path = pick_verification_path(candidates, source)
    if path is None:
        continue
    try:
        payload = json.loads(path.read_text(encoding="utf-8"))
    except Exception:
        continue
    verdict_raw = payload.get("verdict") or payload.get("overall_verdict")
    verdict = str(verdict_raw or "").strip().upper()
    if verdict == "REJECT":
        reject_video_ids.append(video_id)
        reject_videos.append({
            "video_id": video_id,
            "source": source,
            "checks": ["reverify_reject"],
            "reasons": [{
                "severity": "error",
                "check": "reverify_reject",
                "message": "Stage 06b.LLM.reverify verdict is REJECT",
            }],
            "verification": str(path),
        })

reject_video_ids = sorted(set(reject_video_ids))
reject_videos.sort(key=lambda row: str(row.get("video_id", "")))

payload = {
    "version": 1,
    "generated_at": time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime()),
    "manifest": str(manifest_path),
    "source_filter": None,
    "quarantine_level": "error",
    "video_count": len(entries),
    "quarantined_video_count": len(reject_video_ids),
    "quarantined_video_ids": reject_video_ids,
    "videos": reject_videos,
    "generator": "sub-batch-pipeline:reverify",
}
out_path.parent.mkdir(parents=True, exist_ok=True)
out_path.write_text(json.dumps(payload, indent=2) + "\n", encoding="utf-8")
print(
    f"[sub-batch-pipeline] Reverify quarantine emitted: {out_path} "
    f"(rejected={len(reject_video_ids)}, manifest_videos={len(entries)})"
)
PYEOF
}

# List all batches
list_batches() {
  echo ""
  echo "=== All Batches ==="
  echo ""

  for status_file in "$BATCHES_DIR"/*.status.json; do
    [[ -f "$status_file" ]] || continue
    python3 - "$status_file" <<'PYEOF'
import json
import sys

with open(sys.argv[1]) as f:
    data = json.load(f)

batch_id = data["batch_id"]
total = data["total_videos"]
sub_count = len(data["sub_batches"])

# Count statuses
counts = {"approved": 0, "pending_review": 0, "in_progress": 0, "not_started": 0}
for info in data["sub_batches"].values():
    counts[info["status"]] = counts.get(info["status"], 0) + 1

print(f"{batch_id}: {total} videos in {sub_count} sub-batches")
print(f"  approved: {counts['approved']}, pending: {counts['pending_review']}, in_progress: {counts['in_progress']}, not_started: {counts['not_started']}")
PYEOF
  done

  # List batches without status files (not yet split)
  for batch_file in "$BATCHES_DIR"/P*.txt; do
    [[ -f "$batch_file" ]] || continue
    batch_name=$(basename "$batch_file" .txt)
    # Skip sub-batch files (have dots)
    [[ "$batch_name" =~ \. ]] && continue
    status_file="$BATCHES_DIR/${batch_name}.status.json"
    if [[ ! -f "$status_file" ]]; then
      count=$(grep -v "^#" "$batch_file" | grep -c . || echo 0)
      echo "$batch_name: $count videos (not split yet - run sub-batch-create)"
    fi
  done
}

# Show per-stage status for a sub-batch
show_sub_batch_status() {
  local sub_id="$1"
  local batch_id
  batch_id=$(get_batch_id "$sub_id")

  local manifest="$BATCHES_DIR/${sub_id}.txt"
  if [[ ! -f "$manifest" ]]; then
    error "Sub-batch manifest not found: $manifest"
  fi

  # Get video IDs from manifest
  local -a video_ids=()
  while IFS='|' read -r source folder || [[ -n "$source" ]]; do
    [[ "$source" =~ ^# ]] && continue
    [[ -z "$source" ]] && continue
    # Trim leading/trailing whitespace without collapsing internal spaces
    source="${source#"${source%%[![:space:]]*}"}"
    source="${source%"${source##*[![:space:]]}"}"
    folder="${folder#"${folder%%[![:space:]]*}"}"
    folder="${folder%"${folder##*[![:space:]]}"}"
    vid_id=$(extract_video_id "$folder")
    [[ -n "$vid_id" ]] && video_ids+=("$vid_id:$source:$folder")
  done < "$manifest"

  local total=${#video_ids[@]}

  echo ""
  echo "=== Sub-batch $sub_id Status ($total videos) ==="
  echo ""

  for stage in "${RUNNABLE_STAGES[@]}"; do
    local stage_dir="${STAGE_DIRS[$stage]}"
    local pattern="${STAGE_PATTERNS[$stage]}"
    local found=0
    local missing=()

    for entry in "${video_ids[@]}"; do
      IFS=':' read -r vid_id source folder <<< "$entry"

      # Check if output exists for this video
      local found_file=false

      # Search in stage directory
      if [[ -d "$DATA_DIR/$stage_dir" ]]; then
        while IFS= read -r -d '' f; do
          found_file=true
          break
        # Follow symlinks when the stage dir itself is a symlink (common in worktree-based experiments).
        done < <(find -L "$DATA_DIR/$stage_dir" -name "*${vid_id}*" -name "$pattern" -print0 2>/dev/null)
      fi

      if [[ "$found_file" == true ]]; then
        ((++found))
      else
        missing+=("$folder")
      fi
    done

    local status_icon="â¬œ"
    if [[ $found -eq $total ]]; then
      status_icon="âœ…"
    elif [[ $found -gt 0 ]]; then
      status_icon="ðŸ”¶"
    fi

    printf "  %s Stage %s: %d/%d\n" "$status_icon" "$stage" "$found" "$total"

    # Show missing files if partially complete
    if [[ $found -gt 0 && $found -lt $total && ${#missing[@]} -le 5 ]]; then
      for m in "${missing[@]}"; do
        echo "      missing: $m"
      done
    elif [[ ${#missing[@]} -gt 5 ]]; then
      echo "      (${#missing[@]} missing)"
    fi
  done

  echo ""

  # Show current status from status file
  local status_file="$BATCHES_DIR/${batch_id}.status.json"
  if [[ -f "$status_file" ]]; then
    python3 - "$status_file" "$sub_id" <<'PYEOF'
import json
import sys

with open(sys.argv[1]) as f:
    data = json.load(f)

sub_id = sys.argv[2]
info = data["sub_batches"].get(sub_id, {})
status = info.get("status", "unknown")
print(f"Overall status: {status}")
PYEOF
  fi
}

# Show batch-level status (all sub-batches)
show_batch_status() {
  local batch_id="$1"
  local status_file="$BATCHES_DIR/${batch_id}.status.json"

  if [[ ! -f "$status_file" ]]; then
    error "Status file not found: $status_file (run sub-batch-create first)"
  fi

  python3 - "$status_file" <<'PYEOF'
import json
import sys

with open(sys.argv[1]) as f:
    data = json.load(f)

print(f"\nBatch {data['batch_id']} Status")
print(f"Total videos: {data['total_videos']}")
print(f"Sub-batch size: {data['sub_batch_size']}")
print()

status_symbols = {
    "approved": "\u2705",
    "pending_review": "\U0001F50D",
    "in_progress": "\u23F3",
    "not_started": "\u2B1C",
}

for sub_id in sorted(data["sub_batches"].keys(), key=lambda x: int(x.split(".")[-1])):
    info = data["sub_batches"][sub_id]
    status = info["status"]
    symbol = status_symbols.get(status, "?")
    count = info.get("video_count", "?")
    print(f"  {symbol} {sub_id}: {status} ({count} videos)")

# Summary
counts = {}
for info in data["sub_batches"].values():
    s = info["status"]
    counts[s] = counts.get(s, 0) + info.get("video_count", 0)

print()
print("Summary:")
for status in ["approved", "pending_review", "in_progress", "not_started"]:
    if status in counts:
        print(f"  {status}: {counts[status]} videos")
PYEOF
}

# Run a single stage
run_stage() {
  local sub_id="$1"
  local stage="$2"
  local batch_id
  batch_id=$(get_batch_id "$sub_id")

  local manifest="$BATCHES_DIR/${sub_id}.txt"
  if [[ ! -f "$manifest" ]]; then
    error "Sub-batch manifest not found: $manifest"
  fi

  local script="${STAGE_SCRIPTS[$stage]:-}"
  if [[ -z "$script" ]]; then
    error "Unknown stage: $stage (valid: ${RUNNABLE_STAGES[*]})"
  fi

  local stage_dir="${STAGE_DIRS[$stage]}"
  local -a stage_args=(--manifest "$manifest")
  local run_reverify=false
  local reverify_script="$REPO_ROOT/scripts/training-data/06b.LLM.reverify"
  local -a reverify_args=(--manifest "$manifest")
  local quarantine_file=""
  local auto_quarantine="$QUARANTINE_DIR/${sub_id}.json"

  echo ""
  echo "=== Running Stage $stage for $sub_id ==="
  echo ""

  if [[ "$stage" == "07" ]]; then
    stage_args+=(--verification-gate-policy "$RUN_VERIFICATION_GATE_POLICY")
    if [[ "$RUN_VERIFICATION_GATE_POLICY" == "reverify_patched" ]]; then
      run_reverify=true
    fi
    quarantine_file="$RUN_QUARANTINE_FILE"
    if [[ -z "$quarantine_file" ]]; then
      if [[ -f "$auto_quarantine" ]]; then
        quarantine_file="$auto_quarantine"
      fi
    fi
  fi

  if [[ "$DRY_RUN" == true ]]; then
    if [[ "$run_reverify" == true ]]; then
      echo "[DRY RUN] Would run pre-gate reverify: $reverify_script ${reverify_args[*]}"
    fi
    if [[ "$stage" == "07" && -z "$quarantine_file" && "$run_reverify" == true ]]; then
      echo "[DRY RUN] Would synthesize quarantine from reverify verdicts: $auto_quarantine"
      stage_args+=(--quarantine-file "$auto_quarantine")
    elif [[ "$stage" == "07" && -n "$quarantine_file" ]]; then
      stage_args+=(--quarantine-file "$quarantine_file")
    fi
    echo "[DRY RUN] Would run: $script ${stage_args[*]}"
    return 0
  fi

  # Update status to in_progress
  local status_file="$BATCHES_DIR/${batch_id}.status.json"
  if [[ -f "$status_file" ]]; then
    python3 - "$status_file" "$sub_id" "$stage" <<'PYEOF'
import json
import sys

status_file, sub_id, stage = sys.argv[1:4]

with open(status_file) as f:
    data = json.load(f)

if sub_id in data["sub_batches"]:
    data["sub_batches"][sub_id]["status"] = "in_progress"
    data["sub_batches"][sub_id]["current_stage"] = stage

with open(status_file, "w") as f:
    json.dump(data, f, indent=2)
    f.write("\n")
PYEOF
  fi

  # Optional pre-gate reverify pass
  if [[ "$run_reverify" == true ]]; then
    if [[ ! -x "$reverify_script" ]]; then
      echo "ERROR: Missing executable reverify script: $reverify_script"
      return 1
    fi
    echo "Running Stage 06b.LLM.reverify prerequisite (policy=reverify_patched)"
    if ! "$reverify_script" "${reverify_args[@]}"; then
      echo "Stage 06b.LLM.reverify: FAILED"
      return 1
    fi
    echo "Stage 06b.LLM.reverify: COMPLETE"

    if [[ -z "$quarantine_file" ]]; then
      echo "Synthesizing quarantine file from 06b.LLM.reverify REJECT verdicts: $auto_quarantine"
      if emit_reverify_quarantine "$manifest" "$auto_quarantine"; then
        quarantine_file="$auto_quarantine"
      else
        echo "WARNING: Failed to synthesize reverify quarantine; continuing without quarantine file"
      fi
    fi
  fi

  if [[ "$stage" == "07" ]]; then
    if [[ -n "$quarantine_file" && -f "$quarantine_file" ]]; then
      stage_args+=(--quarantine-file "$quarantine_file")
      echo "Using quarantine file: $quarantine_file"
    elif [[ -n "$quarantine_file" ]]; then
      echo "WARNING: Quarantine file not found, ignoring: $quarantine_file"
    fi
  fi

  # Build execution command (TypeScript stages need tsx loader)
  local -a exec_cmd
  if [[ "$script" == *.ts ]]; then
    local tsx_loader="$REPO_ROOT/node_modules/tsx/dist/loader.mjs"
    if [[ ! -f "$tsx_loader" ]]; then
      echo "ERROR: tsx loader not found: $tsx_loader"
      return 1
    fi
    exec_cmd=(node --import "$tsx_loader" "$script")
  else
    exec_cmd=("$script")
  fi

  # Run the stage
  if "${exec_cmd[@]}" "${stage_args[@]}"; then
    echo ""
    echo "Stage $stage: COMPLETE"
  else
    local stage_rc=$?
    echo ""
    echo "Stage $stage: FAILED (exit code $stage_rc)"
    # For stage 08, try to quarantine per-video failures instead of hard-failing
    if [[ "$stage" == "08" ]]; then
      local report_file
      report_file=$(find "$DATA_DIR/08.DET.taxonomy-validation" -name "*.report.json" -newer "$manifest" -print -quit 2>/dev/null)
      if [[ -n "$report_file" ]]; then
        echo "--- Checking for per-video quarantine from Stage 08 report ---"
        python3 "$SCRIPT_DIR/quarantine_updater.py" \
          --quarantine-file "$auto_quarantine" \
          --stage08-report "$report_file" 2>&1 || true
      fi
    fi
    return 1
  fi

  # Create symlink view
  create_stage_view "$sub_id" "$stage"

  # --- Post-stage validation hooks ---
  run_post_stage_validation "$sub_id" "$stage" "$manifest" "$auto_quarantine"
  local validate_rc=$?
  if [[ $validate_rc -eq 2 ]]; then
    return 2  # all videos quarantined
  fi

  return 0
}

# Count non-comment lines in manifest
count_manifest_videos() {
  grep -v "^#" "$1" | grep -c . || echo 0
}

# Post-stage validation hooks
run_post_stage_validation() {
  local sub_id="$1"
  local stage="$2"
  local manifest="$3"
  local quarantine_file="$4"

  case "$stage" in
    06b)
      echo ""
      echo "--- Post-stage validation: checking 06b verdicts ---"
      python3 "$SCRIPT_DIR/quarantine_updater.py" \
        --quarantine-file "$quarantine_file" \
        --stage06b-dir "$DATA_DIR/06b.LLM.verify" \
        --manifest "$manifest" 2>&1 || true
      ;;
    07)
      echo ""
      echo "--- Post-stage validation: cross-stage (06/06c vs 07) ---"
      local cross_stage_json
      cross_stage_json=$(python3 "$REPO_ROOT/scripts/training-data/validation/validate_cross_stage.py" \
        --manifest "$manifest" --json 2>/dev/null) || true
      if [[ -n "$cross_stage_json" ]]; then
        echo "$cross_stage_json" | python3 "$SCRIPT_DIR/quarantine_updater.py" \
          --quarantine-file "$quarantine_file" \
          --stage 07 2>&1 || true
        # Show summary
        echo "$cross_stage_json" | python3 -c "
import json, sys
data = json.load(sys.stdin)
s = data.get('summary', {})
errors = s.get('errors', 0)
warnings = s.get('warnings', 0)
total = s.get('total_checks', 0)
status = 'PASS' if errors == 0 else 'FAIL'
print(f'  Cross-stage: {status} ({total} checks, {errors} errors, {warnings} warnings)')
" 2>/dev/null || true
      fi
      ;;
    09)
      echo ""
      echo "--- Post-stage validation: chunk integrity ---"
      local chunks_json
      chunks_json=$(python3 "$REPO_ROOT/scripts/training-data/validation/validate_chunks.py" \
        --manifest "$manifest" --json 2>/dev/null) || true
      if [[ -n "$chunks_json" ]]; then
        echo "$chunks_json" | python3 "$SCRIPT_DIR/quarantine_updater.py" \
          --quarantine-file "$quarantine_file" \
          --stage 09 2>&1 || true
        # Show summary
        echo "$chunks_json" | python3 -c "
import json, sys
data = json.load(sys.stdin)
errors = data.get('issues_summary', {}).get('errors', 0)
warnings = data.get('issues_summary', {}).get('warnings', 0)
files = data.get('processed_files', 0)
status = 'PASS' if errors == 0 else 'FAIL'
print(f'  Chunks: {status} ({files} files, {errors} errors, {warnings} warnings)')
" 2>/dev/null || true
      fi
      ;;
  esac

  # Check if all videos are now quarantined
  if [[ -f "$quarantine_file" ]]; then
    local manifest_count
    manifest_count=$(count_manifest_videos "$manifest")
    local all_quarantined
    all_quarantined=$(python3 -c "
import json, sys
q = json.load(open(sys.argv[1]))
m_count = int(sys.argv[2])
print('yes' if len(q.get('quarantined_video_ids',[])) >= m_count else 'no')
" "$quarantine_file" "$manifest_count" 2>/dev/null) || all_quarantined="no"
    if [[ "$all_quarantined" == "yes" ]]; then
      echo ""
      echo "ALL videos in $sub_id are quarantined. Skipping remaining stages."
      return 2
    fi
  fi

  return 0
}

# Run pipeline: stages sequentially with post-stage validation + end-of-run validation
run_pipeline() {
  local sub_id="$1"
  local start_stage="${2:-06}"

  # Find start index in STAGE_ORDER
  local start_idx=-1
  for i in "${!STAGE_ORDER[@]}"; do
    if [[ "${STAGE_ORDER[$i]}" == "$start_stage" ]]; then
      start_idx=$i
      break
    fi
  done

  if [[ $start_idx -eq -1 ]]; then
    error "Unknown stage: $start_stage (valid: ${STAGE_ORDER[*]})"
  fi

  local remaining=("${STAGE_ORDER[@]:$start_idx}")
  local manifest="$BATCHES_DIR/${sub_id}.txt"
  local manifest_count
  manifest_count=$(count_manifest_videos "$manifest")

  echo ""
  echo "========================================"
  echo "  Pipeline: $sub_id"
  echo "  Stages: ${remaining[*]}"
  echo "  Videos: $manifest_count"
  echo "  Config: $SCRIPT_DIR/pipeline.config.json"
  echo "========================================"

  local pipeline_failed=false
  for stage in "${remaining[@]}"; do
    run_stage "$sub_id" "$stage"
    local rc=$?
    if [[ $rc -eq 2 ]]; then
      echo ""
      echo "Sub-batch $sub_id: all videos quarantined at stage $stage"
      break
    elif [[ $rc -ne 0 ]]; then
      echo ""
      echo "PIPELINE HALTED at stage $stage (non-zero exit)"
      pipeline_failed=true
      break
    fi
  done

  echo ""
  echo "=== End-of-run validation: $sub_id ==="
  echo ""
  validate_sub_batch "$sub_id"

  echo ""
  print_pipeline_summary "$sub_id"

  if [[ "$pipeline_failed" == true ]]; then
    return 1
  fi
  return 0
}

# Run all incomplete sub-batches in a batch
run_all_sub_batches() {
  local batch_id="$1"
  local count="${2:-0}"
  local status_file="$BATCHES_DIR/${batch_id}.status.json"

  if [[ ! -f "$status_file" ]]; then
    error "Status file not found: $status_file (run sub-batch-create first)"
  fi

  local sub_batches
  sub_batches=$(python3 - "$status_file" "$count" <<'PYEOF'
import json
import sys

with open(sys.argv[1]) as f:
    data = json.load(f)

count = int(sys.argv[2])
found = 0

for sub_id in sorted(data["sub_batches"].keys(), key=lambda x: int(x.split(".")[-1])):
    info = data["sub_batches"][sub_id]
    if info["status"] in ("not_started", "in_progress"):
        print(sub_id)
        found += 1
        if count > 0 and found >= count:
            break
PYEOF
  )

  if [[ -z "$sub_batches" ]]; then
    echo "No incomplete sub-batches found in $batch_id"
    return 0
  fi

  local total_subs
  total_subs=$(echo "$sub_batches" | wc -l)
  echo ""
  echo "========================================"
  echo "  Batch run: $batch_id ($total_subs sub-batches)"
  echo "  Sub-batches: $(echo $sub_batches | tr '\n' ' ')"
  echo "========================================"

  local pass_count=0
  local fail_count=0

  for sub_id in $sub_batches; do
    run_pipeline "$sub_id" "${START_STAGE:-06}"
    local rc=$?
    if [[ $rc -eq 0 ]]; then
      ((++pass_count))
    else
      ((++fail_count))
      echo ""
      echo "Sub-batch $sub_id had failures. Continuing to next sub-batch."
    fi
  done

  echo ""
  echo "========================================"
  echo "  Batch $batch_id complete"
  echo "  Passed: $pass_count / $total_subs"
  if [[ $fail_count -gt 0 ]]; then
    echo "  Failed: $fail_count / $total_subs"
  fi
  echo "========================================"
  echo ""
  show_batch_status "$batch_id"
}

# Legacy alias
run_from_stage() {
  run_pipeline "$1" "$2"
}

# Check if stage is nested (has source/video folder structure)
is_nested_stage() {
  local stage="$1"
  for s in "${NESTED_STAGES[@]}"; do
    [[ "$s" == "$stage" ]] && return 0
  done
  return 1
}

# Create symlink view for a stage
create_stage_view() {
  local sub_id="$1"
  local stage="$2"
  local batch_id
  batch_id=$(get_batch_id "$sub_id")

  local stage_dir="${STAGE_DIRS[$stage]:-}"
  [[ -z "$stage_dir" ]] && return 0

  local pattern="${STAGE_PATTERNS[$stage]}"
  local manifest="$BATCHES_DIR/${sub_id}.txt"
  local view_base="$VIEWS_DIR/$batch_id/$sub_id/$stage_dir"
  local stage_data_root="$DATA_DIR/$stage_dir"
  if [[ -L "$stage_data_root" ]]; then
    stage_data_root=$(realpath "$stage_data_root")
  fi

  # Clear existing view for this stage
  rm -rf "$view_base" 2>/dev/null || true

  local link_count=0
  local pending_count=0

  # Process each video from manifest
  while IFS='|' read -r source folder || [[ -n "$source" ]]; do
    [[ "$source" =~ ^# ]] && continue
    [[ -z "$source" ]] && continue
    # Trim leading/trailing whitespace without collapsing internal spaces
    source="${source#"${source%%[![:space:]]*}"}"
    source="${source%"${source##*[![:space:]]}"}"
    folder="${folder#"${folder%%[![:space:]]*}"}"
    folder="${folder%"${folder##*[![:space:]]}"}"
    vid_id=$(extract_video_id "$folder")
    [[ -z "$vid_id" ]] && continue

    # Create source/video folder structure in view
    local video_view_dir="$view_base/$source/$folder"
    mkdir -p "$video_view_dir"

    local found_file=false

	    if is_nested_stage "$stage"; then
	      # Nested stage: data/<stage>/<source>/<video>/
	      local src_video_dir="$stage_data_root/$source/$folder"
	      if [[ ! -d "$src_video_dir" && -d "$stage_data_root/$source" ]]; then
	        # Folder names in manifests are display-only and can drift (unicode punctuation, title edits).
	        # Resolve nested stage dirs by video_id when the exact folder is missing.
	        while IFS= read -r -d '' d; do
	          local bn
	          bn=$(basename "$d")
	          local alt_id
	          alt_id=$(extract_video_id "$bn")
	          if [[ -n "$alt_id" && "$alt_id" == "$vid_id" ]]; then
	            src_video_dir="$d"
	            break
	          fi
	        done < <(find "$stage_data_root/$source" -mindepth 1 -maxdepth 1 -type d -print0 2>/dev/null)
	      fi
	      if [[ -d "$src_video_dir" ]]; then
	        # Symlink all matching files in the video folder
	        while IFS= read -r -d '' f; do
	          found_file=true
          local link_name=$(basename "$f")
          local rel_target
          rel_target=$(realpath --relative-to="$video_view_dir" "$f")
          ln -sf "$rel_target" "$video_view_dir/$link_name" 2>/dev/null || true
          ((++link_count))
        done < <(find "$src_video_dir" -maxdepth 1 -type f -name "$pattern" -print0 2>/dev/null)
      fi
    else
      # Source-scoped flat stage (06-07 family):
      # - Preferred (future/canonical): data/<stage>/<source>/<video>/
      # - Current (manifest runs):      data/<stage>/<source>/*.json
      # - Legacy (ad-hoc runs):         data/<stage>/*.json

      # 1) Source-video layout (if present)
      local src_video_dir="$stage_data_root/$source/$folder"
      if [[ -d "$src_video_dir" ]]; then
        while IFS= read -r -d '' f; do
          found_file=true
          local link_name=$(basename "$f")
          local rel_target
          rel_target=$(realpath --relative-to="$video_view_dir" "$f")
          ln -sf "$rel_target" "$video_view_dir/$link_name" 2>/dev/null || true
          ((++link_count))
        done < <(find "$src_video_dir" -maxdepth 1 -type f -name "$pattern" -print0 2>/dev/null)
      fi

      # 2) Source-flat layout (if present)
      local src_source_dir="$stage_data_root/$source"
      if [[ "$found_file" == false && -d "$src_source_dir" ]]; then
        while IFS= read -r -d '' f; do
          found_file=true
          local link_name=$(basename "$f")
          local rel_target
          rel_target=$(realpath --relative-to="$video_view_dir" "$f")
          ln -sf "$rel_target" "$video_view_dir/$link_name" 2>/dev/null || true
          ((++link_count))
        done < <(find "$src_source_dir" -maxdepth 1 -type f -name "*${vid_id}*" -name "$pattern" -print0 2>/dev/null)
      fi

      # 3) Root-flat fallback
      if [[ "$found_file" == false && -d "$stage_data_root" ]]; then
        while IFS= read -r -d '' f; do
          found_file=true
          local link_name=$(basename "$f")
          local rel_target
          rel_target=$(realpath --relative-to="$video_view_dir" "$f")
          ln -sf "$rel_target" "$video_view_dir/$link_name" 2>/dev/null || true
          ((++link_count))
        done < <(find "$stage_data_root" -maxdepth 1 -type f -name "*${vid_id}*" -name "$pattern" -print0 2>/dev/null)
      fi
    fi

    # Create .pending marker if no files found
    if [[ "$found_file" == false ]]; then
      touch "$video_view_dir/.pending"
      ((++pending_count))
    fi
  done < "$manifest"

  echo "  $stage_dir: $link_count files, $pending_count pending"
}

# Create full symlink view for all stages
create_full_view() {
  local sub_id="$1"
  local batch_id
  batch_id=$(get_batch_id "$sub_id")

  echo ""
  echo "=== Creating View for $sub_id ==="
  echo ""

  for stage in "${ALL_STAGES[@]}"; do
    create_stage_view "$sub_id" "$stage"
  done

  echo ""
  echo "Browse: $VIEWS_DIR/$batch_id/$sub_id/"
}

# Update status
update_status() {
  local batch_id="$1"
  local sub_id="$2"
  local new_status="$3"
  local status_file="$BATCHES_DIR/${batch_id}.status.json"

  local timestamp
  timestamp=$(date -u +"%Y-%m-%dT%H:%M:%SZ")

  python3 - "$status_file" "$sub_id" "$new_status" "$timestamp" <<'PYEOF'
import json
import sys

status_file, sub_id, new_status, timestamp = sys.argv[1:5]

with open(status_file) as f:
    data = json.load(f)

if sub_id not in data["sub_batches"]:
    print(f"ERROR: Sub-batch {sub_id} not found", file=sys.stderr)
    sys.exit(1)

data["sub_batches"][sub_id]["status"] = new_status
if new_status == "approved":
    data["sub_batches"][sub_id]["approved_at"] = timestamp
elif new_status == "pending_review":
    data["sub_batches"][sub_id]["completed_at"] = timestamp

with open(status_file, "w") as f:
    json.dump(data, f, indent=2)
    f.write("\n")
PYEOF
}

# Approve sub-batch
approve_sub_batch() {
  local sub_id="$1"
  local batch_id
  batch_id=$(get_batch_id "$sub_id")

  if [[ "$DRY_RUN" == true ]]; then
    echo "[DRY RUN] Would mark $sub_id as approved"
    return 0
  fi

  update_status "$batch_id" "$sub_id" "approved"

  # Remove symlink view folder
  local view_dir="$VIEWS_DIR/$batch_id/$sub_id"
  if [[ -d "$view_dir" ]]; then
    echo "Removing view folder: $view_dir"
    rm -rf "$view_dir"
  fi

  # Also clean old-style view folder if exists
  local old_view_dir="$DATA_DIR/$sub_id"
  if [[ -d "$old_view_dir" ]]; then
    rm -rf "$old_view_dir"
  fi

  echo "Approved: $sub_id"
}

validate_sub_batch() {
  local sub_id="$1"
  local manifest="$BATCHES_DIR/${sub_id}.txt"
  if [[ ! -f "$manifest" ]]; then
    error "Sub-batch manifest not found: $manifest"
  fi

  local rc=0
  local -a validate_args=(--manifest "$manifest")
  local -a batch_report_args=()
  local -a stage_report_check_args=()
  local waiver_file="$VALIDATE_WAIVER_FILE"
  local quarantine_file="$VALIDATE_QUARANTINE_FILE"

  # Always-on: deep checks, stage reports, quarantine emission
  validate_args+=(--stage07-gate-policy "$CFG_STAGE07_GATE_POLICY")
  validate_args+=(--quarantine-level "$CFG_QUARANTINE_LEVEL")
  validate_args+=(--check-stage05-audio --check-stage08-report --check-stage09-chunks)
  validate_args+=(--emit-stage-reports --emit-quarantine)

  local stage_reports_dir="$REPO_ROOT/data/validation/stage_reports/$sub_id"

  if [[ -n "$VALIDATE_SOURCE" ]]; then
    validate_args+=(--source "$VALIDATE_SOURCE")
    batch_report_args+=(--source "$VALIDATE_SOURCE")
    stage_report_check_args+=(--source "$VALIDATE_SOURCE")
    stage_reports_dir="$REPO_ROOT/data/validation/stage_reports/${sub_id}.${VALIDATE_SOURCE}"
  else
    batch_report_args+=(--all)
  fi

  # Auto-detect waivers + quarantine
  if [[ -z "$waiver_file" ]]; then
    local auto_waiver="$WAIVERS_DIR/${sub_id}.json"
    if [[ -f "$auto_waiver" ]]; then
      waiver_file="$auto_waiver"
      echo "Using auto-detected waiver file: $waiver_file"
    fi
  fi
  if [[ -z "$quarantine_file" ]]; then
    local auto_quarantine="$QUARANTINE_DIR/${sub_id}.json"
    if [[ -f "$auto_quarantine" ]]; then
      quarantine_file="$auto_quarantine"
      echo "Using auto-detected quarantine file: $quarantine_file"
    fi
  fi

  if [[ -n "$waiver_file" ]]; then
    validate_args+=(--waiver-file "$waiver_file")
  fi
  if [[ -n "$quarantine_file" ]]; then
    validate_args+=(--quarantine-file "$quarantine_file")
  fi

  validate_args+=(--stage-reports-dir "$stage_reports_dir")

  echo ""
  echo "=== Validation: $sub_id ==="
  echo "  Gate policy: $CFG_STAGE07_GATE_POLICY"
  [[ -n "$quarantine_file" ]] && echo "  Quarantine: $quarantine_file"
  [[ -n "$waiver_file" ]] && echo "  Waivers: $waiver_file"
  echo ""

  # 1. Manifest + cross-stage validation
  python3 "$REPO_ROOT/scripts/training-data/validation/validate_manifest.py" "${validate_args[@]}" || rc=$?

  # 2. Stage report contract validation + readiness
  if [[ "$CFG_ALLOW_REVIEW_INGEST" == true ]]; then
    stage_report_check_args+=(--allow-review-ingest)
  fi
  if [[ -n "$CFG_MAX_WARNING_CHECKS" ]]; then
    stage_report_check_args+=(--max-warning-checks "$CFG_MAX_WARNING_CHECKS")
  fi
  for rule in "${CFG_MAX_WARNING_CHECKS_BY_TYPE[@]}"; do
    stage_report_check_args+=(--max-warning-check "$rule")
  done
  for chk in "${CFG_BLOCK_WARNING_CHECKS[@]}"; do
    stage_report_check_args+=(--block-warning-check "$chk")
  done

  echo ""
  echo "=== Stage Report Validation ==="
  echo ""
  python3 "$REPO_ROOT/scripts/training-data/validation/validate_stage_report.py" \
    --dir "$stage_reports_dir" \
    --manifest "$manifest" \
    "${stage_report_check_args[@]}" \
    --emit-readiness-summary || rc=$?

  # 3. Batch report (statistics + drift detection, no semantic gate in auto mode)
  echo ""
  echo "=== Batch Report ==="
  echo ""
  python3 "$REPO_ROOT/scripts/training-data/validation/batch_report.py" \
    "${batch_report_args[@]}" --manifest "$manifest" --batch-id "$sub_id" --no-write || rc=$?

  return "$rc"
}

# Print pipeline summary
print_pipeline_summary() {
  local sub_id="$1"
  local quarantine_file="$QUARANTINE_DIR/${sub_id}.json"
  local manifest="$BATCHES_DIR/${sub_id}.txt"

  python3 - "$manifest" "$quarantine_file" "$sub_id" <<'PYEOF'
import json
import sys
from pathlib import Path

manifest_path = Path(sys.argv[1])
quarantine_path = Path(sys.argv[2])
sub_id = sys.argv[3]

# Count manifest videos
total = 0
for line in manifest_path.read_text(encoding="utf-8").splitlines():
    line = line.strip()
    if line and not line.startswith("#"):
        total += 1

# Load quarantine
quarantined = []
if quarantine_path.exists():
    try:
        data = json.loads(quarantine_path.read_text(encoding="utf-8"))
        quarantined = data.get("videos", [])
    except (json.JSONDecodeError, KeyError):
        pass

q_count = len(quarantined)
p_count = total - q_count

print()
print("=" * 56)
print(f"  Pipeline Summary: {sub_id}")
print("=" * 56)
print(f"  Videos: {total} total, {p_count} passed, {q_count} quarantined")

if quarantined:
    print()
    print("  Quarantined:")
    for v in quarantined:
        vid = v.get("video_id", "???")
        checks = ", ".join(v.get("checks", ["unknown"]))
        print(f"    {vid}  {checks}")

print()
if quarantine_path.exists():
    print(f"  Quarantine: {quarantine_path}")
print(f"  Reports:    data/validation/stage_reports/{sub_id}/")
print("=" * 56)
print()
PYEOF
}

# Parse arguments
while [[ $# -gt 0 ]]; do
  case "$1" in
    -h|--help)
      usage
      exit 0
      ;;
    --list)
      MODE="list"
      shift
      ;;
    --dry-run)
      DRY_RUN=true
      shift
      ;;
    --status)
      MODE="status"
      shift
      ;;
    --view)
      MODE="view"
      shift
      ;;
    --approve)
      MODE="approve"
      shift
      ;;
    --validate)
      MODE="validate"
      shift
      ;;
    --run)
      MODE="run_pipeline"
      shift
      ;;
    --run-all)
      MODE="run_all"
      shift
      ;;
    --from)
      [[ $# -lt 2 ]] && error "--from requires a stage"
      START_STAGE="$2"
      shift 2
      ;;
    --from=*)
      START_STAGE="${1#--from=}"
      shift
      ;;
    --count)
      [[ $# -lt 2 ]] && error "--count requires a number"
      RUN_ALL_COUNT="$2"
      shift 2
      ;;
    --count=*)
      RUN_ALL_COUNT="${1#--count=}"
      shift
      ;;
    --stage)
      MODE="run_stage"
      STAGE="$2"
      shift 2
      ;;
    --stage=*)
      MODE="run_stage"
      STAGE="${1#--stage=}"
      shift
      ;;
    --from-stage)
      # Legacy alias for --run --from
      MODE="run_pipeline"
      START_STAGE="$2"
      shift 2
      ;;
    --from-stage=*)
      MODE="run_pipeline"
      START_STAGE="${1#--from-stage=}"
      shift
      ;;
    --source)
      [[ $# -lt 2 ]] && error "--source requires a name"
      VALIDATE_SOURCE="$2"
      shift 2
      ;;
    --source=*)
      VALIDATE_SOURCE="${1#--source=}"
      shift
      ;;
    --quarantine-file)
      [[ $# -lt 2 ]] && error "--quarantine-file requires a path"
      RUN_QUARANTINE_FILE="$2"
      VALIDATE_QUARANTINE_FILE="$2"
      shift 2
      ;;
    --quarantine-file=*)
      RUN_QUARANTINE_FILE="${1#--quarantine-file=}"
      VALIDATE_QUARANTINE_FILE="${1#--quarantine-file=}"
      shift
      ;;
    --waiver-file)
      [[ $# -lt 2 ]] && error "--waiver-file requires a path"
      VALIDATE_WAIVER_FILE="$2"
      shift 2
      ;;
    --waiver-file=*)
      VALIDATE_WAIVER_FILE="${1#--waiver-file=}"
      shift
      ;;
    # Legacy flags â€” warn and ignore
    --quality-gate|--validate-deep|--check-stage08-report|--check-stage09-chunks|--check-stage05-audio|--check-stage10|--emit-stage-reports|--emit-quarantine)
      echo "WARNING: $1 is deprecated (validation is now always-on via pipeline.config.json)"
      shift
      ;;
    --allow-flag)
      CFG_STAGE07_GATE_POLICY="allow_flag"
      shift
      ;;
    -*)
      error "Unknown option: $1 (see --help)"
      ;;
    *)
      if [[ -z "$SUB_BATCH_ID" ]]; then
        SUB_BATCH_ID="$1"
      else
        error "Unexpected argument: $1"
      fi
      shift
      ;;
  esac
done

# Handle --list (no sub-batch needed)
if [[ "$MODE" == "list" ]]; then
  list_batches
  exit 0
fi

# Require sub-batch ID for other modes
if [[ -z "$SUB_BATCH_ID" ]]; then
  usage
  exit 1
fi

# Determine if this is a batch ID or sub-batch ID
if is_sub_batch "$SUB_BATCH_ID"; then
  BATCH_ID=$(get_batch_id "$SUB_BATCH_ID")
else
  BATCH_ID="$SUB_BATCH_ID"
fi

# Use config gate policy for run_stage's stage 07 handling
RUN_VERIFICATION_GATE_POLICY="$CFG_STAGE07_GATE_POLICY"

# Execute based on mode
case "$MODE" in
  status)
    if is_sub_batch "$SUB_BATCH_ID"; then
      show_sub_batch_status "$SUB_BATCH_ID"
    else
      show_batch_status "$BATCH_ID"
    fi
    ;;
  run_stage)
    if ! is_sub_batch "$SUB_BATCH_ID"; then
      error "--stage requires a sub-batch ID (e.g., P001.1)"
    fi
    run_stage "$SUB_BATCH_ID" "$STAGE"
    ;;
  run_pipeline)
    if ! is_sub_batch "$SUB_BATCH_ID"; then
      error "--run requires a sub-batch ID (e.g., P001.1)"
    fi
    run_pipeline "$SUB_BATCH_ID" "${START_STAGE:-06}"
    ;;
  run_all)
    if is_sub_batch "$SUB_BATCH_ID"; then
      error "--run-all requires a batch ID (e.g., P001), not a sub-batch"
    fi
    run_all_sub_batches "$BATCH_ID" "$RUN_ALL_COUNT"
    ;;
  view)
    if ! is_sub_batch "$SUB_BATCH_ID"; then
      error "--view requires a sub-batch ID (e.g., P001.1)"
    fi
    create_full_view "$SUB_BATCH_ID"
    ;;
  approve)
    if ! is_sub_batch "$SUB_BATCH_ID"; then
      error "--approve requires a sub-batch ID (e.g., P001.1)"
    fi
    approve_sub_batch "$SUB_BATCH_ID"
    ;;
  validate)
    if ! is_sub_batch "$SUB_BATCH_ID"; then
      error "--validate requires a sub-batch ID (e.g., P001.1)"
    fi
    validate_sub_batch "$SUB_BATCH_ID"
    ;;
  *)
    # Default: show status
    if is_sub_batch "$SUB_BATCH_ID"; then
      show_sub_batch_status "$SUB_BATCH_ID"
    else
      show_batch_status "$BATCH_ID"
    fi
    ;;
esac
