#!/usr/bin/env python3
"""
scripts/training-data/06.video-type

Video Type Classification + Speaker Labeling + Conversation Boundaries

Reads ALL segments and performs analysis in 1 LLM call:
  Classify video type, label speakers, and detect conversation boundaries in a single pass.

Reads:
  - Audio feature files (from Stage 05):
      data/05.audio-features/<source>/<video>/*.audio_features.json

Writes:
  - Combined analysis files:
      data/06.video-type/<source>/<video>/*.conversations.json

Use:

  A) From pipeline:
     ./scripts/training-data/06.video-type "source_name" "youtube_url"

  B) Test videos:
     ./scripts/training-data/06.video-type --test

  C) Single file:
     ./scripts/training-data/06.video-type --input data/test/05.audio-features/video.audio_features.json

  D) Batch from sources file:
     ./scripts/training-data/06.video-type --sources

Requirements:
  - Claude Code CLI installed and authenticated (claude command available)
"""

from __future__ import annotations

import argparse
import hashlib
import json
import re
import shlex
import subprocess
import sys
import time
from dataclasses import dataclass, asdict, field
from pathlib import Path
from typing import Any, Dict, List, Optional, Set, Tuple

import jsonschema

from batch.manifest_parser import load_manifest, load_manifest_sources, manifest_filter_files


# ---------------------------
# Configuration
# ---------------------------

SCHEMA_VERSION = "3.4.0"
PIPELINE_VERSION = "06.video-type-v3.5.1"  # v3.5.1: add deterministic fragment/opener validation checks
PROMPT_VERSION = "3.5.0"

DEBUG_MODE = False

CLAUDE_BINARY_PATHS = [
    "claude",
    Path.home() / ".vscode-server/extensions/anthropic.claude-code-2.1.17-linux-x64/resources/native-binary/claude",
    Path.home() / ".vscode/extensions/anthropic.claude-code-2.1.17-linux-x64/resources/native-binary/claude",
    "/usr/local/bin/claude",
]

LOG_PREFIX = "[06.video-type]"

SCHEMA_PATH = Path(__file__).parent / "schemas" / "conversations.schema.json"

# Failure budget: halt batch if too many failures
MAX_CONSECUTIVE_FAILURES = 3
MAX_FAILURE_RATE = 0.20  # halt if >20% of videos fail validation


# ---------------------------
# Validation
# ---------------------------

@dataclass
class ValidationResult:
    severity: str  # "error", "warning", "info"
    check: str
    message: str

    def to_dict(self) -> Dict[str, str]:
        return {"severity": self.severity, "check": self.check, "message": self.message}


def load_schema() -> Optional[Dict]:
    """Load JSON schema for output validation."""
    if SCHEMA_PATH.exists():
        return json.loads(SCHEMA_PATH.read_text())
    print(f"{LOG_PREFIX} WARNING: Schema not found at {SCHEMA_PATH}")
    return None


_CACHED_SCHEMA: Optional[Dict] = None


def get_schema() -> Optional[Dict]:
    global _CACHED_SCHEMA
    if _CACHED_SCHEMA is None:
        _CACHED_SCHEMA = load_schema()
    return _CACHED_SCHEMA


def validate_output(output: Dict, input_segments: List[Dict]) -> List[ValidationResult]:
    """Comprehensive validation of Stage 06 output.

    Returns a list of ValidationResult with severity levels:
    - error: halt, do not write output
    - warning: flag for review but still write
    - info: logged only
    """
    results: List[ValidationResult] = []

    # --- 1. JSON Schema validation ---
    schema = get_schema()
    if schema:
        try:
            jsonschema.validate(instance=output, schema=schema)
            results.append(ValidationResult("info", "schema_valid", "Output matches JSON schema"))
        except jsonschema.ValidationError as e:
            path = ".".join(str(p) for p in e.absolute_path) if e.absolute_path else "(root)"
            results.append(ValidationResult(
                "error", "schema_invalid",
                f"Schema validation failed at {path}: {e.message[:200]}"
            ))

    # --- 2. Segment count match ---
    output_segments = output.get("segments", [])
    if len(output_segments) != len(input_segments):
        results.append(ValidationResult(
            "warning", "segment_count_mismatch",
            f"Output has {len(output_segments)} segments, input had {len(input_segments)}"
        ))

    # --- 3. Conversation contiguity ---
    conv_segment_ids: Dict[int, List[int]] = {}
    for seg in output_segments:
        conv_id = seg.get("conversation_id", 0)
        if conv_id > 0:
            if conv_id not in conv_segment_ids:
                conv_segment_ids[conv_id] = []
            conv_segment_ids[conv_id].append(seg.get("id", -1))

    for conv_id, seg_ids in conv_segment_ids.items():
        sorted_ids = sorted(seg_ids)
        if sorted_ids != list(range(sorted_ids[0], sorted_ids[0] + len(sorted_ids))):
            results.append(ValidationResult(
                "warning", "conversation_not_contiguous",
                f"Conversation {conv_id} has non-contiguous segment IDs: {sorted_ids}"
            ))

    # --- 4. Video type ↔ segment type consistency ---
    video_type = output.get("video_type", {}).get("type", "")
    if video_type in ("talking_head", "podcast"):
        non_commentary = [
            s for s in output_segments
            if s.get("segment_type") != "commentary" or s.get("conversation_id", 0) > 0
        ]
        if non_commentary:
            results.append(ValidationResult(
                "error", "video_type_segment_mismatch",
                f"{video_type} video has {len(non_commentary)} non-commentary segments "
                f"(should all be commentary with conversation_id=0)"
            ))

    # --- 5. Speaker role sanity ---
    speaker_labels = output.get("speaker_labels", {})
    roles = [v.get("role", "unknown") for v in speaker_labels.values()]
    collapse_meta = output.get("speaker_collapse")

    if collapse_meta and collapse_meta.get("detected"):
        # When collapse is detected, check resolved segment roles for coach/target presence
        segment_roles = set(s.get("speaker_role", "unknown") for s in output_segments)

        if not (("coach" in roles or "student" in roles) or ("coach" in segment_roles or "student" in segment_roles)):
            results.append(ValidationResult(
                "warning", "no_coach_speaker",
                f"No speaker labeled as coach/student (even after collapse resolution). "
                f"Global roles: {roles}, Segment roles: {sorted(segment_roles)}"
            ))

        if video_type in ("infield", "compilation") and "target" not in roles and "target" not in segment_roles:
            results.append(ValidationResult(
                "warning", "no_target_in_infield",
                f"No target speaker in {video_type} video (even after collapse resolution). "
                f"Global roles: {roles}, Segment roles: {sorted(segment_roles)}"
            ))

        # Report collapse reassignment quality
        rate = collapse_meta.get("reassignment_rate", 0)
        unknown_ct = collapse_meta.get("unknown_count", 0)
        total = collapse_meta.get("total_segments_affected", 0)

        if rate < 0.5:
            results.append(ValidationResult(
                "warning", "low_reassignment_rate",
                f"Speaker collapse: only {rate:.0%} of {total} affected segments were reassigned. "
                f"{unknown_ct} remain unknown."
            ))
        else:
            results.append(ValidationResult(
                "info", "speaker_collapse_resolved",
                f"Speaker collapse: {collapse_meta.get('reassigned_count', 0)}/{total} segments "
                f"reassigned ({rate:.0%}), {unknown_ct} remain unknown."
            ))
    else:
        # Standard checks (no collapse)
        if "coach" not in roles and "student" not in roles:
            results.append(ValidationResult(
                "warning", "no_coach_speaker",
                f"No speaker labeled as coach/student. Roles found: {roles}"
            ))

        if video_type in ("infield", "compilation") and "target" not in roles:
            results.append(ValidationResult(
                "warning", "no_target_in_infield",
                f"No target speaker in {video_type} video. Roles found: {roles}"
            ))

    # Unknown speaker ratio (exclude collapsed from this check)
    non_collapsed_roles = [r for r in roles if r != "collapsed"]
    unknown_count = sum(1 for r in non_collapsed_roles if r == "unknown")
    if non_collapsed_roles and unknown_count / len(non_collapsed_roles) > 0.5:
        results.append(ValidationResult(
            "warning", "high_unknown_speaker_ratio",
            f"{unknown_count}/{len(non_collapsed_roles)} non-collapsed speakers are unknown"
        ))

    # --- 6. Conversations-per-minute ratio ---
    conversations = output.get("conversations", [])
    if output_segments:
        first_start = min(s.get("start", 0) for s in output_segments)
        last_end = max(s.get("end", 0) for s in output_segments)
        duration_min = (last_end - first_start) / 60.0
        if duration_min > 0.5:  # only check if video is > 30 seconds
            conv_per_min = len(conversations) / duration_min
            if video_type in ("infield", "compilation"):
                if conversations and conv_per_min > 3.0:
                    results.append(ValidationResult(
                        "warning", "high_conversation_rate",
                        f"{len(conversations)} conversations in {duration_min:.1f}min "
                        f"({conv_per_min:.1f}/min) — possible over-segmentation"
                    ))
                elif len(conversations) == 0:
                    results.append(ValidationResult(
                        "warning", "zero_conversations_in_infield",
                        f"0 conversations in {video_type} video ({duration_min:.1f}min)"
                    ))

    # --- 7. Conversation ID sequentiality ---
    conv_ids = sorted(conv_segment_ids.keys())
    if conv_ids and conv_ids != list(range(1, len(conv_ids) + 1)):
        results.append(ValidationResult(
            "warning", "conversation_ids_not_sequential",
            f"Conversation IDs are not sequential 1..N: {conv_ids}"
        ))

    # --- 8. Fragment conversation sanity ---
    # Catch common false splits: very short coach-only/student-only fragments.
    if video_type in ("infield", "compilation"):
        for conv_id, seg_ids in conv_segment_ids.items():
            conv_segs = [s for s in output_segments if s.get("conversation_id") == conv_id]
            if not conv_segs:
                continue
            c_start = min(float(s.get("start", 0.0)) for s in conv_segs)
            c_end = max(float(s.get("end", 0.0)) for s in conv_segs)
            c_dur = max(0.0, c_end - c_start)
            c_roles = {str(s.get("speaker_role", "unknown")) for s in conv_segs}
            has_target = "target" in c_roles
            has_approacher = ("coach" in c_roles) or ("student" in c_roles)

            if c_dur < 10.0 and (not has_target or not has_approacher):
                results.append(ValidationResult(
                    "warning", "fragment_conversation_suspect",
                    f"Conversation {conv_id} is short ({c_dur:.1f}s) with roles={sorted(c_roles)}; "
                    f"likely commentary/fragment split error"
                ))

    # --- 9. Opener attribution sanity ---
    # If opener-like text is tagged as target, role assignment is likely inverted.
    opener_re = re.compile(
        r"\\b(excuse me|hey\\b|hi\\b|hello\\b|quick question|i just saw you|you looked really)\\b",
        re.IGNORECASE,
    )
    for seg in output_segments:
        if seg.get("conversation_id", 0) <= 0:
            continue
        if seg.get("segment_type") != "approach":
            continue
        text = str(seg.get("text", "") or "")
        if not text:
            continue
        if opener_re.search(text) and seg.get("speaker_role") == "target":
            sid = seg.get("id", "?")
            cid = seg.get("conversation_id", "?")
            snippet = text.strip().replace("\n", " ")[:80]
            results.append(ValidationResult(
                "warning", "opener_labeled_as_target",
                f"Segment {sid} in conversation {cid} looks like an opener but speaker_role=target: '{snippet}'"
            ))

    # --- 10. Video type sanity vs conversation count ---
    if video_type == "compilation" and len(conversations) <= 1:
        results.append(ValidationResult(
            "warning", "compilation_with_single_conversation",
            f"Video labeled compilation but only {len(conversations)} conversation(s); possible infield"
        ))
    if video_type == "infield" and len(conversations) >= 2:
        results.append(ValidationResult(
            "warning", "infield_with_multiple_conversations",
            f"Video labeled infield with {len(conversations)} conversations; verify not a compilation"
        ))

    return results


def write_validation_results(
    results: List[ValidationResult], output_path: Path, video_id: str
) -> None:
    """Write validation results to a .validation.json file alongside the output."""
    validation_output = {
        "video_id": video_id,
        "validated_at": time.strftime("%Y-%m-%dT%H:%M:%SZ"),
        "pipeline_version": PIPELINE_VERSION,
        "prompt_version": PROMPT_VERSION,
        "summary": {
            "errors": sum(1 for r in results if r.severity == "error"),
            "warnings": sum(1 for r in results if r.severity == "warning"),
            "info": sum(1 for r in results if r.severity == "info"),
            "passed": all(r.severity != "error" for r in results),
        },
        "results": [r.to_dict() for r in results],
    }

    validation_path = output_path.with_suffix(".validation.json")
    validation_path.parent.mkdir(parents=True, exist_ok=True)
    with validation_path.open("w", encoding="utf-8") as f:
        json.dump(validation_output, f, indent=2, ensure_ascii=False)

    # Log summary
    errors = validation_output["summary"]["errors"]
    warnings = validation_output["summary"]["warnings"]
    if errors > 0:
        print(f"{LOG_PREFIX} VALIDATION FAILED: {errors} error(s), {warnings} warning(s)")
        for r in results:
            if r.severity == "error":
                print(f"{LOG_PREFIX}   ERROR: [{r.check}] {r.message}")
    elif warnings > 0:
        print(f"{LOG_PREFIX} VALIDATION PASSED with {warnings} warning(s)")
        for r in results:
            if r.severity == "warning":
                print(f"{LOG_PREFIX}   WARN: [{r.check}] {r.message}")
    else:
        print(f"{LOG_PREFIX} VALIDATION PASSED (clean)")


# ---------------------------
# Prompts
# ---------------------------

ANALYZE_VIDEO_PROMPT = """You are analyzing a daygame coaching video. Your FIVE tasks in a single pass:
1. Classify the video type
2. Assign a role to each speaker
3. Detect conversation boundaries
4. Assess transcript quality (how reliable/accurate is the transcription)
5. Identify teaser/preview segments (content shown in intro that repeats later)

VIDEO TYPES:

1. "infield" - Coach DOING live approaches on the street
   - Live interaction with women (not hypothetical)
   - Real-time responses from targets
   - Street/shopping/park environment implied
   - Nervous energy, real rejection/acceptance
   - Incomplete thoughts, interruptions, ambient noise

2. "talking_head" - Coach EXPLAINING concepts to camera
   - Educational/instructional tone
   - No second party responding
   - Abstract examples, theory discussion
   - "Guys", "everyone", "you should"
   - Clean, complete sentences

3. "podcast" - Multiple speakers DISCUSSING topics
   - Back-and-forth dialogue about theory
   - Named co-hosts or guests
   - Interview-style questions
   - Both speakers have long turns

4. "compilation" - Mixed content types
   - Shifts between infield and commentary
   - Multiple approaches with breakdowns between
   - "As you saw..." followed by explanation

VIDEO TYPE DECISION RULES (apply in order):
1. Count DISTINCT approaches to different targets:
   - 0 approaches => talking_head OR podcast
   - 1 approach => infield (even with intro/outro commentary)
   - 2+ approaches => compilation
2. For 0-approach videos, use speaker dominance:
   - One speaker >80% of segments => talking_head
   - Balanced multi-speaker discussion => podcast
3. Intro/outro commentary alone does NOT make a video "compilation".
   - One long approach + short setup/debrief should still be "infield".

CANONICAL EXAMPLES:
- "7 Minute Pull" (one extended interaction + intro/outro) => infield
- "10 approaches in one video" => compilation
- "single-speaker theory breakdown" => talking_head
- "balanced host/guest discussion" => podcast

SPEAKER ROLES:

1. "coach" - The person teaching/demonstrating
   - Opens conversations (excuse me, quick question...)
   - Asks personal questions (name, origin, occupation)
   - Gives compliments
   - Longer, confident statements
   - Commentary to camera between approaches
   - May reference teaching ("guys", "as you can see")

2. "student" - A coached student/participant (immersion program)
   - May deliver openers and run an approach, but is not the instructor/coach
   - Often receives feedback, practices drills, or gives student-style field reports
   - If unsure, label as "other" (do NOT overuse "student")

3. "target" - Women being approached
   - Responds to questions (short answers initially)
   - Asked about herself (not asking)
   - Gives name when asked
   - May laugh, show surprise, hesitation
   - Typically shorter turns than coach

4. "voiceover" - Post-production narration
   - Instructional tone disconnected from live action
   - No back-and-forth dialogue
   - Perfect sentences (not fragmented)
   - "Notice how he..." or "Watch what happens..."

5. "other" - Background voices, friends, staff
   - Brief interjections
   - Not involved in approach

6. "unknown" - Cannot determine with confidence
   - Mark as unknown if unsure
   - Flag for manual review

7. "collapsed" - Pyannote merged multiple real speakers into this ID
   - Shows BOTH coach AND target speech patterns
   - Per-segment roles provided via speaker_role_override in the segments array

SEGMENT TYPES (for conversation boundaries):

1. "approach" - Part of live interaction with a woman
   - Includes: opener, small talk, flirting, number close
   - Gets non-zero conversation_id

2. "commentary" - Coach talking to camera (not to woman)
   - Pre-approach setup, post-approach breakdown
   - conversation_id: 0

3. "transition" - Brief moment between content
   - Walking, repositioning, audio gaps
   - conversation_id: 0

CONVERSATION BOUNDARY RULES:
- NEW conversation starts when: direct address to new person, change from commentary to dialogue, location shift, previous approach ended
- SAME conversation continues when: same thread, questions followed by answers, no camera break
- APPROACH ends when: number exchange, goodbye, rejection, coach pivots to camera
- Use time gaps between segments to help detect boundaries. Gaps >30s between segments strongly suggest a new conversation.
- When uncertain: default to commentary (safer)
- conversation_id must be sequential (1, 2, 3...)
- Segments with same conversation_id must be contiguous
- A valid conversation must usually be >=10 seconds and include BOTH approacher (coach/student) and target turns.
- Coach/student-only fragments or ultra-short snippets (<10s, typically 2-3 segments) should default to commentary (conversation_id: 0), unless there is clear multi-turn target participation.

CRITICAL RULES:
1. The person who delivers approach OPENERS ("excuse me", "I just saw you", "you looked really cute") is ALWAYS the APPROACHER (coach OR student), NEVER the target
   - If a speaker opens with "Hey, excuse me, hi", do NOT label that speaker as target.
2. The person giving SHORT RESPONSES to questions is typically the target
3. If unsure about a speaker, mark as "unknown" with low confidence - DO NOT GUESS
4. Consider speech patterns across ALL segments, not just one or two
5. In coaching videos with participants:
   - Student/participant running the approach = "student" (never target)
   - Main instructor analysis = "coach" (or "voiceover" if post-production narration)
   - Do not mislabel students as targets simply because they are not the main instructor
6. SPEAKER COLLAPSE DETECTION: If one speaker ID shows BOTH approacher patterns (coach/student: opens conversations, leads) AND target patterns (short responses), this indicates pyannote merged multiple speakers into one ID.
   When you detect this:
   a) In speaker_labels: Set that speaker's role to "collapsed" with confidence 0.3 and reasoning starting with "mixed_diarization_error: " followed by a brief description
   b) In segments: For EACH segment from the collapsed speaker, add a "speaker_role_override" field:
      - "coach" if the segment delivers an opener, compliment, cold read, or camera commentary
      - "student" if the segment is a coached participant running an approach/drill (not the instructor)
      - "target" if the segment is a short response to a question, gives personal info when asked, or reacts to the coach
      - "unknown" if you genuinely cannot determine from context
   c) Only add speaker_role_override to segments from collapsed speakers. Omit it for all other segments.
7. If video_type is "talking_head" or "podcast": ALL segments must be "commentary" with conversation_id 0
8. If video_type is "infield" or "compilation": classify each segment as approach/commentary/transition with sequential conversation_ids

TRANSCRIPT QUALITY ASSESSMENT:

Evaluate the overall quality and reliability of the transcript. Consider:

1. COHERENCE: Does the text flow logically? Can you follow conversations?
2. SPEAKER CLARITY: Can you distinguish who is speaking? Are speaker changes clear?
3. ASR ARTIFACTS: Are there obvious transcription errors? (repeated words, nonsense, garbled text)
4. COMPLETENESS: Does the transcript seem complete or are there gaps/missing dialogue?
5. LANGUAGE ISSUES: Wrong language detected? Mixed languages causing confusion?

Output a confidence score from 1-100:
- 90-100: Excellent - clear, coherent, easy to follow, minimal errors
- 70-89: Good - mostly clear, some minor issues but usable
- 50-69: Fair - noticeable issues, some segments unclear but core content understandable
- 30-49: Poor - significant issues, hard to follow, many errors
- 1-29: Unusable - severely garbled, mostly nonsense, cannot reliably extract meaning

TEASER/PREVIEW DETECTION:

Many YouTube videos show a "teaser" or "highlight reel" in the first 30-60 seconds, then replay that same content in full later. This creates duplicate content we want to skip during embedding.

TEASER PATTERNS:
1. Short clip at video start (0-60s) showing an exciting moment
2. Coach voiceover on top: "Watch what happens..." or "Coming up..."
3. Same conversation appears in full later in the video
4. Often has different audio (music, narration) than the full version

HOW TO IDENTIFY:
- Compare early segments (first 60s) to later content
- Look for matching dialogue/content that appears twice
- Teaser is usually shorter/edited version of the full conversation
- Coach may talk OVER the teaser, making it harder to follow

MARKING TEASERS:
- If a segment is a teaser preview of content that appears later, add:
  "is_teaser": true
  "teaser_of_conversation_id": N (the conversation_id of the full version)
- If you cannot match the teaser to a specific conversation, use:
  "is_teaser": true
  "teaser_of_conversation_id": null
- Only mark the TEASER segments, not the full conversation later
- Teaser segments should still have conversation_id: 0 (they're preview content)

VIDEO TITLE: "{title}"

SPEAKERS FOUND: {speakers}

SPEAKER STATS:
{speaker_stats}

FULL TRANSCRIPT ({segment_count} segments):
{transcript}

OUTPUT: First, briefly reason about the video type, speaker assignments, transcript quality, and any teasers (3-5 sentences). Then output the JSON object in a ```json code block with this exact structure:
{{"video_type": {{"type": "infield|talking_head|podcast|compilation", "confidence": 0.0-1.0, "reasoning": "brief explanation"}}, "transcript_confidence": {{"score": 1-100, "reasoning": "brief explanation of transcript quality issues or strengths"}}, "speaker_labels": {{"SPEAKER_XX": {{"role": "coach|student|target|voiceover|other|unknown|collapsed", "confidence": 0.0-1.0, "reasoning": "brief explanation"}}}}, "segments": [{{"id": 0, "segment_type": "approach|commentary|transition", "conversation_id": 0, "is_conversation_start": false, "speaker_role_override": "coach|student|target|unknown (ONLY for collapsed speakers, omit otherwise)", "is_teaser": true (ONLY if this segment is a teaser/preview, omit otherwise), "teaser_of_conversation_id": N or null (ONLY if is_teaser is true)}}, ...]}}
"""


# ---------------------------
# State Management
# ---------------------------

@dataclass
class ProcessingState:
    version: int
    completed_files: List[str]
    in_progress: Optional[str]
    failures: List[Dict[str, str]]


def load_state(state_path: Path) -> ProcessingState:
    if state_path.exists():
        try:
            data = json.loads(state_path.read_text())
            return ProcessingState(
                version=data.get("version", 1),
                completed_files=data.get("completed_files", []),
                in_progress=data.get("in_progress"),
                failures=data.get("failures", []),
            )
        except (json.JSONDecodeError, KeyError):
            pass
    return ProcessingState(version=1, completed_files=[], in_progress=None, failures=[])


def save_state(state_path: Path, state: ProcessingState) -> None:
    state_path.parent.mkdir(parents=True, exist_ok=True)
    state_path.write_text(json.dumps(asdict(state), indent=2))


# ---------------------------
# Claude CLI Interface
# ---------------------------

def find_claude_binary() -> Optional[str]:
    for path in CLAUDE_BINARY_PATHS:
        path = Path(path)
        if path.exists() and path.is_file():
            return str(path)
        if str(path) == "claude":
            try:
                result = subprocess.run(["which", "claude"], capture_output=True, text=True)
                if result.returncode == 0:
                    return "claude"
            except Exception:
                pass
    return None


def call_claude(
    prompt: str,
    retries: int = 3,
    timeout: int = 300,
    model: Optional[str] = None,
) -> Optional[str]:
    claude_bin = find_claude_binary()
    if not claude_bin:
        raise RuntimeError("Claude CLI binary not found - cannot proceed")

    cmd: List[str] = [claude_bin, "-p", prompt, "--output-format", "text"]
    if isinstance(model, str) and model.strip():
        cmd += ["--model", model.strip()]

    for attempt in range(retries):
        try:
            result = subprocess.run(
                cmd,
                capture_output=True,
                text=True,
                timeout=timeout,
            )
            if result.returncode == 0:
                return result.stdout.strip()
            else:
                if attempt < retries - 1:
                    wait = 2 ** attempt
                    print(f"{LOG_PREFIX} Claude CLI error, retrying in {wait}s...")
                    print(f"{LOG_PREFIX}   stderr: {result.stderr[:200]}")
                    time.sleep(wait)
                    continue
                raise RuntimeError(f"Claude CLI failed: {result.stderr[:500]}")
        except subprocess.TimeoutExpired:
            if attempt < retries - 1:
                print(f"{LOG_PREFIX} Timeout, retrying...")
                time.sleep(2 ** attempt)
                continue
            raise RuntimeError(f"Claude CLI timeout after {timeout}s")
        except FileNotFoundError:
            raise RuntimeError("'claude' command not found. Install Claude Code CLI.")
    return None


def parse_json_response(response: str) -> Optional[Dict]:
    if not response:
        if DEBUG_MODE:
            print(f"{LOG_PREFIX} DEBUG: Response is empty/None")
        return None

    if DEBUG_MODE:
        print(f"{LOG_PREFIX} DEBUG: Response length: {len(response)} chars")
        print(f"{LOG_PREFIX} DEBUG: Response preview:\n{response[:1000]}")
        print(f"{LOG_PREFIX} DEBUG: Response end:\n...{response[-500:]}")

    try:
        code_block_match = re.search(r"```(?:json)?\s*(\{[\s\S]*?\})\s*```", response)
        if code_block_match:
            if DEBUG_MODE:
                print(f"{LOG_PREFIX} DEBUG: Found JSON in code block")
            return json.loads(code_block_match.group(1))

        start = response.find("{")
        if start != -1:
            bracket_count = 0
            for i, char in enumerate(response[start:], start):
                if char == "{":
                    bracket_count += 1
                elif char == "}":
                    bracket_count -= 1
                    if bracket_count == 0:
                        json_str = response[start:i + 1]
                        if DEBUG_MODE:
                            print(f"{LOG_PREFIX} DEBUG: Extracted JSON ({len(json_str)} chars)")
                        return json.loads(json_str)
            if DEBUG_MODE:
                print(f"{LOG_PREFIX} DEBUG: Unbalanced brackets, final count: {bracket_count}")
    except (json.JSONDecodeError, ValueError) as e:
        print(f"{LOG_PREFIX} JSON parse error: {e}")
        print(f"{LOG_PREFIX} Response preview: {response[:500]}...")

    # Save failed response for debugging under data/ (avoid polluting scripts/ worktree).
    try:
        ts = time.strftime("%Y%m%dT%H%M%SZ")
        debug_dir = repo_root() / "data" / "06.video-type" / "debug"
        debug_dir.mkdir(parents=True, exist_ok=True)
        debug_path = debug_dir / f"claude_parse_failed_{ts}_{int(time.time() * 1000)}.txt"
        debug_path.write_text(response, encoding="utf-8")
        print(f"{LOG_PREFIX} DEBUG: Saved full response to {debug_path}")
    except Exception as e:
        print(f"{LOG_PREFIX} DEBUG: Failed to save debug response: {e}")

    return None


# ---------------------------
# Single-Pass Analysis: Video Type + Speakers + Boundaries
# ---------------------------

def analyze_video(
    title: str,
    segments: List[Dict],
    claude_model: Optional[str] = None,
) -> Tuple[Dict[str, Any], Dict[str, Any], Dict[str, Dict[str, Any]], List[Dict[str, Any]], List[str]]:
    """Classify video type, label speakers, assess transcript quality, and detect conversation boundaries in a single LLM call.

    Returns (video_type_dict, transcript_confidence_dict, speaker_labels_dict, segment_classifications, extra_flags).
    """
    speakers = sorted(set(seg.get("pyannote_speaker", "UNKNOWN") for seg in segments))

    # Compute per-speaker stats
    speaker_seg_counts: Dict[str, int] = {}
    speaker_word_counts: Dict[str, int] = {}
    speaker_duration: Dict[str, float] = {}
    for seg in segments:
        spk = seg.get("pyannote_speaker", "UNKNOWN")
        text = seg.get("text", "").strip()
        dur = seg.get("end", 0) - seg.get("start", 0)
        speaker_seg_counts[spk] = speaker_seg_counts.get(spk, 0) + 1
        speaker_word_counts[spk] = speaker_word_counts.get(spk, 0) + len(text.split())
        speaker_duration[spk] = speaker_duration.get(spk, 0) + dur

    stats_lines = []
    for spk in sorted(speakers, key=lambda s: speaker_seg_counts.get(s, 0), reverse=True):
        segs = speaker_seg_counts.get(spk, 0)
        words = speaker_word_counts.get(spk, 0)
        dur = speaker_duration.get(spk, 0)
        stats_lines.append(f"{spk}: {segs} segments, ~{words} words, {dur:.1f}s speaking time")
    speaker_stats_text = "\n".join(stats_lines)

    # Build chronological transcript with speaker IDs
    transcript_lines = []
    for i, seg in enumerate(segments):
        speaker = seg.get("pyannote_speaker", "UNKNOWN")
        text = seg.get("text", "").strip()
        start = seg.get("start", 0)
        end = seg.get("end", 0)
        if text:
            transcript_lines.append(f"[{i}] {speaker} ({start:.1f}-{end:.1f}s): \"{text}\"")

    transcript_text = "\n".join(transcript_lines)

    prompt = ANALYZE_VIDEO_PROMPT.format(
        title=title,
        speakers=", ".join(speakers),
        speaker_stats=speaker_stats_text,
        segment_count=len(segments),
        transcript=transcript_text,
    )

    print(f"{LOG_PREFIX} Analyzing: type + {len(speakers)} speakers + boundaries ({len(segments)} segments)...")

    max_retries = 3
    for attempt in range(max_retries):
        response = call_claude(prompt, timeout=300, model=claude_model)
        result = parse_json_response(response)

        if result and "video_type" in result and "speaker_labels" in result and "segments" in result:
            vtype = result["video_type"]
            labels = result["speaker_labels"]
            classifications = result["segments"]

            # Extract transcript_confidence (with fallback for older responses)
            transcript_conf = result.get("transcript_confidence", {
                "score": 70,
                "reasoning": "Not assessed (legacy response)"
            })

            # Ensure all speakers have labels
            for speaker in speakers:
                if speaker not in labels:
                    labels[speaker] = {"role": "unknown", "confidence": 0.3, "reasoning": "Not labeled by LLM"}

            # Fix segment count mismatches
            extra_flags = []
            if len(classifications) != len(segments):
                print(f"{LOG_PREFIX}   WARNING: Got {len(classifications)} segment classifications for {len(segments)} segments")
                extra_flags.append(f"segment_count_mismatch_{len(classifications)}_vs_{len(segments)}")
                if len(classifications) < len(segments):
                    padded = len(segments) - len(classifications)
                    extra_flags.append(f"padded_{padded}_segments_as_commentary")
                while len(classifications) < len(segments):
                    classifications.append({
                        "id": len(classifications),
                        "segment_type": "commentary",
                        "conversation_id": 0,
                        "is_conversation_start": False,
                    })
                classifications = classifications[:len(segments)]

            # Log results
            conf = vtype.get("confidence", 0)
            print(f"{LOG_PREFIX}   Video type: {vtype.get('type')} ({conf * 100:.0f}%)")
            tconf = transcript_conf.get("score", 0)
            print(f"{LOG_PREFIX}   Transcript quality: {tconf}/100 - {transcript_conf.get('reasoning', '')[:60]}")
            for speaker, label in labels.items():
                lconf = label.get("confidence", 0) * 100
                print(f"{LOG_PREFIX}   {speaker}: {label.get('role')} ({lconf:.0f}%) - {label.get('reasoning', '')[:50]}")
            conv_ids = set(c.get("conversation_id", 0) for c in classifications if c.get("conversation_id", 0) > 0)
            print(f"{LOG_PREFIX}   Conversations: {len(conv_ids)}")

            return vtype, transcript_conf, labels, classifications, extra_flags

        if attempt < max_retries - 1:
            print(f"{LOG_PREFIX}   WARNING: JSON parsing failed, retrying ({attempt + 2}/{max_retries})...")
            time.sleep(2)

    raise RuntimeError(f"Video analysis failed after {max_retries} retries")


# ---------------------------
# Speaker Collapse Resolution
# ---------------------------

def resolve_speaker_roles(
    segments: List[Dict],
    speaker_labels: Dict[str, Dict],
    classifications: List[Dict],
) -> Tuple[List[str], Optional[Dict[str, Any]]]:
    """Resolve per-segment speaker roles, handling collapsed speakers.

    For non-collapsed speakers, uses the global speaker_labels mapping.
    For collapsed speakers, uses per-segment speaker_role_override from LLM output.

    Returns:
        - List of speaker_role strings (one per segment, in order)
        - speaker_collapse metadata dict (or None if no collapse detected)
    """
    collapsed_ids = {
        spk_id for spk_id, label in speaker_labels.items()
        if label.get("role") == "collapsed"
    }

    if not collapsed_ids:
        roles = []
        for seg in segments:
            speaker_id = seg.get("pyannote_speaker", "UNKNOWN")
            roles.append(speaker_labels.get(speaker_id, {}).get("role", "unknown"))
        return roles, None

    roles = []
    total_affected = 0
    reassigned = 0
    unknown = 0

    for i, seg in enumerate(segments):
        speaker_id = seg.get("pyannote_speaker", "UNKNOWN")

        if speaker_id in collapsed_ids:
            total_affected += 1
            classification = classifications[i] if i < len(classifications) else {}
            override = classification.get("speaker_role_override", "unknown")

            if override in ("coach", "student", "target", "voiceover", "other"):
                reassigned += 1
            else:
                override = "unknown"
                unknown += 1

            roles.append(override)
        else:
            roles.append(speaker_labels.get(speaker_id, {}).get("role", "unknown"))

    collapse_meta = {
        "detected": True,
        "collapsed_speakers": sorted(collapsed_ids),
        "total_segments_affected": total_affected,
        "reassigned_count": reassigned,
        "unknown_count": unknown,
        "reassignment_rate": round(reassigned / max(total_affected, 1), 3),
    }

    return roles, collapse_meta


# ---------------------------
# Main Processing
# ---------------------------

def extract_video_title(filename: str) -> str:
    name = Path(filename).stem
    name = re.sub(r"\.(audio_features|video_type|conversations)$", "", name)
    match = re.match(r"^(.+?)\s*\[", name)
    return match.group(1).strip() if match else name


def extract_video_id(filename: str) -> str:
    match = re.search(r"\[([^\]]+)\]", filename)
    return match.group(1) if match else Path(filename).stem


def compute_checksum(data: Any) -> str:
    return hashlib.sha256(json.dumps(data, sort_keys=True).encode()).hexdigest()[:16]


def process_file(
    input_path: Path,
    output_path: Path,
    dry_run: bool = False,
    claude_model: Optional[str] = None,
) -> Dict[str, Any]:
    """Process a single audio_features.json file through single-pass LLM analysis."""

    print(f"\n{LOG_PREFIX} Processing: {input_path.name}")

    with input_path.open("r", encoding="utf-8") as f:
        data = json.load(f)

    segments = data.get("segments", [])
    video_title = extract_video_title(str(input_path))
    video_id = extract_video_id(str(input_path))

    print(f"{LOG_PREFIX}   Video: \"{video_title}\" [{video_id}]")
    print(f"{LOG_PREFIX}   Segments: {len(segments)}")

    if not segments:
        raise RuntimeError(f"No segments found in {input_path}")

    if dry_run:
        print(f"{LOG_PREFIX}   [DRY RUN] Would process this file")
        return {"video_type": None, "conversations": 0, "flags": []}

    start_time = time.time()

    # Single-pass analysis: video type + transcript quality + speakers + boundaries
    video_type_info, transcript_confidence, speaker_labels, classifications, extra_flags = analyze_video(
        video_title,
        segments,
        claude_model=claude_model,
    )

    video_type = video_type_info.get("type", "compilation")

    # Legacy flags from segment count mismatch
    review_flags = list(extra_flags)

    # Resolve speaker roles (handles collapsed speakers with per-segment overrides)
    resolved_roles, collapse_meta = resolve_speaker_roles(segments, speaker_labels, classifications)

    if collapse_meta:
        for spk_id in collapse_meta["collapsed_speakers"]:
            review_flags.append(
                f"speaker_collapse: {spk_id} "
                f"({collapse_meta['total_segments_affected']} segs, "
                f"{collapse_meta['reassignment_rate']:.0%} reassigned, "
                f"{collapse_meta['unknown_count']} unknown)"
            )
            print(f"{LOG_PREFIX}   SPEAKER COLLAPSE: {spk_id} — "
                  f"{collapse_meta['total_segments_affected']} segments affected")
            print(f"{LOG_PREFIX}   Reassigned: "
                  f"{collapse_meta['reassigned_count']}/{collapse_meta['total_segments_affected']} "
                  f"({collapse_meta['reassignment_rate']:.0%}), "
                  f"Unknown: {collapse_meta['unknown_count']}")

    # Build output segments
    output_segments = []
    for i, seg in enumerate(segments):
        classification = classifications[i] if i < len(classifications) else {
            "segment_type": "commentary",
            "conversation_id": 0,
            "is_conversation_start": False,
        }

        speaker_id = seg.get("pyannote_speaker", "UNKNOWN")
        speaker_role = resolved_roles[i]

        out_seg = {
            "id": i,
            "start": seg.get("start", 0),
            "end": seg.get("end", 0),
            "text": seg.get("text", ""),
            "speaker_id": speaker_id,
            "speaker_role": speaker_role,
            "segment_type": classification.get("segment_type", "commentary"),
            "conversation_id": classification.get("conversation_id", 0),
            "is_conversation_start": classification.get("is_conversation_start", False),
        }

        # Include override field for collapsed speakers (audit trail)
        override = classification.get("speaker_role_override")
        if override:
            out_seg["speaker_role_override"] = override

        # Include teaser fields if present
        is_teaser = classification.get("is_teaser")
        if is_teaser:
            out_seg["is_teaser"] = True
            teaser_of = classification.get("teaser_of_conversation_id")
            out_seg["teaser_of_conversation_id"] = teaser_of

        output_segments.append(out_seg)

    # Build conversation summaries
    conversations = []
    conv_segments: Dict[int, List[Dict]] = {}
    for seg in output_segments:
        conv_id = seg["conversation_id"]
        if conv_id > 0:
            if conv_id not in conv_segments:
                conv_segments[conv_id] = []
            conv_segments[conv_id].append(seg)

    for conv_id, segs in sorted(conv_segments.items()):
        conversations.append({
            "conversation_id": conv_id,
            "segment_ids": [s["id"] for s in segs],
            "start_time": segs[0]["start"],
            "end_time": segs[-1]["end"],
        })

    elapsed = time.time() - start_time

    # Build output
    output = {
        "video_id": video_id,
        "source_file": str(input_path),
        "processed_at": time.strftime("%Y-%m-%dT%H:%M:%SZ"),
        "video_type": {
            "type": video_type,
            "confidence": video_type_info.get("confidence"),
            "reasoning": video_type_info.get("reasoning"),
        },
        "transcript_confidence": {
            "score": transcript_confidence.get("score", 70),
            "reasoning": transcript_confidence.get("reasoning", ""),
        },
        "speaker_labels": speaker_labels,
        "segments": output_segments,
        "conversations": conversations,
        "review_flags": review_flags if review_flags else None,
        "speaker_collapse": collapse_meta,
        "metadata": {
            "pipeline_version": PIPELINE_VERSION,
            "prompt_version": PROMPT_VERSION,
            "schema_version": SCHEMA_VERSION,
            "input_checksum": compute_checksum(data),
            "llm_calls": 1,
            "processing_time_sec": elapsed,
            "model": "claude-cli",
            **({"claude_model": claude_model.strip()} if isinstance(claude_model, str) and claude_model.strip() else {}),
        },
    }

    # Validate output before writing
    validation_results = validate_output(output, segments)

    # Write validation results (always, even if validation fails)
    write_validation_results(validation_results, output_path, video_id)

    has_errors = any(r.severity == "error" for r in validation_results)

    if has_errors:
        # Don't write invalid output to the main output file
        print(f"{LOG_PREFIX} SKIPPING output write due to validation errors")
    else:
        # Write output
        output_path.parent.mkdir(parents=True, exist_ok=True)
        with output_path.open("w", encoding="utf-8") as f:
            json.dump(output, f, indent=2, ensure_ascii=False)

    # Summary
    type_counts: Dict[str, int] = {}
    for seg in output_segments:
        st = seg["segment_type"]
        type_counts[st] = type_counts.get(st, 0) + 1

    print(f"{LOG_PREFIX} Results:")
    print(f"  Video type: {video_type}")
    print(f"  Conversations: {len(conversations)}")
    print(f"  Segment types: {type_counts}")
    print(f"  LLM calls: 1")
    print(f"  Time: {elapsed:.1f}s")
    print(f"  Output: {output_path}")
    print(f"  Validation: {'FAILED' if has_errors else 'PASSED'}")

    return {
        "video_type": video_type,
        "conversations": len(conversations),
        "flags": review_flags,
        "validation_passed": not has_errors,
        "validation_errors": sum(1 for r in validation_results if r.severity == "error"),
        "validation_warnings": sum(1 for r in validation_results if r.severity == "warning"),
    }


# ---------------------------
# Path helpers
# ---------------------------

def repo_root() -> Path:
    return Path(__file__).resolve().parents[2]


def input_root() -> Path:
    return repo_root() / "data" / "05.audio-features"


def output_root() -> Path:
    return repo_root() / "data" / "06.video-type"


def test_input_root() -> Path:
    return repo_root() / "data" / "test" / "05.audio-features"


def test_output_root() -> Path:
    return repo_root() / "data" / "test" / "06.video-type"


def compute_output_path(input_path: Path, output_dir: Path) -> Path:
    stem = input_path.stem
    if stem.endswith(".audio_features"):
        stem = stem[:-len(".audio_features")]
    return output_dir / f"{stem}.conversations.json"


def parse_sources_file(path: Path) -> List[Tuple[str, str]]:
    sources: List[Tuple[str, str]] = []
    for raw in path.read_text(encoding="utf-8").splitlines():
        line = raw.strip()
        if not line or line.startswith("#"):
            continue
        if "|" in line:
            name, url = line.split("|", 1)
            sources.append((name.strip(), url.strip()))
            continue
        parts = shlex.split(line)
        if len(parts) >= 2:
            sources.append((parts[0], parts[1]))
    return sources


def find_input_files(in_dir: Path) -> List[Path]:
    """Find audio_features.json files, preferring clean16k over raw16k per video.

    Matches convention from stages 03-05: clean16k is canonical for downstream stages.
    Falls back to raw16k only when clean16k is missing for a given video.
    """
    all_files = sorted(in_dir.rglob("*.audio_features.json"))

    # Group by video directory
    dir_files: Dict[Path, List[Path]] = {}
    for f in all_files:
        dir_files.setdefault(f.parent, []).append(f)

    selected: List[Path] = []
    for video_dir in sorted(dir_files.keys()):
        files = dir_files[video_dir]
        clean = [f for f in files if ".clean16k." in f.name]
        raw = [f for f in files if ".raw16k." in f.name]
        other = [f for f in files if ".clean16k." not in f.name and ".raw16k." not in f.name]

        if clean:
            selected.extend(clean)
        elif raw:
            selected.extend(raw)
        else:
            selected.extend(other)

    return sorted(selected)


# ---------------------------
# CLI
# ---------------------------

def main() -> None:
    parser = argparse.ArgumentParser(
        description="Video type classification + speaker labeling + conversation boundaries"
    )
    parser.add_argument(
        "name", nargs="?",
        help="Source name (folder under data/05.audio-features/)"
    )
    parser.add_argument(
        "youtube_url", nargs="?",
        help="YouTube URL (unused, accepted for pipeline compatibility)"
    )
    parser.add_argument(
        "--input",
        help="Input .audio_features.json file or directory"
    )
    parser.add_argument(
        "--output",
        help="Output directory (defaults to data/06.video-type/)"
    )
    parser.add_argument(
        "--test",
        action="store_true",
        help="Process test videos (data/test/05.audio-features/)"
    )
    parser.add_argument(
        "--sources",
        nargs="?",
        const="docs/pipeline/sources.txt",
        help="Process all sources from sources.txt file"
    )
    parser.add_argument(
        "--manifest",
        help="Manifest file: only process videos listed (docs/pipeline/batches/P001.txt)."
    )
    parser.add_argument(
        "--dry-run",
        action="store_true",
        help="Preview what would be processed"
    )
    parser.add_argument(
        "--overwrite",
        action="store_true",
        help="Overwrite existing output files"
    )
    parser.add_argument(
        "--debug",
        action="store_true",
        help="Enable debug logging for LLM responses"
    )
    parser.add_argument(
        "--model",
        help="Claude model alias or full name (e.g. sonnet, opus). Only used when calling Claude."
    )

    args = parser.parse_args()

    global DEBUG_MODE
    DEBUG_MODE = args.debug

    # Test Claude CLI availability
    claude_bin = find_claude_binary()
    if not claude_bin:
        print(f"{LOG_PREFIX} Error: Claude CLI binary not found")
        print(f"{LOG_PREFIX} Searched paths:")
        for p in CLAUDE_BINARY_PATHS:
            print(f"  - {p}")
        print(f"{LOG_PREFIX} Install Claude Code CLI: https://claude.ai/code")
        raise SystemExit(1)

    try:
        result = subprocess.run(
            [claude_bin, "--version"],
            capture_output=True, text=True, timeout=10,
        )
        if result.returncode != 0:
            print(f"{LOG_PREFIX} Warning: Claude CLI not responding properly")
        else:
            print(f"{LOG_PREFIX} Using Claude CLI: {claude_bin}")
    except subprocess.TimeoutExpired:
        print(f"{LOG_PREFIX} Warning: Claude CLI slow to respond")

    if isinstance(args.model, str) and args.model.strip():
        print(f"{LOG_PREFIX} Claude model: {args.model.strip()}")

    # Route to appropriate mode
    if args.test:
        _run_directory(test_input_root(), test_output_root(), args)
    elif args.manifest:
        _run_manifest(args)
    elif args.input:
        _run_input(args)
    elif args.sources:
        _run_sources(args)
    elif args.name:
        _run_named_source(args)
    else:
        raise SystemExit("Provide a source name, --input, --test, --manifest, or --sources")


def _run_input(args) -> None:
    input_path = Path(args.input)
    if not input_path.exists():
        input_path = repo_root() / args.input
    if not input_path.exists():
        raise SystemExit(f"Input not found: {args.input}")

    if input_path.is_file():
        out_dir = Path(args.output) if args.output else output_root()
        output_path = compute_output_path(input_path, out_dir)

        if output_path.exists() and not args.overwrite:
            print(f"{LOG_PREFIX} Output exists, skipping: {output_path}")
            return

        result = process_file(input_path, output_path, dry_run=args.dry_run, claude_model=args.model)
        print(f"\n{LOG_PREFIX} Done. Type: {result.get('video_type')}, Conversations: {result.get('conversations')}")
        return

    out_dir = Path(args.output) if args.output else output_root()
    _run_directory(input_path, out_dir, args)


def _run_sources(args) -> None:
    sources_path = repo_root() / args.sources
    if not sources_path.exists():
        raise SystemExit(f"Sources file not found: {sources_path}")

    out_base = Path(args.output) if args.output else output_root()
    total_files = 0
    total_convs = 0

    for src_name, _ in parse_sources_file(sources_path):
        src_in_dir = input_root() / src_name
        if not src_in_dir.exists():
            print(f"{LOG_PREFIX} Skipping {src_name}: no 05.audio-features output")
            continue

        src_out_dir = out_base / src_name
        files = find_input_files(src_in_dir)

        for input_file in files:
            output_file = compute_output_path(input_file, src_out_dir)
            if output_file.exists() and not args.overwrite:
                continue
            try:
                result = process_file(input_file, output_file, dry_run=args.dry_run, claude_model=args.model)
                total_convs += result.get("conversations", 0)
                total_files += 1
            except Exception as e:
                print(f"{LOG_PREFIX} Error: {e}")

    print(f"\n{LOG_PREFIX} Done. Processed {total_files} files, {total_convs} conversations")


def _run_manifest(args) -> None:
    """Run for videos listed in a manifest file."""
    manifest_path = Path(args.manifest)
    if not manifest_path.is_absolute():
        manifest_path = repo_root() / manifest_path
    if not manifest_path.exists():
        raise SystemExit(f"Manifest file not found: {manifest_path}")

    out_base = Path(args.output) if args.output else output_root()
    sources_map = load_manifest_sources(manifest_path)
    total_files = 0
    total_convs = 0

    for src_name, vid_ids in sorted(sources_map.items()):
        src_in_dir = input_root() / src_name
        if not src_in_dir.exists():
            print(f"{LOG_PREFIX} Skipping {src_name}: no 05.audio-features output")
            continue

        src_out_dir = out_base / src_name
        files = manifest_filter_files(find_input_files(src_in_dir), vid_ids)
        if not files:
            print(f"{LOG_PREFIX} Skipping {src_name}: no manifest videos found in input")
            continue

        print(f"{LOG_PREFIX} Manifest: {src_name} ({len(files)} videos)")
        _run_directory_with_files(files, src_in_dir, src_out_dir, args)


def _run_directory_with_files(files: List[Path], in_dir: Path, out_dir: Path, args) -> None:
    """Process a specific list of files (used by manifest mode and _run_directory)."""
    if not files:
        return

    print(f"{LOG_PREFIX} Input : {in_dir}")
    print(f"{LOG_PREFIX} Output: {out_dir}")
    print(f"{LOG_PREFIX} Files : {len(files)}")

    state_path = out_dir / ".video_type_state.json"
    state = load_state(state_path)

    total_convs = 0
    processed = 0
    skipped = 0
    failed = 0
    consecutive_failures = 0
    validation_failed = 0

    for input_file in files:
        file_key = str(input_file.relative_to(in_dir))

        if file_key in state.completed_files and not args.overwrite:
            skipped += 1
            continue

        output_file = compute_output_path(input_file, out_dir)

        if output_file.exists() and not args.overwrite:
            skipped += 1
            state.completed_files.append(file_key)
            save_state(state_path, state)
            continue

        state.in_progress = file_key
        save_state(state_path, state)

        try:
            result = process_file(input_file, output_file, dry_run=args.dry_run, claude_model=args.model)
            total_convs += result.get("conversations", 0)
            processed += 1

            if not result.get("validation_passed", True):
                validation_failed += 1
                consecutive_failures += 1
            else:
                consecutive_failures = 0

            if not args.dry_run:
                state.completed_files.append(file_key)
                state.in_progress = None
                save_state(state_path, state)

            if consecutive_failures >= MAX_CONSECUTIVE_FAILURES:
                print(f"\n{LOG_PREFIX} HALTING: {consecutive_failures} consecutive validation failures")
                break

        except Exception as e:
            print(f"{LOG_PREFIX} Error processing {input_file}: {e}")
            state.failures.append({"file": file_key, "error": str(e)})
            state.in_progress = None
            save_state(state_path, state)
            failed += 1
            consecutive_failures += 1

            if consecutive_failures >= MAX_CONSECUTIVE_FAILURES:
                print(f"\n{LOG_PREFIX} HALTING: {consecutive_failures} consecutive failures")
                break

    print(f"\n{LOG_PREFIX} Done.")
    print(f"  Processed:           {processed}")
    print(f"  Skipped:             {skipped}")
    print(f"  Failed (exception):  {failed}")
    print(f"  Failed (validation): {validation_failed}")
    print(f"  Conversations:       {total_convs}")

    total_attempted = processed + failed
    if total_attempted > 0:
        failure_rate = (failed + validation_failed) / total_attempted
        if failure_rate > MAX_FAILURE_RATE and total_attempted >= 5:
            print(f"\n{LOG_PREFIX} HALTING: Failure rate {failure_rate:.0%} exceeds {MAX_FAILURE_RATE:.0%} threshold")
            sys.exit(1)


def _run_named_source(args) -> None:
    """Run for a named source (from pipeline: ./06.video-type source_name url)."""
    name = args.name
    in_dir = input_root() / name
    if not in_dir.exists():
        raise SystemExit(f"Input directory not found: {in_dir}")

    out_dir = Path(args.output) if args.output else output_root() / name
    _run_directory(in_dir, out_dir, args)


def _run_directory(in_dir: Path, out_dir: Path, args) -> None:
    files = find_input_files(in_dir)
    if not files:
        print(f"{LOG_PREFIX} No .audio_features.json files found in: {in_dir}")
        return
    _run_directory_with_files(files, in_dir, out_dir, args)


if __name__ == "__main__":
    main()
