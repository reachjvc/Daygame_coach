#!/usr/bin/env python3
"""
scripts/training-data/07.LLM-conversations

Conversation Boundary Detection for Daygame Transcripts (LLM / Ollama)

v2.0 - Now with video type detection to prevent false positives:
- Detects if video is "infield", "talking_head", or "podcast"
- For non-infield videos, forces ALL segments to commentary (0 false positives)
- Only uses LLM segment classification for actual infield footage

Adds per-segment fields:
- conversation_id: integer ID for each distinct approach
- segment_type: "approach" | "commentary" | "transition"

Default I/O (recommended):
- Input : data/06.speakers/<source>/**/*.json
- Output: data/07.LLM-conversations/<source>/**/*.conversations.json

Use:

  A) One source (video / playlist / channel):
     ./scripts/training-data/07.LLM-conversations "daily_evolution" "https://www.youtube.com/watch?v=utuuVOXJunM"

  B) Batch from sources file:
     ./scripts/training-data/07.LLM-conversations --sources
     ./scripts/training-data/07.LLM-conversations --sources docs/sources.txt

Environment:
  OLLAMA_API_URL=http://localhost:11434
  OLLAMA_MODEL=llama3.1
"""

from __future__ import annotations

import argparse
import json
import os
import re
import shlex
import time
from dataclasses import dataclass
from pathlib import Path
from typing import Dict, List, Optional, Tuple

import requests


# ---------------------------
# Ollama configuration
# ---------------------------

OLLAMA_BASE_URL = os.environ.get("OLLAMA_API_URL", "http://localhost:11434")
OLLAMA_MODEL = os.environ.get("OLLAMA_MODEL", "llama3.1")


# ---------------------------
# Windowing (LLM batching)
# ---------------------------

WINDOW_SIZE = 10
WINDOW_OVERLAP = 3


# ---------------------------
# Patterns (heuristics fallback)
# ---------------------------

COMMENTARY_PATTERNS = [
    r"(hey |hi )?(guys|everyone|youtube)\b",
    r"\badam here\b",
    r"\bwelcome back\b",
    r"\b(let me|i('ll| will)) (explain|break down|show you)\b",
    r"\bwhat you('re| are) (about to|going to) see\b",
    r"\bas you (saw|can see|noticed)\b",
    r"\b(this|that) is (what|why|how)\b",
    r"\b(the reason|the key|the trick|the point) (is|here)\b",
    r"\bnotice (how|here|that)\b",
    r"\b(so|okay),? (guys|basically|to summarize)\b",
    r"\b(first|second|third)(ly)?\b",
    r"\btip (number )?\d+\b",
    r"\bmistake (number )?\d+\b",
    r"\b(don't|do not) (do|want|forget)\b",
    r"\b(always|never) (do|say|approach)\b",
    r"\bthank(s| you) for (watching|listening)\b",
    r"\b(subscribe|like|comment|link)\b",
    r"\bcoaching|bootcamp|program\b",
]

# IMPORTANT:
# These are meant to detect the START of an infield interaction, so anchor them.
# Avoid bare greetings ("hey") here — too many talking-head intros start that way.
APPROACH_STARTER_PATTERNS = [
    r"^\s*excuse me\b",
    r"^\s*sorry to bother you\b",
    r"^\s*sorry to interrupt\b",
    r"^\s*i don't mean to interrupt\b",
    r"^\s*i know you're busy but\b",
    r"^\s*hey,?\s*(just )?(one|two|quick) (second|sec)\b",
    r"^\s*(hi|hey|hello),?\s*excuse me\b",
    r"^\s*(hi|hey|hello),?\s*(i )?(just )?(saw|noticed) you\b",
    r"^\s*super quick question\b",
    r"^\s*quick question\b",
    r"^\s*sorry,?\s*(super )?quick question\b",
    r"^\s*can i ask you (something|a question)\b",
    r"^\s*can i ask you a quick question\b",
    r"^\s*do you have a (second|sec|minute)\b",
    r"^\s*can i stop you for a (second|sec)\b",
    r"^\s*are you in a rush\b",
    r"^\s*i promise i'll be quick\b",
    r"^\s*this is (so |really )?random\b",
    r"^\s*i know this is (so |really )?(random|weird) but\b",
    r"^\s*i had to (come|stop|say)\b",
    r"^\s*i had to say hi\b",
    r"^\s*i had to meet you\b",
    r"^\s*i had to come say hello\b",
    r"^\s*you caught my (eye|attention)\b",
    r"^\s*i like your (style|vibe|energy|outfit|shoes|jacket|coat|dress|hair)\b",
    r"^\s*you have a really (nice|cool) (style|vibe|energy)\b",
    r"^\s*i like your (shoes|jacket|coat|outfit|dress|hair)\b",
    r"^\s*you seem (cool|nice|interesting)\b",
    r"^\s*where did you get your (shoes|jacket|coat|dress|boots)\b",
    r"^\s*you('re| are) from around here\b",
    r"^\s*are you from around here\??(?:\s|$)",
    r"^\s*do you speak english\??(?:\s|$)",
    r"^\s*what are you up to( today)?\b",
    r"^\s*i'm (not|really not) trying to be weird but\b",
    r"^\s*you look like you know this area\b",
]

APPROACH_EXCHANGE_PATTERNS = [
    r"\bwhat's your name\b",
    r"\bwhere (are you|you) from\b",
    r"\bhow long (have you been|you been) (here|in)\b",
    r"\bdo you live (around here|here)\b",
    r"\bare you (a student|studying)\b",
    r"\bwhat (do you do|are you doing|are you up to|brings you)\b",
    r"\bwhat do you do for (work|a living)\b",
    r"\bwhat do you study\b",
    r"\bhow old are you\b",
    r"\b(nice|good|great) to meet you\b",
    r"\bnice meeting you\b",
    r"\bwhat's your (instagram|insta|snap)\b",
    r"\b(can i|get|have|take) your (number|instagram|insta|snap|whatsapp)\b",
    r"\bcan i grab your (number|instagram|insta|snap|whatsapp)\b",
    r"\blet('s| us) (exchange|swap) (numbers|instagrams|instas)\b",
    r"\byou('re| are) (so |really |very )?(cute|beautiful|pretty|gorgeous)\b",
    r"\byou have a (nice|cool) (style|vibe|energy)\b",
    r"\bdo you have (instagram|insta|snap|whatsapp)\b",
    r"\bwe should (hang out|grab|get) (a )?(coffee|drink|cocktail)\b",
    r"\bhow are you( doing)?\b",
    r"\bhow's your day( going)?\b",
    r"\bwhere are you (going|headed)\b",
    r"\bwhat are you doing later\b",
    r"\bare you (single|seeing someone)\b",
    r"\bdo you have a boyfriend\b",
    r"\bput your number in\b",
    r"\btype (your number|it) in\b",
    r"\b(i('ll| will) )?(text|message) you\b",
]


# ---------------------------
# Video Type Detection (v2.0)
# ---------------------------

# Keywords that strongly indicate talking_head content
TALKING_HEAD_KEYWORDS = [
    "guide", "advice", "tips", "mistakes", "how to", "mindset", "rant",
    "motivation", "charisma", "communication", "texting", "messaging",
    "psychology", "confidence", "anxiety", "introvert", "what to say",
    "secrets", "rules", "principles", "theory", "breakdown", "podcast",
    "interview", "q&a", "questions"
]

# Keywords that strongly indicate infield content
INFIELD_KEYWORDS = [
    "infield", "daygame session", "approaches", "number close",
    "instant date", "street approach", "cold approach footage",
    "brutal session", "field report"
]


def detect_video_type_from_title(title: str) -> Optional[str]:
    """Fast heuristic detection from video title - no LLM needed."""
    title_lower = title.lower()

    # Check infield keywords first (higher priority)
    if any(kw in title_lower for kw in INFIELD_KEYWORDS):
        return "infield"

    # Check talking_head keywords
    if any(kw in title_lower for kw in TALKING_HEAD_KEYWORDS):
        return "talking_head"

    return None


def detect_video_type_with_llm(segments: List[Dict], video_title: str) -> str:
    """LLM-based video type detection by sampling segments."""

    # First try heuristic - it's fast and reliable for clear cases
    heuristic_type = detect_video_type_from_title(video_title)
    if heuristic_type == "talking_head":
        return "talking_head"
    if heuristic_type == "infield":
        return "infield"

    # Sample segments for LLM analysis
    sample_indices = list(range(min(5, len(segments))))
    if len(segments) > 10:
        mid = len(segments) // 2
        sample_indices += list(range(mid - 2, mid + 3))
    if len(segments) > 15:
        sample_indices += list(range(len(segments) - 5, len(segments)))

    sample_indices = sorted(set(i for i in sample_indices if 0 <= i < len(segments)))[:15]

    sample_texts = []
    for i in sample_indices:
        text = segments[i].get("text", "")[:100]
        sample_texts.append(f"[{i}] {text}")

    prompt = f"""Classify this video's content type based on the title and sample segments.

VIDEO TITLE: "{video_title}"

SAMPLE SEGMENTS:
{chr(10).join(sample_texts)}

Content types:
- "infield": Coach approaching women on the street. Contains ACTUAL openers like "excuse me, two seconds", real responses from women, number closes.
- "talking_head": Coach talking to camera about theory, tips, or experiences. No actual approaches happening.
- "podcast": Coach talking with a co-host or student in interview/discussion format. Two people discussing topics, NOT approaching women.

KEY DISTINCTION: In "infield", the coach is DOING approaches. In "talking_head" and "podcast", the coach is TALKING ABOUT approaches.

Respond with ONLY one word: infield, talking_head, or podcast

Video type:"""

    try:
        response = requests.post(
            f"{OLLAMA_BASE_URL}/api/generate",
            json={
                "model": OLLAMA_MODEL,
                "prompt": prompt,
                "stream": False,
                "options": {"temperature": 0.3, "num_predict": 50},
            },
            timeout=60,
        )
        if response.ok:
            response_text = response.json().get("response", "").lower().strip()
            for vtype in ["infield", "talking_head", "podcast"]:
                if vtype in response_text:
                    return vtype
    except requests.exceptions.RequestException:
        pass

    # Default to infield if uncertain (safer to over-classify than miss real approaches)
    return "infield"


def force_commentary_for_non_infield(segments: List[Dict], video_type: str) -> List[Dict]:
    """For talking_head/podcast videos, force ALL segments to commentary.

    This guarantees 0 false positives for non-infield content.
    """
    for seg in segments:
        seg["conversation_id"] = 0
        seg["segment_type"] = "commentary"
        seg["boundary_detection"] = {
            "is_new_conversation": False,
            "confidence": 0.95,
            "method": f"forced_{video_type}",
        }
    return segments


def extract_video_title_from_filename(filename: str) -> str:
    """Extract video title from filename like 'Video Title [id].something.json'."""
    # Remove extension(s)
    name = filename
    while "." in name:
        name = name.rsplit(".", 1)[0]

    # Extract title before [video_id]
    match = re.match(r"(.+?)\s*\[", name)
    if match:
        return match.group(1).strip()
    return name


@dataclass
class SegmentAnalysis:
    segment_type: str  # "approach", "commentary", "transition"
    conversation_id: int
    is_new_conversation: bool
    confidence: float
    reasoning: str


def compile_patterns(patterns: List[str]) -> List[re.Pattern]:
    return [re.compile(p, re.IGNORECASE) for p in patterns]


class ConversationDetector:
    def __init__(self, use_llm: bool = True):
        self.use_llm = use_llm
        self.commentary_re = compile_patterns(COMMENTARY_PATTERNS)
        self.approach_start_re = compile_patterns(APPROACH_STARTER_PATTERNS)
        self.approach_exchange_re = compile_patterns(APPROACH_EXCHANGE_PATTERNS)

    def _heuristic_classify(self, text: str, prev_type: Optional[str] = None) -> Tuple[str, float]:
        text_lower = text.lower().strip()

        commentary_score = sum(1 for p in self.commentary_re if p.search(text_lower))

        approach_score = sum(1 for p in self.approach_start_re if p.search(text_lower))
        approach_score += sum(0.5 for p in self.approach_exchange_re if p.search(text_lower))

        # Short responses are likely approach exchanges
        word_count = len(text.split())
        if word_count < 8 and "?" not in text:
            approach_score += 0.5

        # Questions from coach are likely approach
        if text.strip().endswith("?") and word_count < 15:
            approach_score += 0.5

        if commentary_score > approach_score and commentary_score > 0:
            return "commentary", min(0.9, 0.5 + commentary_score * 0.1)
        if approach_score > commentary_score and approach_score > 0:
            return "approach", min(0.9, 0.5 + approach_score * 0.1)

        return prev_type or "unknown", 0.3

    def _is_likely_new_approach(self, text: str) -> bool:
        text_lower = text.lower().strip()
        return any(p.search(text_lower) for p in self.approach_start_re)

    def _call_ollama(self, prompt: str, retries: int = 3) -> Optional[str]:
        for attempt in range(retries):
            try:
                response = requests.post(
                    f"{OLLAMA_BASE_URL}/api/generate",
                    json={
                        "model": OLLAMA_MODEL,
                        "prompt": prompt,
                        "stream": False,
                        "options": {
                            "temperature": 0.3,
                            "num_predict": 500,
                        },
                    },
                    timeout=60,
                )
                if response.ok:
                    return response.json().get("response", "")
            except requests.exceptions.RequestException as e:
                if attempt < retries - 1:
                    time.sleep(1)
                    continue
                print(f"[LLM_conversations] Ollama error: {e}")
        return None

    def _analyze_window_with_llm(
        self,
        segments: List[Dict],
        start_idx: int,
        current_conversation_id: int,
        prev_type: str,
    ) -> List[SegmentAnalysis]:
        window = segments[start_idx : start_idx + WINDOW_SIZE]

        segment_texts = []
        for i, seg in enumerate(window):
            text = seg.get("text", "").strip()
            segment_texts.append(f"[{i}] {text}")

        prompt = f"""Analyze these transcript segments from a daygame video where a coach approaches women on the street.

Classify each segment as one of:
- "approach": Part of an actual conversation with a woman (opener, vibing, number close, etc.)
- "commentary": Coach talking to camera (explanation, theory, breakdown, intro/outro)
- "transition": Brief marker between sections

Also identify if a segment starts a NEW approach (a different woman).

Previous segment type: {prev_type}
Current conversation ID: {current_conversation_id}

Segments:
{chr(10).join(segment_texts)}

IMPORTANT: Respond with ONLY a valid JSON array. No explanation, no text before or after. Just the JSON.
There are exactly {len(segment_texts)} segments, so return exactly {len(segment_texts)} objects.

Example format:
[{{"index": 0, "type": "commentary", "new_conversation": false, "confidence": 0.8}}, {{"index": 1, "type": "approach", "new_conversation": true, "confidence": 0.9}}]

Key signals (new conversation openers often start with):
- "excuse me", "sorry to interrupt", "quick question", "do you have a second", "I just saw you", "this is so random"
- Short responses like "yeah", "kind of", "thank you" are likely approach exchanges (girl talking)
- References to "you guys", "as you can see", "notice how" are commentary

JSON response:"""

        response = self._call_ollama(prompt)
        if not response:
            return self._analyze_window_heuristic(window, current_conversation_id, prev_type)

        try:
            code_block_match = re.search(r"```(?:json)?\\s*(\\[[\\s\\S]*?\\])\\s*```", response)
            if code_block_match:
                json_str = code_block_match.group(1)
            else:
                json_str = None
                start = response.find("[")
                if start != -1:
                    bracket_count = 0
                    for i, char in enumerate(response[start:], start):
                        if char == "[":
                            bracket_count += 1
                        elif char == "]":
                            bracket_count -= 1
                            if bracket_count == 0:
                                json_str = response[start : i + 1]
                                break

            if json_str:
                analyses = json.loads(json_str)
                results: List[SegmentAnalysis] = []
                conv_id = current_conversation_id

                for item in analyses:
                    seg_type = item.get("type", "unknown")
                    is_new = bool(item.get("new_conversation", False))
                    raw_conf = item.get("confidence")
                    confidence = float(raw_conf) if raw_conf is not None else 0.5

                    if is_new and seg_type == "approach":
                        conv_id += 1

                    results.append(
                        SegmentAnalysis(
                            segment_type=seg_type,
                            conversation_id=conv_id if seg_type == "approach" else 0,
                            is_new_conversation=is_new,
                            confidence=confidence,
                            reasoning="llm_analysis",
                        )
                    )

                return results
        except (json.JSONDecodeError, KeyError, ValueError) as e:
            print(f"[LLM_conversations] Failed to parse LLM response: {e}")
            print(f"[LLM_conversations] Response preview: {response[:500]}...")

        return self._analyze_window_heuristic(window, current_conversation_id, prev_type)

    def _analyze_window_heuristic(
        self,
        window: List[Dict],
        current_conversation_id: int,
        prev_type: str,
    ) -> List[SegmentAnalysis]:
        results: List[SegmentAnalysis] = []
        conv_id = current_conversation_id
        last_type = prev_type

        for seg in window:
            text = seg.get("text", "").strip()
            seg_type, confidence = self._heuristic_classify(text, last_type)

            is_new = False
            if seg_type == "approach" and self._is_likely_new_approach(text):
                is_new = True
                conv_id += 1

            results.append(
                SegmentAnalysis(
                    segment_type=seg_type,
                    conversation_id=conv_id if seg_type == "approach" else 0,
                    is_new_conversation=is_new,
                    confidence=confidence,
                    reasoning="heuristic",
                )
            )

            last_type = seg_type

        return results

    def detect_conversations(self, segments: List[Dict]) -> List[Dict]:
        if not segments:
            return segments

        total = len(segments)
        current_conversation_id = 0
        prev_type = "unknown"

        idx = 0
        while idx < total:
            end_idx = min(idx + WINDOW_SIZE, total)

            if self.use_llm:
                analyses = self._analyze_window_with_llm(segments, idx, current_conversation_id, prev_type)
            else:
                window = segments[idx:end_idx]
                analyses = self._analyze_window_heuristic(window, current_conversation_id, prev_type)

            non_overlap_count = WINDOW_SIZE - WINDOW_OVERLAP if idx > 0 else WINDOW_SIZE
            for i, analysis in enumerate(analyses[:non_overlap_count]):
                seg_idx = idx + i
                if seg_idx >= total:
                    break

                segments[seg_idx]["conversation_id"] = analysis.conversation_id
                segments[seg_idx]["segment_type"] = analysis.segment_type
                segments[seg_idx]["boundary_detection"] = {
                    "is_new_conversation": analysis.is_new_conversation,
                    "confidence": analysis.confidence,
                    "method": analysis.reasoning,
                }

                if analysis.conversation_id > current_conversation_id:
                    current_conversation_id = analysis.conversation_id
                prev_type = analysis.segment_type

            idx += WINDOW_SIZE - WINDOW_OVERLAP
            if idx >= total:
                break

            if (idx % 50) == 0:
                print(f"[LLM_conversations] Processed {idx}/{total} segments...")

        return segments


def process_file(input_path: Path, output_path: Path, use_llm: bool = True) -> Dict:
    with input_path.open("r", encoding="utf-8") as f:
        data = json.load(f)

    segments = data.get("segments", [])
    print(f"[LLM_conversations] Processing {input_path.name} ({len(segments)} segments)")

    # Step 1: Detect video type (v2.0 improvement)
    video_title = extract_video_title_from_filename(input_path.name)
    print(f"[LLM_conversations] Video title: {video_title}")

    video_type = detect_video_type_with_llm(segments, video_title)
    print(f"[LLM_conversations] Detected video type: {video_type}")

    # Step 2: Process based on video type
    if video_type in ["talking_head", "podcast"]:
        # Force all segments to commentary - guarantees 0 false positives
        print(f"[LLM_conversations] Non-infield content - forcing all segments to commentary")
        processed_segments = force_commentary_for_non_infield(segments, video_type)
    else:
        # Use LLM to detect conversation boundaries for infield content
        detector = ConversationDetector(use_llm=use_llm)
        processed_segments = detector.detect_conversations(segments)

    data["segments"] = processed_segments
    data["video_type"] = video_type  # Store detected type

    conversation_ids = set()
    type_counts: Dict[str, int] = {}
    for seg in processed_segments:
        conv_id = int(seg.get("conversation_id", 0) or 0)
        if conv_id > 0:
            conversation_ids.add(conv_id)
        seg_type = str(seg.get("segment_type", "unknown"))
        type_counts[seg_type] = type_counts.get(seg_type, 0) + 1

    data["conversation_summary"] = {
        "total_conversations": len(conversation_ids),
        "segment_type_counts": type_counts,
        "video_type": video_type,
    }

    output_path.parent.mkdir(parents=True, exist_ok=True)
    with output_path.open("w", encoding="utf-8") as f:
        json.dump(data, f, indent=2)

    print(f"[LLM_conversations] Found {len(conversation_ids)} distinct conversations")
    print(f"[LLM_conversations] Segment types: {type_counts}")

    return data


# ---------------------------
# Path helpers
# ---------------------------

def repo_root() -> Path:
    # scripts/training-data/<thisfile> -> repo root = parents[2]
    return Path(__file__).resolve().parents[2]


def features_root() -> Path:
    return repo_root() / "data" / "03.audio-features"


def speakers_root() -> Path:
    return repo_root() / "data" / "06.speakers"


def output_root() -> Path:
    return repo_root() / "data" / "07.LLM-conversations"


def infer_name_from_input(input_path: Path) -> Optional[str]:
    try:
        parts = input_path.resolve().parts
    except OSError:
        return None

    # Look for: .../data/<stage>/<name>/...
    if "data" not in parts:
        return None
    i = parts.index("data")
    if i + 2 >= len(parts):
        return None
    stage = parts[i + 1]
    name = parts[i + 2]
    if stage in {"03.audio-features", "05.tonality", "06.speakers", "07.LLM-conversations"}:
        return name
    return None


def extract_video_id_from_url(url: str) -> Optional[str]:
    url = (url or "").strip()
    if not url:
        return None
    m = re.search(r"[?&]v=([^&]+)", url)
    if m:
        return m.group(1)
    m = re.search(r"youtu\.be/([^?&/]+)", url)
    if m:
        return m.group(1)
    return None


def parse_sources_file(path: Path) -> List[Tuple[str, str]]:
    sources: List[Tuple[str, str]] = []
    for raw in path.read_text(encoding="utf-8").splitlines():
        line = raw.strip()
        if not line or line.startswith("#"):
            continue
        if "|" in line:
            name, url = line.split("|", 1)
            name = name.strip()
            url = url.strip()
            if name and url:
                sources.append((name, url))
            continue
        parts = shlex.split(line)
        if len(parts) >= 2:
            sources.append((parts[0], parts[1]))
    return sources


def resolve_existing_path(p: Path) -> Optional[Path]:
    if p.exists():
        return p.resolve()
    p2 = repo_root() / p
    if p2.exists():
        return p2.resolve()
    return None


def compute_output_file_for_input_file(input_file: Path, out_dir: Path, suffix: str) -> Path:
    stem = input_file.stem
    if stem.endswith(".classified"):
        stem = stem[:-11]
    return out_dir / f"{stem}{suffix}"


def main() -> None:
    parser = argparse.ArgumentParser(description="Detect conversation boundaries (LLM / Ollama).")
    parser.add_argument(
        "name",
        nargs="?",
        help="Channel/playlist name (same as folder under data/06.speakers).",
    )
    parser.add_argument(
        "youtube_url",
        nargs="?",
        help="YouTube URL (video/playlist/channel). Video URL filters by ID unless --video is provided.",
    )
    parser.add_argument(
        "--channel",
        help="Alias for the positional name argument.",
    )
    parser.add_argument(
        "--sources",
        nargs="?",
        const="docs/sources.txt",
        help="Process all sources from a sources.txt file (default: docs/sources.txt).",
    )
    parser.add_argument("--input", help="Input JSON file or directory (defaults to data/06.speakers/<name>).")
    parser.add_argument(
        "--output",
        help="Output JSON file or directory (defaults to data/07.LLM-conversations/<name>).",
    )
    parser.add_argument("--video", help="Optional filename substring filter when --input is a directory.")
    parser.add_argument("--no-llm", action="store_true", help="Use only heuristics (no LLM calls).")
    parser.add_argument("--suffix", default=".conversations.json", help="Output file suffix.")
    parser.add_argument("--overwrite", action="store_true", help="Overwrite already-written outputs.")
    parser.add_argument("--dry-run", action="store_true", help="Print what would happen, don’t write files.")
    args = parser.parse_args()

    if args.name and args.channel:
        raise SystemExit("Provide either positional <name> OR --channel, not both.")
    name = (args.channel or args.name or "").strip() or None

    def compute_use_llm() -> bool:
        use_llm = not args.no_llm
        if use_llm:
            try:
                response = requests.get(f"{OLLAMA_BASE_URL}/api/tags", timeout=5)
                if not response.ok:
                    print("[LLM_conversations] Warning: Ollama not available, using heuristics only")
                    return False
            except requests.exceptions.RequestException:
                print(f"[LLM_conversations] Warning: Cannot connect to Ollama at {OLLAMA_BASE_URL}")
                return False
        return use_llm

    def run_detection(input_path: Path, output_path: Path, video_filter: Optional[str], use_llm: bool) -> None:
        if input_path.is_dir():
            files = sorted(
                f
                for f in input_path.rglob("*.json")
                if not f.name.endswith(args.suffix) and not f.name.endswith(".conversations.json")
            )
            if video_filter:
                needle = video_filter.strip()
                files = [f for f in files if needle in f.name or needle in f.as_posix()]

            if not files:
                print(f"[LLM_conversations] No input JSON files found under: {input_path}")
                return

            print(f"[LLM_conversations] Input : {input_path}")
            print(f"[LLM_conversations] Output: {output_path}")
            print(f"[LLM_conversations] Files : {len(files)}")

            written = 0
            skipped = 0
            for src in files:
                rel_path = src.relative_to(input_path)
                dest_dir = output_path / rel_path.parent
                dest = compute_output_file_for_input_file(src, dest_dir, args.suffix)

                if dest.exists() and not args.overwrite:
                    skipped += 1
                    continue

                if args.dry_run:
                    print(f"[dry-run] Would write: {dest}")
                    written += 1
                    continue

                print(f"\n[LLM_conversations] {src.name} -> {dest.name}")
                process_file(src, dest, use_llm=use_llm)
                written += 1

            print("")
            print("[LLM_conversations] Done.")
            print(f"  written : {written}")
            print(f"  skipped : {skipped}")
            return

        # Single file input
        if output_path.suffix:
            dest = output_path
        else:
            dest = compute_output_file_for_input_file(input_path, output_path, args.suffix)

        if dest.exists() and not args.overwrite:
            print(f"[LLM_conversations] Output exists, skipping: {dest}")
            return

        if args.dry_run:
            print(f"[dry-run] Would write: {dest}")
            return

        print(f"[LLM_conversations] {input_path} -> {dest}")
        process_file(input_path, dest, use_llm=use_llm)

    if args.sources:
        sources_path = Path(args.sources)
        if not sources_path.is_absolute():
            sources_path = (repo_root() / sources_path).resolve()
        if not sources_path.exists():
            raise SystemExit(f"Missing sources file: {sources_path}")

        use_llm = compute_use_llm()
        for src_name, src_url in parse_sources_file(sources_path):
            input_path = speakers_root() / src_name
            output_path = output_root() / src_name
            video_filter = args.video or extract_video_id_from_url(src_url)
            run_detection(input_path, output_path, video_filter=video_filter, use_llm=use_llm)
        return

    if args.input:
        resolved = resolve_existing_path(Path(args.input))
        if not resolved and name:
            resolved = resolve_existing_path(speakers_root() / args.input) or resolve_existing_path(features_root() / args.input)
        if not resolved:
            raise SystemExit(f"Input not found: {args.input}")
        input_path = resolved
    else:
        if not name:
            raise SystemExit("Missing input: provide --input or a <name>/--channel.")
        input_path = speakers_root() / name
        if not input_path.exists():
            legacy_features = features_root() / name
            if legacy_features.exists():
                print(f"[LLM_conversations] Note: speakers folder not found, falling back to features: {legacy_features}")
                input_path = legacy_features
            else:
                raise SystemExit(f"Expected speakers folder not found: {input_path}")

    inferred_name = infer_name_from_input(input_path)
    if not name:
        name = inferred_name

    if args.output:
        output_path = resolve_existing_path(Path(args.output)) or (repo_root() / args.output).resolve()
    else:
        if not name:
            raise SystemExit("Missing output: provide --output, or use an input under data/<stage>/<name>.")
        output_path = output_root() / name
    use_llm = compute_use_llm()
    video_filter = args.video or extract_video_id_from_url(args.youtube_url or "")
    run_detection(input_path, output_path, video_filter=video_filter, use_llm=use_llm)
    return


if __name__ == "__main__":
    main()
