#!/usr/bin/env python3
"""
scripts/training-data/05.speaker-fusion

Speaker Fusion - Multi-signal speaker identification for daygame videos.

Combines:
1. Pyannote diarization (SPEAKER_XX from Stage 02)
2. Pitch-based gender classification (from Stage 04)
3. LLM conversation inference
4. Conversation flow priors

Target: â‰¥95% speaker accuracy

Reads:
  data/02.transcribe/<source>/<video>/*.whisperx.json  (diarization labels)
  data/04.audio-features/<source>/<video>/*.audio_features.json  (pitch data, optional)

Writes:
  data/05.speaker-fusion/<source>/<video>/*.speakers.json

Use:
  ./scripts/training-data/05.speaker-fusion "daily_evolution" "https://www.youtube.com/watch?v=utuuVOXJunM"
  ./scripts/training-data/05.speaker-fusion --test  # Run accuracy test only
"""

from __future__ import annotations

import argparse
import json
import re
import subprocess
import sys
import time
from collections import Counter, defaultdict
from dataclasses import dataclass, field
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple


# -------------------------
# Config
# -------------------------

@dataclass
class FusionConfig:
    # Signal weights (must sum to 1.0)
    # Tuned based on testing: flow rules are accurate for patterns,
    # pitch often fails on short segments, LLM needs context
    weight_diarization: float = 0.20
    weight_pitch: float = 0.25
    weight_llm: float = 0.30
    weight_flow: float = 0.25

    # LLM settings
    llm_model: str = "llama3.1"
    llm_timeout: int = 120
    llm_batch_size: int = 30  # segments per LLM call

    # Pitch thresholds (Hz)
    pitch_male_max: float = 165.0
    pitch_female_min: float = 180.0

    # Confidence thresholds
    min_confidence_for_label: float = 0.5

    # Diarization mapping
    # Assume SPEAKER_00 or first speaker is usually coach (initiates)
    assume_first_speaker_coach: bool = True


# -------------------------
# Utilities
# -------------------------

def repo_root() -> Path:
    return Path(__file__).resolve().parents[2]


def extract_video_id(url: str) -> Optional[str]:
    url = (url or "").strip()
    if not url:
        return None
    m = re.search(r"[?&]v=([^&]+)", url)
    if m:
        return m.group(1)
    m = re.search(r"youtu\.be/([^?&/]+)", url)
    if m:
        return m.group(1)
    return None


# -------------------------
# Signal 1: Diarization (from Stage 02)
# -------------------------

def load_diarization_transcript(path: Path) -> Tuple[List[Dict], Dict[str, str]]:
    """
    Load whisperx transcript with SPEAKER_XX labels.
    Returns: (segments, speaker_mapping)
    """
    data = json.loads(path.read_text(encoding="utf-8"))
    segments = data.get("segments", [])

    # Build speaker mapping based on order of appearance
    speaker_order = []
    for seg in segments:
        spk = seg.get("speaker", "")
        if spk and spk not in speaker_order:
            speaker_order.append(spk)

    # Map speakers: first = coach, second = girl, others = other
    mapping = {}
    for i, spk in enumerate(speaker_order):
        if i == 0:
            mapping[spk] = "COACH"
        elif i == 1:
            mapping[spk] = "GIRL"
        else:
            mapping[spk] = "OTHER"

    return segments, mapping


def get_diarization_signal(seg: Dict, mapping: Dict[str, str], cfg: FusionConfig) -> Dict[str, Any]:
    """Get speaker signal from diarization."""
    spk = seg.get("speaker", "")
    if not spk or spk not in mapping:
        return {"speaker": "unknown", "confidence": 0.0, "source": "diarization"}

    label = mapping[spk]
    # Diarization confidence based on segment duration
    duration = seg.get("end", 0) - seg.get("start", 0)
    if duration >= 2.0:
        confidence = 0.7
    elif duration >= 1.0:
        confidence = 0.5
    else:
        confidence = 0.3  # Short segments are less reliable

    return {"speaker": label, "confidence": confidence, "source": "diarization"}


# -------------------------
# Signal 2: Pitch-based gender (from Stage 04 or computed)
# -------------------------

def load_audio_features(path: Path) -> Dict[int, Dict]:
    """Load audio features indexed by segment start time."""
    if not path.exists():
        return {}

    data = json.loads(path.read_text(encoding="utf-8"))
    segments = data.get("segments", [])

    # Index by start time (rounded to 2 decimals for matching)
    features_by_time = {}
    for seg in segments:
        start = round(seg.get("start", 0), 2)
        features_by_time[start] = seg.get("features", {})

    return features_by_time


def get_pitch_signal(seg: Dict, audio_features: Dict, cfg: FusionConfig) -> Dict[str, Any]:
    """Get speaker signal from pitch analysis."""
    start = round(seg.get("start", 0), 2)
    features = audio_features.get(start, {})
    pitch = features.get("pitch", {})

    # Check for pre-computed gender classification
    pitch_gender = pitch.get("pitch_gender", "")
    pitch_gender_conf = pitch.get("pitch_gender_confidence", 0.0)

    # Only trust pitch if confidence is high enough (>0.6)
    # Low-confidence pitch often misclassifies due to short segments/harmonics
    MIN_PITCH_CONFIDENCE = 0.6

    if pitch_gender == "male" and pitch_gender_conf >= MIN_PITCH_CONFIDENCE:
        return {"speaker": "COACH", "confidence": pitch_gender_conf, "source": "pitch"}
    elif pitch_gender == "female" and pitch_gender_conf >= MIN_PITCH_CONFIDENCE:
        return {"speaker": "GIRL", "confidence": pitch_gender_conf, "source": "pitch"}
    elif pitch_gender in ("male", "female"):
        # Low confidence - mark as unknown but include some info
        return {"speaker": "unknown", "confidence": 0.0, "source": "pitch"}

    # Fallback: compute from mean_hz
    mean_hz = pitch.get("mean_hz", 0)
    voiced_ratio = pitch.get("voiced_ratio", 0)

    if mean_hz <= 0 or voiced_ratio < 0.1:
        return {"speaker": "unknown", "confidence": 0.0, "source": "pitch"}

    if mean_hz < cfg.pitch_male_max:
        confidence = min(0.9, 0.5 + (cfg.pitch_male_max - mean_hz) / 100)
        if confidence >= MIN_PITCH_CONFIDENCE:
            return {"speaker": "COACH", "confidence": confidence, "source": "pitch"}
    elif mean_hz > cfg.pitch_female_min:
        confidence = min(0.9, 0.5 + (mean_hz - cfg.pitch_female_min) / 100)
        if confidence >= MIN_PITCH_CONFIDENCE:
            return {"speaker": "GIRL", "confidence": confidence, "source": "pitch"}

    # Ambiguous or low confidence
    return {"speaker": "unknown", "confidence": 0.0, "source": "pitch"}


# -------------------------
# Signal 3: LLM conversation inference
# -------------------------

def call_ollama(prompt: str, model: str, timeout: int = 120) -> str:
    """Call Ollama LLM."""
    try:
        result = subprocess.run(
            ["ollama", "run", model],
            input=prompt,
            capture_output=True,
            text=True,
            timeout=timeout
        )
        return result.stdout.strip()
    except Exception as e:
        print(f"  [LLM ERROR] {e}")
        return ""


def parse_llm_speaker_output(response: str, num_segments: int) -> Dict[int, Dict[str, Any]]:
    """Parse LLM speaker output: [N]: SPEAKER -> dict"""
    results = {}
    for line in response.strip().split('\n'):
        match = re.search(r'\[?(\d+)\]?[:\s]+(\w+)', line)
        if match:
            idx = int(match.group(1))
            speaker = match.group(2).upper()
            if speaker in ["COACH", "GIRL", "OTHER"] and 0 <= idx < num_segments:
                results[idx] = {"speaker": speaker, "confidence": 0.75, "source": "llm"}
    return results


def get_llm_signals_batch(segments: List[Dict], cfg: FusionConfig) -> Dict[int, Dict[str, Any]]:
    """Get speaker signals from LLM for a batch of segments."""
    if not segments:
        return {}

    # Build transcript text
    lines = []
    for i, seg in enumerate(segments):
        text = seg.get("text", "").strip()
        lines.append(f"[{i}]: {text}")

    transcript_text = '\n'.join(lines)
    num_segments = len(segments)

    prompt = f'''Identify the speaker for each segment in this daygame approach conversation.

CONTEXT: A male COACH approaches a woman (GIRL) on the street.

COACH characteristics:
- Opens the conversation (first speaker)
- Asks questions ("Where are you from?", "What do you do?")
- Introduces himself ("I'm [name]", "My name is...")
- Makes statements/observations ("I noticed...", "You look like...")
- Self-corrects or backtracks ("No, I'm just playing", "I'm kidding")
- Gives compliments or teases
- May speak multiple consecutive lines

GIRL characteristics:
- Responds to questions (often short: "Yes", "No", "Okay")
- Shares personal info WHEN ASKED ("I'm from...", "I work at...")
- Repeats/echoes what coach said (if coach says "I'm Erin", girl might say "Erin" or "Erin?")
- Asks reciprocal questions ("What about you?")
- Reacts to statements ("Really?", "That's nice")

CRITICAL PATTERNS:
- "I'm [Name]" as introduction = COACH (the one who approaches introduces himself)
- Single word echoing a name just said = GIRL (she's repeating his name)
- "No, I'm just..." or "I'm kidding" = COACH (self-correction)
- "I used to be..." in response to a question/tease = GIRL (sharing info)
- "Nice to meet you" after introduction = can be either, but FIRST one is usually COACH

TRANSCRIPT:
{transcript_text}

For EACH segment [0] to [{num_segments - 1}], identify the speaker based on the patterns above:
[0]: COACH
[1]: GIRL
...continue for all {num_segments} segments...'''

    response = call_ollama(prompt, cfg.llm_model, cfg.llm_timeout)
    return parse_llm_speaker_output(response, num_segments)


# -------------------------
# Signal 4: Conversation flow priors
# -------------------------

def get_flow_signal(
    seg_idx: int,
    seg: Dict,
    prev_speaker: Optional[str],
    segments: List[Dict],
    cfg: FusionConfig
) -> Dict[str, Any]:
    """Get speaker signal from conversation flow heuristics."""
    text = seg.get("text", "").strip().lower()
    word_count = len(text.split())

    # First segment is typically coach (opener)
    if seg_idx == 0:
        return {"speaker": "COACH", "confidence": 0.9, "source": "flow"}

    # Look at previous segment for context
    prev_text = segments[seg_idx - 1].get("text", "").strip().lower() if seg_idx > 0 else ""

    # "Nice to meet you" pattern - first is COACH, second (echo) is GIRL
    if "nice to meet you" in text:
        if "nice to meet you" in prev_text:
            return {"speaker": "GIRL", "confidence": 0.8, "source": "flow"}  # Echoing
        elif prev_speaker == "GIRL" or seg_idx < 25:  # Early in conversation, coach introduces
            return {"speaker": "COACH", "confidence": 0.7, "source": "flow"}

    # "I'm [name]" introduction pattern - COACH introduces himself
    if re.match(r"^i'm \w+\.?$", text) or re.match(r"^my name is \w+\.?$", text):
        return {"speaker": "COACH", "confidence": 0.85, "source": "flow"}

    # Single word/short echo of a name - GIRL repeating coach's name
    if word_count == 1 and seg_idx > 0:
        # If previous had "I'm [name]" and this is just "[name]", it's echo
        if re.search(r"i'm (\w+)", prev_text):
            return {"speaker": "GIRL", "confidence": 0.85, "source": "flow"}

    # "I'm actually..." or "I'm just here for..." - GIRL explaining
    if text.startswith("i'm actually") or text.startswith("i'm just here"):
        return {"speaker": "GIRL", "confidence": 0.7, "source": "flow"}

    # Short reactions like "Oh, sick", "Oh, okay", "Oh, nice" after GIRL talking
    if re.match(r"^oh[,.]?\s*(sick|okay|ok|nice|cool|wow|really)", text):
        if prev_speaker == "GIRL":
            return {"speaker": "COACH", "confidence": 0.7, "source": "flow"}
        else:
            return {"speaker": "GIRL", "confidence": 0.5, "source": "flow"}

    # Very short responses after coach question â†’ likely girl
    if prev_speaker == "COACH" and word_count <= 3:
        return {"speaker": "GIRL", "confidence": 0.6, "source": "flow"}

    # Questions are typically coach
    if text.endswith("?") and word_count > 3:
        return {"speaker": "COACH", "confidence": 0.5, "source": "flow"}

    # Long explanations (>20 words) are typically coach commentary
    if word_count > 20:
        return {"speaker": "COACH", "confidence": 0.5, "source": "flow"}

    # Alternation prior (weak)
    if prev_speaker == "COACH":
        return {"speaker": "GIRL", "confidence": 0.3, "source": "flow"}
    elif prev_speaker == "GIRL":
        return {"speaker": "COACH", "confidence": 0.3, "source": "flow"}

    return {"speaker": "unknown", "confidence": 0.1, "source": "flow"}


# -------------------------
# Fusion: Weighted voting
# -------------------------

def fuse_signals(signals: List[Dict[str, Any]], cfg: FusionConfig) -> Dict[str, Any]:
    """
    Combine multiple speaker signals using weighted voting.

    signals: list of {"speaker": str, "confidence": float, "source": str}
    """
    weights = {
        "diarization": cfg.weight_diarization,
        "pitch": cfg.weight_pitch,
        "llm": cfg.weight_llm,
        "flow": cfg.weight_flow,
    }

    # Accumulate weighted votes
    votes: Dict[str, float] = defaultdict(float)
    total_weight = 0.0

    for sig in signals:
        speaker = sig.get("speaker", "unknown")
        confidence = sig.get("confidence", 0.0)
        source = sig.get("source", "unknown")
        weight = weights.get(source, 0.1)

        if speaker != "unknown":
            votes[speaker] += weight * confidence
            total_weight += weight * confidence

    if not votes or total_weight == 0:
        return {
            "label": "unknown",
            "confidence": 0.0,
            "signals": signals,
        }

    # Find winner
    best_speaker = max(votes, key=votes.get)
    best_score = votes[best_speaker]
    confidence = best_score / total_weight if total_weight > 0 else 0.0

    return {
        "label": best_speaker,
        "confidence": round(confidence, 3),
        "signals": signals,
        "votes": dict(votes),
    }


# -------------------------
# Main processing
# -------------------------

def process_video(
    transcript_path: Path,
    audio_features_path: Optional[Path],
    output_path: Path,
    cfg: FusionConfig,
) -> Dict[str, Any]:
    """Process a single video through speaker fusion."""

    # Load diarization transcript
    segments, diarization_mapping = load_diarization_transcript(transcript_path)
    if not segments:
        return {"error": "No segments found"}

    # Load audio features (optional)
    audio_features = {}
    if audio_features_path and audio_features_path.exists():
        audio_features = load_audio_features(audio_features_path)

    # Get LLM signals in batches
    print(f"  [fusion] Running LLM inference on {len(segments)} segments...")
    llm_signals = {}
    for batch_start in range(0, len(segments), cfg.llm_batch_size):
        batch_end = min(batch_start + cfg.llm_batch_size, len(segments))
        batch = segments[batch_start:batch_end]

        # Create indexed segments for this batch
        batch_results = get_llm_signals_batch(batch, cfg)

        # Map back to global indices
        for local_idx, signal in batch_results.items():
            global_idx = batch_start + local_idx
            llm_signals[global_idx] = signal

    # Process each segment
    output_segments = []
    prev_speaker = None

    for i, seg in enumerate(segments):
        # Collect all signals
        signals = []

        # Signal 1: Diarization
        diar_signal = get_diarization_signal(seg, diarization_mapping, cfg)
        signals.append(diar_signal)

        # Signal 2: Pitch
        pitch_signal = get_pitch_signal(seg, audio_features, cfg)
        signals.append(pitch_signal)

        # Signal 3: LLM
        if i in llm_signals:
            signals.append(llm_signals[i])
        else:
            signals.append({"speaker": "unknown", "confidence": 0.0, "source": "llm"})

        # Signal 4: Flow
        flow_signal = get_flow_signal(i, seg, prev_speaker, segments, cfg)
        signals.append(flow_signal)

        # Fuse signals
        result = fuse_signals(signals, cfg)

        # Update previous speaker
        if result["label"] in ["COACH", "GIRL"]:
            prev_speaker = result["label"]

        # Build output segment
        output_segments.append({
            "start": seg.get("start"),
            "end": seg.get("end"),
            "text": seg.get("text", "").strip(),
            "original_speaker": seg.get("speaker", ""),
            "speaker": result["label"],
            "confidence": result["confidence"],
            "signals": result.get("signals", []),
        })

    # Write output
    output = {
        "source_transcript": str(transcript_path),
        "source_audio_features": str(audio_features_path) if audio_features_path else None,
        "diarization_mapping": diarization_mapping,
        "config": {
            "weight_diarization": cfg.weight_diarization,
            "weight_pitch": cfg.weight_pitch,
            "weight_llm": cfg.weight_llm,
            "weight_flow": cfg.weight_flow,
        },
        "segments": output_segments,
    }

    output_path.parent.mkdir(parents=True, exist_ok=True)
    output_path.write_text(json.dumps(output, indent=2, ensure_ascii=False), encoding="utf-8")

    return output


# -------------------------
# Accuracy Testing
# -------------------------

# Ground truth for "Daygame Is This Simple!?" video (first 30 segments)
SPEAKER_GROUND_TRUTH = {
    0: "COACH",   # "Hey, how's we going?" - opener
    1: "GIRL",    # "Hello." - response
    2: "COACH",   # "Do you work at the La Vaca Street Bar?" - question
    3: "GIRL",    # "No." - answer
    4: "COACH",   # "Oh, you don't?" - follow-up
    5: "GIRL",    # "No." - answer
    6: "COACH",   # "Okay." - acknowledgment
    7: "COACH",   # "I saw you, I was like..." - longer explanation
    8: "GIRL",    # "No, I've always wanted to be a bartender" - answer
    9: "COACH",   # "What are you doing instead?" - question
    10: "COACH",  # "Are you like a dancer or something?" - question
    11: "COACH",  # "No, I'm just playing." - coach continues
    12: "GIRL",   # "I used to be though." - girl responds
    13: "COACH",  # "Hey, no shame." - coach response
    14: "GIRL",   # "No, like ballet." - clarification
    15: "COACH",  # "Oh, okay." - acknowledgment
    16: "COACH",  # "That kind of does." - coach continues
    17: "COACH",  # "Okay, nice." - acknowledgment
    18: "COACH",  # "Okay, nice." - continues
    19: "COACH",  # "I'm Erin." - introduction
    20: "GIRL",   # "Erin." - repeats name
    21: "COACH",  # "Nice to meet you." - pleasantry
    22: "GIRL",   # "Nice to meet you." - response
    23: "GIRL",   # "No, I don't." - answer
    24: "GIRL",   # "I'm actually just here for ACO." - explains
    25: "COACH",  # Acknowledgment
    26: "GIRL",   # About Austin
    27: "COACH",  # Question
    28: "GIRL",   # "Laredo" - answer
    29: "COACH",  # Response
}


def test_accuracy(output_path: Path) -> Dict[str, Any]:
    """Test speaker accuracy against ground truth."""
    if not output_path.exists():
        return {"error": f"Output file not found: {output_path}"}

    data = json.loads(output_path.read_text(encoding="utf-8"))
    segments = data.get("segments", [])

    correct = 0
    errors = []

    for i, expected in SPEAKER_GROUND_TRUTH.items():
        if i >= len(segments):
            errors.append({"segment": i, "expected": expected, "got": "MISSING"})
            continue

        got = segments[i].get("speaker", "unknown")
        if got == expected:
            correct += 1
        else:
            errors.append({
                "segment": i,
                "expected": expected,
                "got": got,
                "text": segments[i].get("text", "")[:50],
                "confidence": segments[i].get("confidence", 0),
            })

    total = len(SPEAKER_GROUND_TRUTH)
    accuracy = (correct / total * 100) if total > 0 else 0

    return {
        "correct": correct,
        "total": total,
        "accuracy": round(accuracy, 1),
        "errors": errors,
    }


# -------------------------
# CLI
# -------------------------

def main():
    parser = argparse.ArgumentParser(description="Speaker fusion for daygame videos")
    parser.add_argument("source_name", nargs="?", help="Source name")
    parser.add_argument("youtube_url", nargs="?", help="YouTube URL")
    parser.add_argument("--test", action="store_true", help="Run accuracy test only")
    parser.add_argument("--model", default="llama3.1", help="LLM model (default: llama3.1)")
    parser.add_argument("--overwrite", action="store_true", help="Overwrite existing outputs")

    # Weight tuning
    parser.add_argument("--w-diar", type=float, default=0.25, help="Weight for diarization")
    parser.add_argument("--w-pitch", type=float, default=0.35, help="Weight for pitch")
    parser.add_argument("--w-llm", type=float, default=0.30, help="Weight for LLM")
    parser.add_argument("--w-flow", type=float, default=0.10, help="Weight for flow")

    args = parser.parse_args()

    cfg = FusionConfig(
        weight_diarization=args.w_diar,
        weight_pitch=args.w_pitch,
        weight_llm=args.w_llm,
        weight_flow=args.w_flow,
        llm_model=args.model,
    )

    root = repo_root()

    # Test mode
    if args.test:
        print("[fusion] Running accuracy test...")

        # Find test video
        test_video = "Daygame Is This Simple!ï¼Ÿ ðŸ¥³ (+Daygame Infield) [utuuVOXJunM]"
        test_source = "daily_evolution"

        transcript_path = root / "data" / "02.transcribe" / test_source / test_video / f"{test_video}.full.whisperx.json"
        audio_features_path = root / "data" / "04.audio-features" / test_source / test_video / f"{test_video}.full.audio_features.json"
        output_path = root / "data" / "05.speaker-fusion" / test_source / test_video / f"{test_video}.speakers.json"

        if not transcript_path.exists():
            print(f"[fusion] ERROR: Transcript not found: {transcript_path}")
            return 1

        # Process if needed
        if not output_path.exists() or args.overwrite:
            print(f"[fusion] Processing: {test_video}")
            process_video(transcript_path, audio_features_path, output_path, cfg)

        # Test accuracy
        result = test_accuracy(output_path)

        print("\n" + "=" * 60)
        print("SPEAKER FUSION ACCURACY TEST")
        print("=" * 60)
        print(f"Correct: {result['correct']}/{result['total']}")
        print(f"Accuracy: {result['accuracy']}%")
        print(f"Target: â‰¥95%")

        if result['accuracy'] >= 95:
            print("\nâœ… TARGET MET!")
        else:
            print(f"\nðŸ”´ BELOW TARGET (gap: {95 - result['accuracy']:.1f}%)")

        if result.get("errors"):
            print("\nErrors:")
            for e in result["errors"][:10]:
                print(f"  [{e['segment']}] Expected {e['expected']}, got {e['got']}: '{e.get('text', '')}'")

        return 0

    # Normal mode - process source
    if not args.source_name or not args.youtube_url:
        parser.print_help()
        return 1

    video_id = extract_video_id(args.youtube_url)

    # Find transcript files
    transcribe_dir = root / "data" / "02.transcribe" / args.source_name
    if not transcribe_dir.exists():
        print(f"[fusion] ERROR: Transcribe directory not found: {transcribe_dir}")
        return 1

    # Find whisperx files
    whisperx_files = sorted(transcribe_dir.rglob("*.whisperx.json"))
    if video_id:
        whisperx_files = [f for f in whisperx_files if f"[{video_id}]" in f.as_posix()]

    if not whisperx_files:
        print(f"[fusion] ERROR: No whisperx files found")
        return 1

    for transcript_path in whisperx_files:
        rel = transcript_path.relative_to(transcribe_dir)
        video_name = rel.parent.name
        stem = transcript_path.stem.replace(".full.whisperx", ".full")

        # Find audio features (optional)
        audio_features_path = root / "data" / "04.audio-features" / args.source_name / video_name / f"{stem}.audio_features.json"

        # Output path
        output_path = root / "data" / "05.speaker-fusion" / args.source_name / video_name / f"{stem}.speakers.json"

        if output_path.exists() and not args.overwrite:
            print(f"[fusion] SKIP (exists): {video_name}")
            continue

        print(f"[fusion] Processing: {video_name}")
        process_video(transcript_path, audio_features_path, output_path, cfg)

    return 0


if __name__ == "__main__":
    sys.exit(main())
