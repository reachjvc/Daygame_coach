#!/usr/bin/env python3
"""
scripts/training-data/06g.LLM.damage-adjudicator

Targeted LLM adjudicator over Stage 06f damage-map seeds.

Only runs for infield/compilation videos by default.
Non-infield videos (talking_head, podcast, lecture) are skipped with a sentinel
output file (skipped=true). Override with --skip-video-type-filter.

Reads:
  - data/06f.DET.damage-map/<source>/<video>/*.damage-map.json
  - data/06d.DET.sanitized/<source>/<video>/*.conversations.json (context + video_type)

Writes:
  - data/06g.LLM.damage-adjudicator/<source>/<video>/*.damage-adjudication.json
    (full adjudication for infield/compilation, or skip sentinel for other types)
"""

from __future__ import annotations

import argparse
import json
import re
import shlex
import subprocess
import sys
import time
from pathlib import Path
from typing import Any, Dict, List, Optional, Set, Tuple

from batch.manifest_parser import load_manifest_sources, manifest_filter_files

try:
    import jsonschema  # type: ignore
except Exception:  # pragma: no cover
    jsonschema = None


LOG_PREFIX = "[06g.LLM.damage-adjudicator]"
PIPELINE_VERSION = "06g.LLM.damage-adjudicator-v1.2"

# Video types that contain infield conversations worth adjudicating.
# Non-infield types (talking_head, podcast, lecture) have no conversation seeds
# and are skipped unless --skip-video-type-filter is used.
INFIELD_VIDEO_TYPES = {"infield", "compilation"}
PROMPT_PATH = Path(__file__).resolve().parent / "prompts" / "06g.damage-adjudicator.prompt.md"
SCHEMA_PATH = Path(__file__).resolve().parent / "prompts" / "06g.damage-adjudicator.schema.json"
BATCH_PROMPT_PATH = Path(__file__).resolve().parent / "prompts" / "06g.damage-adjudicator.batch.prompt.md"
BATCH_SCHEMA_PATH = Path(__file__).resolve().parent / "prompts" / "06g.damage-adjudicator.batch.schema.json"
CLAUDE_BINARY_PATHS = [
    "claude",
    Path.home() / ".vscode-server/extensions/anthropic.claude-code-2.1.17-linux-x64/resources/native-binary/claude",
    Path.home() / ".vscode/extensions/anthropic.claude-code-2.1.17-linux-x64/resources/native-binary/claude",
    "/usr/local/bin/claude",
]


def repo_root() -> Path:
    return Path(__file__).resolve().parents[2]


def input_root() -> Path:
    return repo_root() / "data" / "06f.DET.damage-map"


def context_root() -> Path:
    return repo_root() / "data" / "06d.DET.sanitized"


def output_root() -> Path:
    return repo_root() / "data" / "06g.LLM.damage-adjudicator"


def test_input_root() -> Path:
    return repo_root() / "data" / "test" / "06f.DET.damage-map"


def test_context_root() -> Path:
    return repo_root() / "data" / "test" / "06d.DET.sanitized"


def test_output_root() -> Path:
    return repo_root() / "data" / "test" / "06g.LLM.damage-adjudicator"


def resolve_root_path(raw_path: Optional[str], default_root: Path) -> Path:
    if not raw_path:
        return default_root
    path = Path(raw_path)
    if not path.is_absolute():
        path = repo_root() / path
    return path


def parse_sources_file(path: Path) -> List[Tuple[str, str]]:
    sources: List[Tuple[str, str]] = []
    for raw in path.read_text(encoding="utf-8").splitlines():
        line = raw.strip()
        if not line or line.startswith("#"):
            continue
        if "|" in line:
            name, url = line.split("|", 1)
            sources.append((name.strip(), url.strip()))
            continue
        parts = shlex.split(line)
        if len(parts) >= 2:
            sources.append((parts[0], parts[1]))
    return sources


def find_input_files(in_dir: Path) -> List[Path]:
    return sorted(in_dir.rglob("*.damage-map.json"))


def compute_output_path(input_path: Path, output_dir: Path) -> Path:
    base = input_path.name
    if base.endswith(".damage-map.json"):
        base = base[: -len(".damage-map.json")]
    else:
        base = input_path.stem
    return output_dir / f"{base}.damage-adjudication.json"


def compute_output_path_with_layout(
    input_path: Path,
    output_dir: Path,
    input_root_dir: Optional[Path] = None,
) -> Path:
    canonical = compute_output_path(input_path, output_dir)
    if input_root_dir is not None:
        try:
            rel_parent = input_path.parent.relative_to(input_root_dir)
            if rel_parent != Path("."):
                return output_dir / rel_parent / canonical.name
        except ValueError:
            pass
    return canonical


def find_existing_output_path(
    input_path: Path,
    preferred_output_dir: Path,
    output_root_dir: Optional[Path] = None,
    input_root_dir: Optional[Path] = None,
) -> Optional[Path]:
    preferred = compute_output_path_with_layout(
        input_path,
        preferred_output_dir,
        input_root_dir=input_root_dir,
    )
    if preferred.exists():
        return preferred
    source_flat = compute_output_path(input_path, preferred_output_dir)
    if source_flat.exists():
        return source_flat
    if output_root_dir is not None:
        root_flat = compute_output_path(input_path, output_root_dir)
        if root_flat.exists():
            return root_flat
    return None


def _safe_int(value: Any) -> Optional[int]:
    if isinstance(value, bool):
        return None
    if isinstance(value, int):
        return value
    if isinstance(value, float) and value.is_integer():
        return int(value)
    return None


def _read_json(path: Path) -> Optional[Dict[str, Any]]:
    try:
        data = json.loads(path.read_text(encoding="utf-8"))
    except Exception:
        return None
    return data if isinstance(data, dict) else None


def _extract_video_id_from_name(name: str) -> Optional[str]:
    m = re.search(r"\[([A-Za-z0-9_-]{11})\]", name)
    return m.group(1) if m else None


def _get_video_type(data: Dict[str, Any]) -> str:
    """Extract video type string from 06d context data."""
    vt = data.get("video_type", {})
    if isinstance(vt, dict):
        return vt.get("type", "unknown")
    if isinstance(vt, str):
        return vt
    return "unknown"


def _load_prompt_template() -> str:
    if not PROMPT_PATH.exists():
        raise RuntimeError(f"Prompt template missing: {PROMPT_PATH}")
    return PROMPT_PATH.read_text(encoding="utf-8")


def _load_schema() -> Optional[Dict[str, Any]]:
    if not SCHEMA_PATH.exists():
        return None
    try:
        schema = json.loads(SCHEMA_PATH.read_text(encoding="utf-8"))
    except Exception:
        return None
    return schema if isinstance(schema, dict) else None


def _find_claude_binary() -> Optional[str]:
    for cand in CLAUDE_BINARY_PATHS:
        p = Path(cand)
        if str(cand) == "claude":
            try:
                res = subprocess.run(["which", "claude"], capture_output=True, text=True)
            except Exception:
                res = None
            if res and res.returncode == 0:
                return "claude"
            continue
        if p.exists() and p.is_file():
            return str(p)
    return None


def _call_claude(
    prompt: str,
    *,
    model: Optional[str],
    timeout_seconds: int,
    retries: int,
) -> Optional[str]:
    claude_bin = _find_claude_binary()
    if not claude_bin:
        print(f"{LOG_PREFIX} ERROR: Claude CLI binary not found")
        return None

    for attempt in range(max(1, retries)):
        try:
            cmd = [claude_bin]
            if isinstance(model, str) and model.strip():
                cmd += ["--model", model.strip()]
            cmd += ["-p", prompt, "--output-format", "text"]
            res = subprocess.run(
                cmd,
                capture_output=True,
                text=True,
                timeout=max(1, timeout_seconds),
            )
            if res.returncode == 0:
                return (res.stdout or "").strip()
            if attempt < retries - 1:
                wait = 2 ** attempt
                print(f"{LOG_PREFIX} Claude CLI non-zero exit, retrying in {wait}s...")
                time.sleep(wait)
        except subprocess.TimeoutExpired:
            if attempt < retries - 1:
                wait = 2 ** attempt
                print(f"{LOG_PREFIX} Claude CLI timeout, retrying in {wait}s...")
                time.sleep(wait)
    return None


def _extract_json(text: str) -> Optional[Dict[str, Any]]:
    raw = text.strip()
    if not raw:
        return None

    # Strip fenced blocks.
    if raw.startswith("```"):
        raw = re.sub(r"^```(?:json)?\s*", "", raw, flags=re.IGNORECASE)
        raw = re.sub(r"\s*```$", "", raw)

    try:
        parsed = json.loads(raw)
        if isinstance(parsed, dict):
            return parsed
    except Exception:
        pass

    start = raw.find("{")
    end = raw.rfind("}")
    if start >= 0 and end > start:
        snippet = raw[start : end + 1]
        try:
            parsed = json.loads(snippet)
            if isinstance(parsed, dict):
                return parsed
        except Exception:
            return None
    return None


def _validate_schema(payload: Dict[str, Any], schema: Optional[Dict[str, Any]]) -> bool:
    if schema is None or jsonschema is None:
        return True
    try:
        jsonschema.validate(instance=payload, schema=schema)
        return True
    except Exception:
        return False


def _normalize_adjudication(payload: Dict[str, Any]) -> Dict[str, Any]:
    out = dict(payload)
    for key in ("transcript_confidence", "speaker_confidence", "phase_confidence"):
        val = out.get(key)
        if isinstance(val, (int, float)):
            out[key] = round(float(val), 4)
    rationale = out.get("confidence_rationale")
    if isinstance(rationale, str):
        out["confidence_rationale"] = rationale.strip()
    repaired = out.get("repaired_text")
    if isinstance(repaired, str):
        txt = repaired.strip()
        out["repaired_text"] = txt if txt else None
    return out


def _seed_fallback(seg_id: int, severity: str) -> Dict[str, Any]:
    base = {
        "transcript_confidence": 0.35,
        "speaker_confidence": 0.35,
        "phase_confidence": 0.35,
        "repair_possible": False,
        "repaired_text": None,
        "contamination_start_segment_id": seg_id,
        "contamination_end_segment_id": seg_id,
        "confidence_rationale": "fallback_without_llm_output",
    }
    if severity == "high":
        base["transcript_confidence"] = 0.20
        base["speaker_confidence"] = 0.25
        base["phase_confidence"] = 0.20
    return base


def _build_context_window(
    *,
    context_data: Dict[str, Any],
    seed_segment_id: int,
    conversation_id: int,
    left: int,
    right: int,
) -> List[Dict[str, Any]]:
    segments = [
        seg for seg in (context_data.get("segments") or [])
        if isinstance(seg, dict) and isinstance(_safe_int(seg.get("id")), int)
    ]
    if conversation_id > 0:
        conv_only = [
            seg for seg in segments
            if (_safe_int(seg.get("conversation_id")) or 0) == conversation_id
        ]
        if conv_only:
            segments = conv_only
    segments.sort(key=lambda s: _safe_int(s.get("id")) or 0)
    ids = [_safe_int(seg.get("id")) for seg in segments]
    ids = [sid for sid in ids if isinstance(sid, int)]
    if seed_segment_id not in ids:
        return []
    idx = ids.index(seed_segment_id)
    start = max(0, idx - left)
    end = min(len(segments), idx + right + 1)
    window = segments[start:end]
    out: List[Dict[str, Any]] = []
    for seg in window:
        out.append(
            {
                "id": _safe_int(seg.get("id")),
                "conversation_id": _safe_int(seg.get("conversation_id")) or 0,
                "start": seg.get("start"),
                "end": seg.get("end"),
                "speaker_id": seg.get("speaker_id"),
                "speaker_role": seg.get("speaker_role"),
                "segment_type": seg.get("segment_type"),
                "segment_flags": seg.get("segment_flags") if isinstance(seg.get("segment_flags"), list) else [],
                "text": seg.get("text", ""),
            }
        )
    return out


def _build_batch_context_window(
    *,
    context_data: Dict[str, Any],
    seed_segment_ids: List[int],
    conversation_id: int,
    left: int,
    right: int,
) -> List[Dict[str, Any]]:
    """Build a context window spanning all seeds in a batch."""
    segments = [
        seg for seg in (context_data.get("segments") or [])
        if isinstance(seg, dict) and isinstance(_safe_int(seg.get("id")), int)
    ]
    if conversation_id > 0:
        conv_only = [
            seg for seg in segments
            if (_safe_int(seg.get("conversation_id")) or 0) == conversation_id
        ]
        if conv_only:
            segments = conv_only
    segments.sort(key=lambda s: _safe_int(s.get("id")) or 0)
    ids = [_safe_int(seg.get("id")) for seg in segments]
    ids = [sid for sid in ids if isinstance(sid, int)]
    seed_indices = [ids.index(sid) for sid in seed_segment_ids if sid in ids]
    if not seed_indices:
        return []
    start = max(0, min(seed_indices) - left)
    end = min(len(segments), max(seed_indices) + right + 1)
    window = segments[start:end]
    out: List[Dict[str, Any]] = []
    for seg in window:
        out.append(
            {
                "id": _safe_int(seg.get("id")),
                "conversation_id": _safe_int(seg.get("conversation_id")) or 0,
                "start": seg.get("start"),
                "end": seg.get("end"),
                "speaker_id": seg.get("speaker_id"),
                "speaker_role": seg.get("speaker_role"),
                "segment_type": seg.get("segment_type"),
                "segment_flags": seg.get("segment_flags") if isinstance(seg.get("segment_flags"), list) else [],
                "text": seg.get("text", ""),
            }
        )
    return out


def _group_seeds_into_batches(
    seed_rows: List[Dict[str, Any]],
    batch_size: int,
) -> List[List[Dict[str, Any]]]:
    """Group seeds by conversation_id, then chunk into batches of batch_size."""
    from collections import defaultdict

    by_conv: Dict[int, List[Dict[str, Any]]] = defaultdict(list)
    for seed in seed_rows:
        conv_id = _safe_int(seed.get("conversation_id")) or 0
        by_conv[conv_id].append(seed)

    for conv_id in by_conv:
        by_conv[conv_id].sort(key=lambda s: _safe_int(s.get("segment_id")) or 0)

    batches: List[List[Dict[str, Any]]] = []
    for conv_id in sorted(by_conv.keys()):
        conv_seeds = by_conv[conv_id]
        for i in range(0, len(conv_seeds), batch_size):
            batches.append(conv_seeds[i : i + batch_size])
    return batches


def _resolve_context_path(
    damage_map_path: Path,
    damage_map_data: Dict[str, Any],
    *,
    input_root_dir: Path,
    context_root_dir: Path,
) -> Optional[Path]:
    # Preferred: explicit source path embedded by 06f.
    inp = damage_map_data.get("input")
    if isinstance(inp, dict):
        s06d = inp.get("stage06d")
        if isinstance(s06d, str) and s06d.strip():
            p = Path(s06d.strip())
            if p.exists():
                return p
            p2 = repo_root() / s06d.strip()
            if p2.exists():
                return p2

    # Fallback: mirror relative path from 06f root to 06d root.
    try:
        rel = damage_map_path.relative_to(input_root_dir)
        rel_name = rel.name.replace(".damage-map.json", ".conversations.json")
        candidate = context_root_dir / rel.parent / rel_name
        if candidate.exists():
            return candidate
    except Exception:
        pass

    # Final fallback: video_id search.
    video_id = damage_map_data.get("video_id")
    if isinstance(video_id, str) and video_id.strip():
        matches: List[Path] = []
        for p in context_root_dir.rglob("*.conversations.json"):
            if video_id in p.name:
                matches.append(p)
        if matches:
            return sorted(matches, key=lambda p: str(p))[0]
    return None


def _bounded_contamination(adjudication: Dict[str, Any], context_ids: List[int], seed_id: int) -> None:
    if not context_ids:
        adjudication["contamination_start_segment_id"] = seed_id
        adjudication["contamination_end_segment_id"] = seed_id
        return
    lo = min(context_ids)
    hi = max(context_ids)
    start = _safe_int(adjudication.get("contamination_start_segment_id"))
    end = _safe_int(adjudication.get("contamination_end_segment_id"))
    if start is None:
        start = seed_id
    if end is None:
        end = seed_id
    start = max(lo, min(hi, start))
    end = max(lo, min(hi, end))
    if end < start:
        start, end = end, start
    adjudication["contamination_start_segment_id"] = start
    adjudication["contamination_end_segment_id"] = end


def _mean(vals: List[float]) -> float:
    if not vals:
        return 0.0
    return sum(vals) / len(vals)


def _process_single_adjudication(
    adjudication: Dict[str, Any],
    seed: Dict[str, Any],
    context_ids: List[int],
    args: argparse.Namespace,
) -> Dict[str, Any]:
    """Post-process a single adjudication result into an output row."""
    sid = _safe_int(seed.get("segment_id"))
    conv_id = _safe_int(seed.get("conversation_id")) or 0
    sev = str(seed.get("severity", "low")).strip().lower() or "low"

    _bounded_contamination(adjudication, context_ids, sid)

    transcript_conf = float(adjudication.get("transcript_confidence", 0.0))
    speaker_conf = float(adjudication.get("speaker_confidence", 0.0))
    phase_conf = float(adjudication.get("phase_confidence", 0.0))
    overall_conf = _mean([transcript_conf, speaker_conf, phase_conf])

    repair_possible = bool(adjudication.get("repair_possible"))
    repaired_text = adjudication.get("repaired_text")
    accepted_repair = bool(
        repair_possible
        and isinstance(repaired_text, str)
        and repaired_text.strip()
        and transcript_conf >= args.repair_accept_threshold
    )

    allow_anchor = overall_conf >= args.anchor_allow_threshold

    return {
        "seed_segment_id": sid,
        "conversation_id": conv_id,
        "damage_types": seed.get("damage_types", []),
        "damage_reason_codes": seed.get("damage_reason_codes", []),
        "severity": sev,
        "adjudication": adjudication,
        "overall_confidence": round(overall_conf, 4),
        "repair_accepted": accepted_repair,
        "anchor_allowed": allow_anchor,
        "determinism_match": True,
    }


def _adjudicate_single_seed(
    seed: Dict[str, Any],
    *,
    context_data: Dict[str, Any],
    args: argparse.Namespace,
    prompt_template: str,
    schema: Optional[Dict[str, Any]],
) -> Tuple[Dict[str, Any], bool]:
    """Adjudicate a single seed (original per-seed path). Returns (row, llm_failed)."""
    sid = _safe_int(seed.get("segment_id"))
    conv_id = _safe_int(seed.get("conversation_id")) or 0
    sev = str(seed.get("severity", "low")).strip().lower() or "low"
    context_window = _build_context_window(
        context_data=context_data,
        seed_segment_id=sid,
        conversation_id=conv_id,
        left=args.context_left,
        right=args.context_right,
    )
    context_ids = [x for x in (_safe_int(r.get("id")) for r in context_window) if isinstance(x, int)]

    seed_payload = {
        "segment_id": sid,
        "conversation_id": conv_id,
        "severity": sev,
        "damage_types": seed.get("damage_types", []),
        "damage_reason_codes": seed.get("damage_reason_codes", []),
        "contamination_window_hint": seed.get("contamination_window"),
    }
    prompt = (
        prompt_template
        .replace("{{SEED_JSON}}", json.dumps(seed_payload, ensure_ascii=False))
        .replace("{{CONTEXT_JSON}}", json.dumps(context_window, ensure_ascii=False))
    )

    normalized_runs: List[Dict[str, Any]] = []
    raw_runs: List[Optional[str]] = []
    call_count = max(1, int(args.determinism_n))
    if args.dry_run:
        call_count = 1

    for _ in range(call_count):
        if args.dry_run:
            payload = _seed_fallback(sid, sev)
            normalized_runs.append(payload)
            raw_runs.append(None)
            continue
        raw = _call_claude(
            prompt,
            model=args.model,
            timeout_seconds=args.timeout_seconds,
            retries=args.retries,
        )
        raw_runs.append(raw)
        parsed = _extract_json(raw or "") if raw else None
        if not isinstance(parsed, dict) or not _validate_schema(parsed, schema):
            parsed = _seed_fallback(sid, sev)
            parsed["confidence_rationale"] = "fallback_schema_or_parse_failure"
        parsed = _normalize_adjudication(parsed)
        normalized_runs.append(parsed)

    adjudication = normalized_runs[0] if normalized_runs else _seed_fallback(sid, sev)
    deterministic_match = True
    if len(normalized_runs) > 1:
        first = json.dumps(normalized_runs[0], sort_keys=True, ensure_ascii=False)
        for other in normalized_runs[1:]:
            if json.dumps(other, sort_keys=True, ensure_ascii=False) != first:
                deterministic_match = False
                break

    row = _process_single_adjudication(adjudication, seed, context_ids, args)
    row["determinism_match"] = deterministic_match
    if args.include_raw_llm and raw_runs:
        row["raw_llm_output"] = raw_runs[0]

    llm_failed = not args.dry_run and (raw_runs[0] is None)
    return row, llm_failed


def _adjudicate_batch(
    batch: List[Dict[str, Any]],
    *,
    context_data: Dict[str, Any],
    args: argparse.Namespace,
    batch_prompt_template: str,
    batch_schema: Optional[Dict[str, Any]],
) -> Tuple[List[Dict[str, Any]], int]:
    """Adjudicate a batch of seeds in a single LLM call. Returns (rows, llm_failures)."""
    conv_id = _safe_int(batch[0].get("conversation_id")) or 0
    seed_ids = []
    for s in batch:
        sid = _safe_int(s.get("segment_id"))
        if sid is not None:
            seed_ids.append(sid)

    context_window = _build_batch_context_window(
        context_data=context_data,
        seed_segment_ids=seed_ids,
        conversation_id=conv_id,
        left=args.context_left,
        right=args.context_right,
    )
    context_ids = [x for x in (_safe_int(r.get("id")) for r in context_window) if isinstance(x, int)]

    seeds_payload = []
    for seed in batch:
        sid = _safe_int(seed.get("segment_id"))
        sev = str(seed.get("severity", "low")).strip().lower() or "low"
        seeds_payload.append({
            "segment_id": sid,
            "conversation_id": conv_id,
            "severity": sev,
            "damage_types": seed.get("damage_types", []),
            "damage_reason_codes": seed.get("damage_reason_codes", []),
            "contamination_window_hint": seed.get("contamination_window"),
        })

    prompt = (
        batch_prompt_template
        .replace("{{SEEDS_JSON}}", json.dumps(seeds_payload, ensure_ascii=False))
        .replace("{{CONTEXT_JSON}}", json.dumps(context_window, ensure_ascii=False))
    )

    rows: List[Dict[str, Any]] = []
    llm_failures = 0

    if args.dry_run:
        for seed in batch:
            sid = _safe_int(seed.get("segment_id"))
            sev = str(seed.get("severity", "low")).strip().lower() or "low"
            fb = _seed_fallback(sid, sev)
            row = _process_single_adjudication(fb, seed, context_ids, args)
            rows.append(row)
        return rows, 0

    raw = _call_claude(
        prompt,
        model=args.model,
        timeout_seconds=args.timeout_seconds,
        retries=args.retries,
    )

    if not raw:
        print(f"{LOG_PREFIX} WARN: Batch LLM call failed for {len(batch)} seeds, using fallbacks")
        llm_failures = len(batch)
        for seed in batch:
            sid = _safe_int(seed.get("segment_id"))
            sev = str(seed.get("severity", "low")).strip().lower() or "low"
            fb = _seed_fallback(sid, sev)
            fb["confidence_rationale"] = "fallback_batch_llm_failure"
            row = _process_single_adjudication(fb, seed, context_ids, args)
            rows.append(row)
        return rows, llm_failures

    parsed = _extract_json(raw) if raw else None
    if not isinstance(parsed, dict) or not _validate_schema(parsed, batch_schema):
        print(f"{LOG_PREFIX} WARN: Batch response failed schema validation, using fallbacks")
        llm_failures = len(batch)
        for seed in batch:
            sid = _safe_int(seed.get("segment_id"))
            sev = str(seed.get("severity", "low")).strip().lower() or "low"
            fb = _seed_fallback(sid, sev)
            fb["confidence_rationale"] = "fallback_batch_schema_failure"
            row = _process_single_adjudication(fb, seed, context_ids, args)
            rows.append(row)
        return rows, llm_failures

    adj_list = parsed.get("adjudications", [])
    if not isinstance(adj_list, list):
        adj_list = []

    # Index LLM results by segment_id for matching
    adj_by_id: Dict[int, Dict[str, Any]] = {}
    for adj in adj_list:
        if isinstance(adj, dict):
            adj_sid = _safe_int(adj.get("segment_id"))
            if adj_sid is not None:
                adj_by_id[adj_sid] = adj

    for seed in batch:
        sid = _safe_int(seed.get("segment_id"))
        sev = str(seed.get("severity", "low")).strip().lower() or "low"
        matched = adj_by_id.get(sid)
        if matched is not None:
            adjudication = _normalize_adjudication(matched)
        else:
            print(f"{LOG_PREFIX} WARN: Batch response missing segment_id={sid}, using fallback")
            adjudication = _seed_fallback(sid, sev)
            adjudication["confidence_rationale"] = "fallback_batch_missing_segment"
            llm_failures += 1
        row = _process_single_adjudication(adjudication, seed, context_ids, args)
        rows.append(row)

    return rows, llm_failures


def adjudicate_file(
    input_path: Path,
    output_path: Path,
    *,
    args: argparse.Namespace,
    input_root_dir: Path,
    context_root_dir: Path,
    prompt_template: str,
    schema: Optional[Dict[str, Any]],
    batch_prompt_template: Optional[str] = None,
    batch_schema: Optional[Dict[str, Any]] = None,
) -> Dict[str, int]:
    damage_map = _read_json(input_path)
    if not damage_map:
        raise RuntimeError(f"Could not read 06f damage-map JSON: {input_path}")

    context_path = _resolve_context_path(
        input_path,
        damage_map,
        input_root_dir=input_root_dir,
        context_root_dir=context_root_dir,
    )
    if not context_path:
        raise RuntimeError("Could not resolve matching 06d context path")
    context_data = _read_json(context_path)
    if not context_data:
        raise RuntimeError(f"Could not read 06d context JSON: {context_path}")

    # R2: Skip non-infield videos â€” they have no conversation seeds to adjudicate.
    video_type = _get_video_type(context_data)
    if video_type not in INFIELD_VIDEO_TYPES and not args.skip_video_type_filter:
        skip_output = {
            "video_id": damage_map.get("video_id"),
            "generated_at": time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime()),
            "pipeline_version": PIPELINE_VERSION,
            "source_file": str(input_path),
            "damage_map_file": str(input_path),
            "context_file": str(context_path),
            "skipped": True,
            "skip_reason": f"non_infield_video_type:{video_type}",
            "video_type": video_type,
            "model": args.model,
            "summary": {
                "seeds_total": 0,
                "llm_calls": 0,
                "repairs_accepted": 0,
                "anchor_allowed": 0,
                "llm_failures": 0,
                "determinism_mismatches": 0,
                "contamination_spans": 0,
            },
            "seeds": [],
        }
        if not args.dry_run:
            output_path.parent.mkdir(parents=True, exist_ok=True)
            output_path.write_text(
                json.dumps(skip_output, indent=2, ensure_ascii=False) + "\n",
                encoding="utf-8",
            )
        print(
            f"{LOG_PREFIX} SKIP {input_path.name}: video_type={video_type} "
            f"(not in {sorted(INFIELD_VIDEO_TYPES)})"
        )
        return {
            "seeds_total": 0,
            "llm_calls": 0,
            "repairs_accepted": 0,
            "determinism_mismatches": 0,
            "llm_failures": 0,
        }

    seeds = damage_map.get("segments")
    if not isinstance(seeds, list):
        raise RuntimeError("Invalid damage-map: missing segments[]")

    seed_rows: List[Dict[str, Any]] = []
    for row in seeds:
        if not isinstance(row, dict):
            continue
        sid = _safe_int(row.get("segment_id"))
        if sid is None:
            continue
        sev = str(row.get("severity", "low")).strip().lower()
        if sev not in {"high", "medium", "low"}:
            sev = "low"
        if sev == "low" and not args.include_low_severity:
            continue
        seed_rows.append(row)

    adjudications: List[Dict[str, Any]] = []
    repairs_accepted = 0
    llm_failures = 0
    determinism_mismatches = 0
    anchor_allowed = 0
    contamination_spans: Set[str] = set()
    llm_calls = 0

    use_batching = args.batch_size > 1 and batch_prompt_template is not None

    if use_batching:
        batches = _group_seeds_into_batches(seed_rows, args.batch_size)
        print(
            f"{LOG_PREFIX}   Batching: {len(seed_rows)} seeds -> "
            f"{len(batches)} batches (batch_size={args.batch_size})"
        )
        for batch in batches:
            batch_rows, batch_llm_fails = _adjudicate_batch(
                batch,
                context_data=context_data,
                args=args,
                batch_prompt_template=batch_prompt_template,
                batch_schema=batch_schema,
            )
            llm_calls += 1
            llm_failures += batch_llm_fails
            for row in batch_rows:
                if row.get("repair_accepted"):
                    repairs_accepted += 1
                if row.get("anchor_allowed"):
                    anchor_allowed += 1
                adj = row.get("adjudication", {})
                span = (
                    f"{adj.get('contamination_start_segment_id')}->"
                    f"{adj.get('contamination_end_segment_id')}"
                )
                contamination_spans.add(span)
                adjudications.append(row)
    else:
        for seed in seed_rows:
            sid = _safe_int(seed.get("segment_id"))
            if sid is None:
                continue
            row, llm_failed = _adjudicate_single_seed(
                seed,
                context_data=context_data,
                args=args,
                prompt_template=prompt_template,
                schema=schema,
            )
            llm_calls += 1
            if llm_failed:
                llm_failures += 1
            if not row.get("determinism_match", True):
                determinism_mismatches += 1
            if row.get("repair_accepted"):
                repairs_accepted += 1
            if row.get("anchor_allowed"):
                anchor_allowed += 1
            adj = row.get("adjudication", {})
            span = (
                f"{adj.get('contamination_start_segment_id')}->"
                f"{adj.get('contamination_end_segment_id')}"
            )
            contamination_spans.add(span)
            adjudications.append(row)

    out = {
        "video_id": damage_map.get("video_id"),
        "generated_at": time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime()),
        "pipeline_version": PIPELINE_VERSION,
        "source_file": str(input_path),
        "damage_map_file": str(input_path),
        "context_file": str(context_path),
        "video_type": video_type,
        "skipped": False,
        "model": args.model,
        "batch_size": args.batch_size,
        "thresholds": {
            "repair_accept_threshold": args.repair_accept_threshold,
            "anchor_allow_threshold": args.anchor_allow_threshold,
            "context_left": args.context_left,
            "context_right": args.context_right,
            "determinism_n": max(1, int(args.determinism_n)),
        },
        "summary": {
            "seeds_total": len(adjudications),
            "llm_calls": llm_calls,
            "repairs_accepted": repairs_accepted,
            "anchor_allowed": anchor_allowed,
            "llm_failures": llm_failures,
            "determinism_mismatches": determinism_mismatches,
            "contamination_spans": len(contamination_spans),
        },
        "seeds": adjudications,
    }

    if args.dry_run:
        print(
            f"{LOG_PREFIX} [DRY RUN] {input_path.name}: "
            f"seeds={len(adjudications)}, llm_calls={llm_calls}, "
            f"repairs_accepted={repairs_accepted}, "
            f"determinism_mismatches={determinism_mismatches}"
        )
        return {
            "seeds_total": len(adjudications),
            "llm_calls": llm_calls,
            "repairs_accepted": repairs_accepted,
            "determinism_mismatches": determinism_mismatches,
            "llm_failures": llm_failures,
        }

    output_path.parent.mkdir(parents=True, exist_ok=True)
    output_path.write_text(json.dumps(out, indent=2, ensure_ascii=False) + "\n", encoding="utf-8")
    print(
        f"{LOG_PREFIX} {input_path.name}: seeds={len(adjudications)}, "
        f"llm_calls={llm_calls}, repairs_accepted={repairs_accepted}, "
        f"determinism_mismatches={determinism_mismatches}"
    )
    print(f"{LOG_PREFIX}   Output : {output_path}")
    print(f"{LOG_PREFIX}   Context: {context_path}")
    return {
        "seeds_total": len(adjudications),
        "llm_calls": llm_calls,
        "repairs_accepted": repairs_accepted,
        "determinism_mismatches": determinism_mismatches,
        "llm_failures": llm_failures,
    }


def _run_directory_with_files(
    files: List[Path],
    in_dir: Path,
    out_dir: Path,
    args: argparse.Namespace,
    *,
    output_root_dir: Optional[Path] = None,
    input_root_dir: Path,
    context_root_dir: Path,
    prompt_template: str,
    schema: Optional[Dict[str, Any]],
    batch_prompt_template: Optional[str] = None,
    batch_schema: Optional[Dict[str, Any]] = None,
) -> None:
    if not files:
        return
    print(f"{LOG_PREFIX} Input : {in_dir}")
    print(f"{LOG_PREFIX} Output: {out_dir}")
    print(f"{LOG_PREFIX} Files : {len(files)}")

    processed = 0
    skipped = 0
    failed = 0
    totals = {
        "seeds_total": 0,
        "llm_calls": 0,
        "repairs_accepted": 0,
        "determinism_mismatches": 0,
        "llm_failures": 0,
    }

    for input_file in files:
        preferred_output = compute_output_path_with_layout(
            input_file,
            out_dir,
            input_root_dir=in_dir,
        )
        existing_output = find_existing_output_path(
            input_file,
            out_dir,
            output_root_dir=output_root_dir,
            input_root_dir=in_dir,
        )
        if existing_output and not args.overwrite:
            skipped += 1
            continue
        try:
            counts = adjudicate_file(
                input_file,
                preferred_output,
                args=args,
                input_root_dir=input_root_dir,
                context_root_dir=context_root_dir,
                prompt_template=prompt_template,
                schema=schema,
                batch_prompt_template=batch_prompt_template,
                batch_schema=batch_schema,
            )
            processed += 1
            for key in totals:
                totals[key] += int(counts.get(key, 0))
        except Exception as exc:
            failed += 1
            print(f"{LOG_PREFIX} ERROR: {input_file} -> {exc}")

    print(f"\n{LOG_PREFIX} Done.")
    print(f"  Processed:              {processed}")
    print(f"  Skipped:                {skipped}")
    print(f"  Failed:                 {failed}")
    print(f"  Seeds total:            {totals['seeds_total']}")
    print(f"  LLM calls:              {totals['llm_calls']}")
    print(f"  Repairs accepted:       {totals['repairs_accepted']}")
    print(f"  Determinism mismatches: {totals['determinism_mismatches']}")
    print(f"  LLM failures:           {totals['llm_failures']}")
    if failed > 0:
        sys.exit(1)


def _run_directory(
    in_dir: Path,
    out_dir: Path,
    args: argparse.Namespace,
    *,
    input_root_dir: Path,
    context_root_dir: Path,
    prompt_template: str,
    schema: Optional[Dict[str, Any]],
    batch_prompt_template: Optional[str] = None,
    batch_schema: Optional[Dict[str, Any]] = None,
) -> None:
    files = find_input_files(in_dir)
    if not files:
        print(f"{LOG_PREFIX} No .damage-map.json files found in: {in_dir}")
        return
    _run_directory_with_files(
        files,
        in_dir,
        out_dir,
        args,
        input_root_dir=input_root_dir,
        context_root_dir=context_root_dir,
        prompt_template=prompt_template,
        schema=schema,
        batch_prompt_template=batch_prompt_template,
        batch_schema=batch_schema,
    )


def _run_input(
    args: argparse.Namespace,
    in_base: Path,
    out_base: Path,
    *,
    context_root_dir: Path,
    prompt_template: str,
    schema: Optional[Dict[str, Any]],
    batch_prompt_template: Optional[str] = None,
    batch_schema: Optional[Dict[str, Any]] = None,
) -> None:
    input_path = Path(args.input)
    if not input_path.exists():
        input_path = repo_root() / args.input
    if not input_path.exists():
        raise SystemExit(f"Input not found: {args.input}")
    input_path = input_path.resolve()

    if input_path.is_file():
        out_dir = Path(args.output) if args.output else out_base
        output_path = compute_output_path_with_layout(input_path, out_dir, input_root_dir=in_base)
        if output_path.exists() and not args.overwrite:
            print(f"{LOG_PREFIX} Output exists, skipping: {output_path}")
            return
        adjudicate_file(
            input_path,
            output_path,
            args=args,
            input_root_dir=in_base,
            context_root_dir=context_root_dir,
            prompt_template=prompt_template,
            schema=schema,
            batch_prompt_template=batch_prompt_template,
            batch_schema=batch_schema,
        )
        return

    out_dir = Path(args.output) if args.output else out_base
    _run_directory(
        input_path,
        out_dir,
        args,
        input_root_dir=in_base,
        context_root_dir=context_root_dir,
        prompt_template=prompt_template,
        schema=schema,
        batch_prompt_template=batch_prompt_template,
        batch_schema=batch_schema,
    )


def _run_sources(
    args: argparse.Namespace,
    in_base: Path,
    out_base: Path,
    *,
    context_root_dir: Path,
    prompt_template: str,
    schema: Optional[Dict[str, Any]],
    batch_prompt_template: Optional[str] = None,
    batch_schema: Optional[Dict[str, Any]] = None,
) -> None:
    sources_path = repo_root() / args.sources
    if not sources_path.exists():
        raise SystemExit(f"Sources file not found: {sources_path}")
    for src_name, _ in parse_sources_file(sources_path):
        src_in_dir = in_base / src_name
        if not src_in_dir.exists():
            print(f"{LOG_PREFIX} Skipping {src_name}: no 06f.DET.damage-map output")
            continue
        src_out_dir = out_base / src_name
        files = find_input_files(src_in_dir)
        _run_directory_with_files(
            files,
            src_in_dir,
            src_out_dir,
            args,
            output_root_dir=out_base,
            input_root_dir=in_base,
            context_root_dir=context_root_dir,
            prompt_template=prompt_template,
            schema=schema,
            batch_prompt_template=batch_prompt_template,
            batch_schema=batch_schema,
        )


def _run_manifest(
    args: argparse.Namespace,
    in_base: Path,
    out_base: Path,
    *,
    context_root_dir: Path,
    prompt_template: str,
    schema: Optional[Dict[str, Any]],
    batch_prompt_template: Optional[str] = None,
    batch_schema: Optional[Dict[str, Any]] = None,
) -> None:
    manifest_path = Path(args.manifest)
    if not manifest_path.is_absolute():
        manifest_path = repo_root() / manifest_path
    if not manifest_path.exists():
        raise SystemExit(f"Manifest file not found: {manifest_path}")
    sources_map = load_manifest_sources(manifest_path)
    for src_name, vid_ids in sorted(sources_map.items()):
        src_in_dir = in_base / src_name
        if not src_in_dir.exists():
            print(f"{LOG_PREFIX} Skipping {src_name}: no 06f.DET.damage-map output")
            continue
        src_out_dir = out_base / src_name
        files = manifest_filter_files(find_input_files(src_in_dir), vid_ids)
        if not files:
            print(f"{LOG_PREFIX} Skipping {src_name}: no manifest videos found in input")
            continue
        print(f"{LOG_PREFIX} Manifest: {src_name} ({len(files)} videos)")
        _run_directory_with_files(
            files,
            src_in_dir,
            src_out_dir,
            args,
            output_root_dir=out_base,
            input_root_dir=in_base,
            context_root_dir=context_root_dir,
            prompt_template=prompt_template,
            schema=schema,
            batch_prompt_template=batch_prompt_template,
            batch_schema=batch_schema,
        )


def _run_named_source(
    args: argparse.Namespace,
    in_base: Path,
    out_base: Path,
    *,
    context_root_dir: Path,
    prompt_template: str,
    schema: Optional[Dict[str, Any]],
    batch_prompt_template: Optional[str] = None,
    batch_schema: Optional[Dict[str, Any]] = None,
) -> None:
    name = args.name
    in_dir = in_base / name
    if not in_dir.exists():
        raise SystemExit(f"Input directory not found: {in_dir}")
    out_dir = Path(args.output) if args.output else out_base / name
    _run_directory(
        in_dir,
        out_dir,
        args,
        input_root_dir=in_base,
        context_root_dir=context_root_dir,
        prompt_template=prompt_template,
        schema=schema,
        batch_prompt_template=batch_prompt_template,
        batch_schema=batch_schema,
    )


def main() -> None:
    parser = argparse.ArgumentParser(description="Stage 06g LLM damage adjudicator")
    parser.add_argument("name", nargs="?", help="Source name (folder under data/06f.DET.damage-map/)")
    parser.add_argument("youtube_url", nargs="?", help="Unused (pipeline compatibility)")
    parser.add_argument("--input", help="Input .damage-map.json file or directory")
    parser.add_argument(
        "--input-root",
        help=(
            "Root directory for source/manifest runs "
            "(default: data/06f.DET.damage-map, or data/test/06f.damage-map with --test)"
        ),
    )
    parser.add_argument(
        "--context-root",
        help=(
            "06d context root (default: data/06d.DET.sanitized, or data/test/06d.sanitized with --test)"
        ),
    )
    parser.add_argument("--output", help="Output directory (default: data/06g.LLM.damage-adjudicator/)")
    parser.add_argument("--model", default="opus", help="Claude model (default: opus)")
    parser.add_argument("--timeout-seconds", type=int, default=600, help="Per-call timeout seconds")
    parser.add_argument("--retries", type=int, default=2, help="Retries per adjudication call")
    parser.add_argument(
        "--llm-retries",
        type=int,
        help="Alias for --retries (for consistency with other LLM stages)",
    )
    parser.add_argument("--determinism-n", type=int, default=1, help="Run N adjudication calls per seed and compare normalized outputs")
    parser.add_argument("--repair-accept-threshold", type=float, default=0.90, help="Minimum transcript_confidence required to accept repaired_text")
    parser.add_argument("--anchor-allow-threshold", type=float, default=0.80, help="Minimum overall confidence for anchor eligibility")
    parser.add_argument("--context-left", type=int, default=4, help="Context window left size in segments")
    parser.add_argument("--context-right", type=int, default=8, help="Context window right size in segments")
    parser.add_argument("--include-low-severity", action="store_true", help="Also adjudicate low-severity seeds")
    parser.add_argument("--include-raw-llm", action="store_true", help="Include raw LLM output in artifact (debug only)")
    parser.add_argument(
        "--skip-video-type-filter",
        action="store_true",
        help="Run adjudication on all video types (override default infield/compilation-only filter)",
    )
    parser.add_argument("--test", action="store_true", help="Use test roots under data/test/")
    parser.add_argument(
        "--sources",
        nargs="?",
        const="docs/pipeline/sources.txt",
        help="Process all sources from sources.txt file",
    )
    parser.add_argument("--manifest", help="Manifest file: process listed videos only")
    parser.add_argument("--batch-size", type=int, default=8, help="Seeds per LLM call (1=no batching, default=8)")
    parser.add_argument("--dry-run", action="store_true", help="Preview without writing files")
    parser.add_argument("--overwrite", action="store_true", help="Overwrite existing output files")
    args = parser.parse_args()

    if args.llm_retries is not None:
        args.retries = args.llm_retries

    args.timeout_seconds = max(1, int(args.timeout_seconds))
    args.retries = max(1, int(args.retries))
    args.determinism_n = max(1, int(args.determinism_n))
    args.batch_size = max(1, min(15, int(args.batch_size)))
    args.context_left = max(0, int(args.context_left))
    args.context_right = max(0, int(args.context_right))
    args.repair_accept_threshold = max(0.0, min(1.0, float(args.repair_accept_threshold)))
    args.anchor_allow_threshold = max(0.0, min(1.0, float(args.anchor_allow_threshold)))

    prompt_template = _load_prompt_template()
    schema = _load_schema()

    # Load batch prompt/schema (used when batch_size > 1)
    batch_prompt_template: Optional[str] = None
    batch_schema: Optional[Dict[str, Any]] = None
    if args.batch_size > 1:
        if not BATCH_PROMPT_PATH.exists():
            raise RuntimeError(f"Batch prompt template missing: {BATCH_PROMPT_PATH}")
        batch_prompt_template = BATCH_PROMPT_PATH.read_text(encoding="utf-8")
        if BATCH_SCHEMA_PATH.exists():
            try:
                raw = json.loads(BATCH_SCHEMA_PATH.read_text(encoding="utf-8"))
                batch_schema = raw if isinstance(raw, dict) else None
            except Exception:
                batch_schema = None
        print(f"{LOG_PREFIX} Batch mode: batch_size={args.batch_size}")

    if args.test:
        in_root = resolve_root_path(args.input_root, test_input_root())
        ctx_root = resolve_root_path(args.context_root, test_context_root())
        out_root = Path(args.output) if args.output else test_output_root()
    else:
        in_root = resolve_root_path(args.input_root, input_root())
        ctx_root = resolve_root_path(args.context_root, context_root())
        out_root = Path(args.output) if args.output else output_root()

    batch_kw = {
        "batch_prompt_template": batch_prompt_template,
        "batch_schema": batch_schema,
    }

    if args.test:
        _run_directory(
            in_root,
            out_root,
            args,
            input_root_dir=in_root,
            context_root_dir=ctx_root,
            prompt_template=prompt_template,
            schema=schema,
            **batch_kw,
        )
    elif args.manifest:
        _run_manifest(
            args,
            in_root,
            out_root,
            context_root_dir=ctx_root,
            prompt_template=prompt_template,
            schema=schema,
            **batch_kw,
        )
    elif args.input:
        _run_input(
            args,
            in_root,
            out_root,
            context_root_dir=ctx_root,
            prompt_template=prompt_template,
            schema=schema,
            **batch_kw,
        )
    elif args.sources:
        _run_sources(
            args,
            in_root,
            out_root,
            context_root_dir=ctx_root,
            prompt_template=prompt_template,
            schema=schema,
            **batch_kw,
        )
    elif args.name:
        _run_named_source(
            args,
            in_root,
            out_root,
            context_root_dir=ctx_root,
            prompt_template=prompt_template,
            schema=schema,
            **batch_kw,
        )
    else:
        raise SystemExit("Provide a source name, --input, --test, --manifest, or --sources")


if __name__ == "__main__":
    main()
