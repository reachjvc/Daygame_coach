#!/usr/bin/env python3
"""
scripts/training-data/06c.patch

Auto-apply high-confidence fixes from 06b verification reports.

Pure deterministic JSON patching — no LLM calls.

Reads:
  - Stage 06 output:
      data/06.video-type/<source>/<video>/*.conversations.json
  - Stage 06b verification reports:
      data/06b.verify/<source>/<video>/*.verification.json

Writes:
  - Patched conversations (same filename pattern as stage 06):
      data/06c.patched/<source>/<video>/*.conversations.json

Auto-applies:
  - misattributions: changes speaker_role on matched segments
  - collapse_issues: changes speaker_role + speaker_role_override

Flags but does NOT fix:
  - boundary_issues
  - other_flags

Use:

  A) From pipeline:
     ./scripts/training-data/06c.patch "source_name" "youtube_url"

  B) Test videos:
     ./scripts/training-data/06c.patch --test

  C) Single file:
     ./scripts/training-data/06c.patch --input data/06.video-type/video.conversations.json

  D) Batch from sources file:
     ./scripts/training-data/06c.patch --sources

  E) Dry run (show what would be patched):
     ./scripts/training-data/06c.patch --input data/06.video-type/video.conversations.json --dry-run
"""

from __future__ import annotations

import argparse
import copy
import json
import shlex
import sys
import time
from dataclasses import dataclass, asdict
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple

from batch.manifest_parser import load_manifest, load_manifest_sources, manifest_filter_files


# ---------------------------
# Configuration
# ---------------------------

PIPELINE_VERSION = "06c.patch-v1.2"  # v1.2: Handle reclassify_as_commentary, lenient override matching

LOG_PREFIX = "[06c.patch]"

# Failure budget: halt batch if too many failures
MAX_CONSECUTIVE_FAILURES = 3
MAX_FAILURE_RATE = 0.20  # halt if >20% of videos fail

# Confidence thresholds: only auto-apply high-confidence fixes
MIN_MISATTRIBUTION_CONFIDENCE = 0.7
MIN_VIDEO_TYPE_CONFIDENCE = 0.85
MIN_BOUNDARY_CONFIDENCE = 0.90  # Higher threshold - structural changes


# ---------------------------
# State Management
# ---------------------------

@dataclass
class ProcessingState:
    version: int
    completed_files: List[str]
    in_progress: Optional[str]
    failures: List[Dict[str, str]]


def load_state(state_path: Path) -> ProcessingState:
    if state_path.exists():
        try:
            data = json.loads(state_path.read_text())
            return ProcessingState(
                version=data.get("version", 1),
                completed_files=data.get("completed_files", []),
                in_progress=data.get("in_progress"),
                failures=data.get("failures", []),
            )
        except (json.JSONDecodeError, KeyError):
            pass
    return ProcessingState(version=1, completed_files=[], in_progress=None, failures=[])


def save_state(state_path: Path, state: ProcessingState) -> None:
    state_path.parent.mkdir(parents=True, exist_ok=True)
    state_path.write_text(json.dumps(asdict(state), indent=2))


# ---------------------------
# Patching Logic
# ---------------------------

def _merge_conversations(segments: List[Dict], from_conv: int, to_conv: int) -> int:
    """Reassign all segments from from_conv to to_conv. Returns count of segments moved."""
    count = 0
    for seg in segments:
        if seg.get("conversation_id") == from_conv:
            seg["conversation_id"] = to_conv
            count += 1
    return count


def _split_conversation(
    segments: List[Dict], conv_id: int, split_seg_id: int, new_conv_id: int
) -> int:
    """Split conversation at segment, assigning split_seg and after to new_conv_id.

    Returns count of segments reassigned.
    """
    count = 0
    in_split = False
    for seg in segments:
        if seg.get("conversation_id") != conv_id:
            continue
        if seg.get("id") == split_seg_id:
            in_split = True
        if in_split:
            seg["conversation_id"] = new_conv_id
            count += 1
    return count


def _get_next_conversation_id(segments: List[Dict]) -> int:
    """Get the next available conversation ID."""
    max_id = 0
    for seg in segments:
        conv_id = seg.get("conversation_id", 0)
        if conv_id > max_id:
            max_id = conv_id
    return max_id + 1


def apply_patches(
    conversations_data: Dict[str, Any],
    verification_data: Dict[str, Any],
) -> Tuple[List[Dict], List[str]]:
    """Apply high-confidence fixes from verification report to conversations data.

    Returns (fixes_applied, flags_not_fixed).
    """
    segments = conversations_data.get("segments", [])
    # Build segment index by id for fast lookup
    seg_by_id: Dict[int, Dict] = {}
    for seg in segments:
        seg_by_id[seg.get("id", -1)] = seg

    fixes_applied: List[Dict] = []
    flags_not_fixed: List[str] = []

    # --- Apply misattribution fixes ---
    for entry in verification_data.get("misattributions", []):
        seg_id = entry.get("segment_id")
        current_role = entry.get("current_role")
        suggested_role = entry.get("suggested_role")
        evidence = entry.get("evidence", "")
        confidence = entry.get("confidence", 1.0)  # Default 1.0 for backwards compat

        if seg_id is None or suggested_role is None:
            flags_not_fixed.append(
                f"misattribution: incomplete entry (seg_id={seg_id}, suggested={suggested_role})"
            )
            continue

        # Skip prose suggested_roles (should be role values like "coach", "target", "other", "mixed")
        valid_roles = {"coach", "target", "other", "mixed"}
        if suggested_role not in valid_roles:
            flags_not_fixed.append(
                f"misattribution: segment {seg_id} suggested_role '{suggested_role}' is not a valid role — skipping"
            )
            continue

        seg = seg_by_id.get(seg_id)
        if seg is None:
            flags_not_fixed.append(
                f"misattribution: segment {seg_id} not found in conversations data"
            )
            continue

        actual_role = seg.get("speaker_role")
        if current_role and actual_role != current_role:
            flags_not_fixed.append(
                f"misattribution: segment {seg_id} current_role mismatch "
                f"(expected '{current_role}', found '{actual_role}') — skipping"
            )
            continue

        if confidence < MIN_MISATTRIBUTION_CONFIDENCE:
            flags_not_fixed.append(
                f"misattribution: segment {seg_id} confidence {confidence:.0%} below threshold — skipping"
            )
            continue

        old_role = seg["speaker_role"]
        old_override = seg.get("speaker_role_override")
        seg["speaker_role"] = suggested_role
        if old_override is not None:
            seg["speaker_role_override"] = suggested_role
        fixes_applied.append({
            "segment_id": seg_id,
            "field": "speaker_role" + ("+speaker_role_override" if old_override else ""),
            "old": f"role={old_role}" + (f", override={old_override}" if old_override else ""),
            "new": suggested_role,
            "source": "misattribution",
            "evidence": evidence[:200],
        })

    # --- Apply collapse issue fixes ---
    for entry in verification_data.get("collapse_issues", []):
        seg_id = entry.get("segment_id")
        suggested_override = entry.get("suggested_override")
        current_override = entry.get("current_override")
        evidence = entry.get("evidence", "")
        confidence = entry.get("confidence", 1.0)  # Default 1.0 for backwards compat

        if seg_id is None or suggested_override is None:
            flags_not_fixed.append(
                f"collapse_issue: incomplete entry (seg_id={seg_id}, suggested={suggested_override})"
            )
            continue

        # Skip prose suggested_overrides (should be role values like "coach", "target", "other")
        valid_roles = {"coach", "target", "other", "mixed"}
        if suggested_override not in valid_roles:
            flags_not_fixed.append(
                f"collapse_issue: segment {seg_id} suggested_override '{suggested_override}' is not a valid role — skipping"
            )
            continue

        seg = seg_by_id.get(seg_id)
        if seg is None:
            flags_not_fixed.append(
                f"collapse_issue: segment {seg_id} not found in conversations data"
            )
            continue

        actual_override = seg.get("speaker_role_override")
        actual_role = seg.get("speaker_role")

        # Lenient current_override matching: treat null, None, "none", "none (raw label)", "" as equivalent
        def normalize_override(val):
            if val is None or val == "" or (isinstance(val, str) and val.lower().startswith("none")):
                return None
            return val

        normalized_current = normalize_override(current_override)
        normalized_actual = normalize_override(actual_override)

        # Only check mismatch if current_override was specified and non-null
        if normalized_current is not None and normalized_actual != normalized_current and actual_role != normalized_current:
            flags_not_fixed.append(
                f"collapse_issue: segment {seg_id} current_override mismatch "
                f"(expected '{current_override}', found override='{actual_override}', role='{actual_role}') — skipping"
            )
            continue

        if confidence < MIN_MISATTRIBUTION_CONFIDENCE:
            flags_not_fixed.append(
                f"collapse_issue: segment {seg_id} confidence {confidence:.0%} below threshold — skipping"
            )
            continue

        old_role = seg.get("speaker_role")
        old_override = seg.get("speaker_role_override")
        seg["speaker_role"] = suggested_override
        seg["speaker_role_override"] = suggested_override
        fixes_applied.append({
            "segment_id": seg_id,
            "field": "speaker_role+speaker_role_override",
            "old": f"role={old_role}, override={old_override}",
            "new": suggested_override,
            "source": "collapse_issue",
            "evidence": evidence[:200],
        })

    # --- Apply video type fix ---
    video_type_check = verification_data.get("video_type_check", {})
    if not video_type_check.get("agrees", True):
        suggested_type = video_type_check.get("suggested_type")
        confidence = video_type_check.get("confidence", 0)
        reasoning = video_type_check.get("reasoning", "")

        if suggested_type and confidence >= MIN_VIDEO_TYPE_CONFIDENCE:
            video_type_obj = conversations_data.get("video_type", {})
            old_type = video_type_obj.get("type") if isinstance(video_type_obj, dict) else video_type_obj
            if isinstance(video_type_obj, dict):
                conversations_data["video_type"]["type"] = suggested_type
            else:
                conversations_data["video_type"] = {"type": suggested_type}
            fixes_applied.append({
                "field": "video_type",
                "old": old_type,
                "new": suggested_type,
                "source": "video_type_check",
                "confidence": confidence,
                "reasoning": reasoning[:200],
            })
        elif suggested_type:
            flags_not_fixed.append(
                f"video_type: suggested '{suggested_type}' but confidence {confidence:.0%} "
                f"below {MIN_VIDEO_TYPE_CONFIDENCE:.0%} threshold"
            )

    # --- Apply boundary fixes (or log as unfixed) ---
    for entry in verification_data.get("boundary_issues", []):
        conv_id = entry.get("conversation_id")
        severity = entry.get("severity", "unknown")
        issue = entry.get("issue", "")
        suggested_fix = entry.get("suggested_fix")
        confidence = entry.get("confidence", 0)

        # Skip if no fix suggested or confidence too low
        if not suggested_fix or confidence < MIN_BOUNDARY_CONFIDENCE:
            reason = "no fix suggested" if not suggested_fix else f"confidence {confidence:.0%} below {MIN_BOUNDARY_CONFIDENCE:.0%}"
            flags_not_fixed.append(
                f"boundary_issue [{severity}]: Conv {conv_id} — {issue[:100]} ({reason})"
            )
            continue

        # Apply the fix
        if suggested_fix == "merge_with_next":
            target_conv = conv_id + 1
            count = _merge_conversations(segments, conv_id, target_conv)
            if count > 0:
                fixes_applied.append({
                    "field": "conversation_id",
                    "action": "merge",
                    "from_conv": conv_id,
                    "to_conv": target_conv,
                    "segments_moved": count,
                    "source": "boundary_issue",
                    "confidence": confidence,
                    "issue": issue[:150],
                })
            else:
                flags_not_fixed.append(
                    f"boundary_issue: merge_with_next for conv {conv_id} found no segments"
                )

        elif suggested_fix == "merge_with_previous":
            target_conv = conv_id - 1
            if target_conv < 1:
                flags_not_fixed.append(
                    f"boundary_issue: merge_with_previous for conv {conv_id} invalid (no previous)"
                )
                continue
            count = _merge_conversations(segments, conv_id, target_conv)
            if count > 0:
                fixes_applied.append({
                    "field": "conversation_id",
                    "action": "merge",
                    "from_conv": conv_id,
                    "to_conv": target_conv,
                    "segments_moved": count,
                    "source": "boundary_issue",
                    "confidence": confidence,
                    "issue": issue[:150],
                })
            else:
                flags_not_fixed.append(
                    f"boundary_issue: merge_with_previous for conv {conv_id} found no segments"
                )

        elif suggested_fix.startswith("split_at_segment_"):
            try:
                split_seg_id = int(suggested_fix.split("_")[-1])
            except ValueError:
                flags_not_fixed.append(
                    f"boundary_issue: invalid split format '{suggested_fix}'"
                )
                continue

            new_conv_id = _get_next_conversation_id(segments)
            count = _split_conversation(segments, conv_id, split_seg_id, new_conv_id)
            if count > 0:
                fixes_applied.append({
                    "field": "conversation_id",
                    "action": "split",
                    "original_conv": conv_id,
                    "split_at_segment": split_seg_id,
                    "new_conv_id": new_conv_id,
                    "segments_moved": count,
                    "source": "boundary_issue",
                    "confidence": confidence,
                    "issue": issue[:150],
                })
            else:
                flags_not_fixed.append(
                    f"boundary_issue: split_at_segment_{split_seg_id} for conv {conv_id} found no segments"
                )

        elif suggested_fix == "reclassify_as_commentary":
            # Move all segments from this conversation to commentary (conv_id=0)
            count = _merge_conversations(segments, conv_id, 0)
            if count > 0:
                fixes_applied.append({
                    "field": "conversation_id",
                    "action": "reclassify_as_commentary",
                    "from_conv": conv_id,
                    "to_conv": 0,
                    "segments_moved": count,
                    "source": "boundary_issue",
                    "confidence": confidence,
                    "issue": issue[:150],
                })
            else:
                flags_not_fixed.append(
                    f"boundary_issue: reclassify_as_commentary for conv {conv_id} found no segments"
                )

        else:
            flags_not_fixed.append(
                f"boundary_issue [{severity}]: Conv {conv_id} — unknown fix type '{suggested_fix}'"
            )

    # --- Log other_flags as unfixed ---
    for flag in verification_data.get("other_flags", []):
        flags_not_fixed.append(f"other_flag: {str(flag)[:150]}")

    return fixes_applied, flags_not_fixed


def patch_file(
    conversations_path: Path,
    verification_path: Optional[Path],
    output_path: Path,
    dry_run: bool = False,
) -> Dict[str, Any]:
    """Patch a single conversations file using its verification report."""

    print(f"\n{LOG_PREFIX} Patching: {conversations_path.name}")

    with conversations_path.open("r", encoding="utf-8") as f:
        conversations_data = json.load(f)

    segment_count = len(conversations_data.get("segments", []))
    print(f"{LOG_PREFIX}   Segments: {segment_count}")

    # Deep copy so we don't modify original
    patched = copy.deepcopy(conversations_data)

    if verification_path is None or not verification_path.exists():
        # No verification file — copy unchanged
        print(f"{LOG_PREFIX}   No verification report found — copying unchanged")

        patched["patch_metadata"] = {
            "patched_at": time.strftime("%Y-%m-%dT%H:%M:%SZ"),
            "pipeline_version": PIPELINE_VERSION,
            "verification_verdict": None,
            "verification_file": None,
            "fixes_applied": [],
            "fixes_applied_count": 0,
            "flags_not_fixed": [],
            "flags_not_fixed_count": 0,
            "note": "No verification report found — copied unchanged",
        }

        if not dry_run:
            output_path.parent.mkdir(parents=True, exist_ok=True)
            with output_path.open("w", encoding="utf-8") as f:
                json.dump(patched, f, indent=2, ensure_ascii=False)
            print(f"{LOG_PREFIX}   Output: {output_path}")
        else:
            print(f"{LOG_PREFIX}   [DRY RUN] Would copy unchanged to {output_path}")

        return {"verdict": None, "fixes": 0, "flags": 0}

    # Load verification report
    with verification_path.open("r", encoding="utf-8") as f:
        verification_data = json.load(f)

    verdict = verification_data.get("verdict", "FLAG")
    print(f"{LOG_PREFIX}   Verification verdict: {verdict}")
    print(f"{LOG_PREFIX}   Verification file: {verification_path}")

    if verdict == "REJECT":
        raise RuntimeError(
            f"Verification verdict is REJECT for {conversations_path.name}. "
            "06b should have halted the pipeline before reaching 06c."
        )

    if verdict == "APPROVE":
        # Copy unchanged, just add metadata
        fixes_applied: List[Dict] = []
        flags_not_fixed: List[str] = []
    else:
        # FLAG — apply patches
        fixes_applied, flags_not_fixed = apply_patches(patched, verification_data)

    patched["patch_metadata"] = {
        "patched_at": time.strftime("%Y-%m-%dT%H:%M:%SZ"),
        "pipeline_version": PIPELINE_VERSION,
        "verification_verdict": verdict,
        "verification_file": str(verification_path),
        "fixes_applied": fixes_applied,
        "fixes_applied_count": len(fixes_applied),
        "flags_not_fixed": flags_not_fixed,
        "flags_not_fixed_count": len(flags_not_fixed),
    }

    # Print summary
    if fixes_applied:
        print(f"{LOG_PREFIX}   Fixes applied: {len(fixes_applied)}")
        for fix in fixes_applied:
            print(f"{LOG_PREFIX}     seg {fix['segment_id']}: {fix['old']} → {fix['new']} ({fix['source']})")
    if flags_not_fixed:
        print(f"{LOG_PREFIX}   Flags not fixed: {len(flags_not_fixed)}")
        for flag in flags_not_fixed:
            print(f"{LOG_PREFIX}     {flag[:120]}")

    if not dry_run:
        output_path.parent.mkdir(parents=True, exist_ok=True)
        with output_path.open("w", encoding="utf-8") as f:
            json.dump(patched, f, indent=2, ensure_ascii=False)
        print(f"{LOG_PREFIX}   Output: {output_path}")
    else:
        print(f"{LOG_PREFIX}   [DRY RUN] Would write to {output_path}")

    return {
        "verdict": verdict,
        "fixes": len(fixes_applied),
        "flags": len(flags_not_fixed),
    }


# ---------------------------
# Path helpers
# ---------------------------

def repo_root() -> Path:
    return Path(__file__).resolve().parents[2]


def s06_root() -> Path:
    return repo_root() / "data" / "06.video-type"


def s06b_root() -> Path:
    return repo_root() / "data" / "06b.verify"


def output_root() -> Path:
    return repo_root() / "data" / "06c.patched"


def test_s06_root() -> Path:
    return repo_root() / "data" / "test" / "06.video-type"


def test_s06b_root() -> Path:
    return repo_root() / "data" / "test" / "06b.verify"


def test_output_root() -> Path:
    return repo_root() / "data" / "test" / "06c.patched"


def compute_output_path(input_path: Path, output_dir: Path) -> Path:
    """Compute output path — same *.conversations.json pattern as stage 06."""
    stem = input_path.stem
    # Ensure the output has .conversations.json extension
    if stem.endswith(".conversations"):
        return output_dir / f"{stem}.json"
    return output_dir / f"{stem}.conversations.json"


def find_verification_for(
    conversations_path: Path, s06_dir: Path, s06b_dir: Path
) -> Optional[Path]:
    """Find the matching verification JSON for a conversations file.

    Tries two strategies:
    1. Mirror the relative path from s06_dir into s06b_dir, swapping extension
    2. Flat lookup in s06b_dir root (for files not in source subdirectories)
    """
    stem = conversations_path.stem
    if stem.endswith(".conversations"):
        stem = stem[:-len(".conversations")]
    verification_name = f"{stem}.verification.json"

    # Strategy 1: Mirror relative path
    try:
        rel = conversations_path.parent.relative_to(s06_dir)
        candidate = s06b_dir / rel / verification_name
        if candidate.exists():
            return candidate
    except ValueError:
        pass

    # Strategy 2: Flat lookup at s06b root
    candidate = s06b_dir / verification_name
    if candidate.exists():
        return candidate

    # Strategy 3: Search recursively in s06b_dir
    for found in s06b_dir.rglob(verification_name):
        return found

    return None


def parse_sources_file(path: Path) -> List[Tuple[str, str]]:
    sources: List[Tuple[str, str]] = []
    for raw in path.read_text(encoding="utf-8").splitlines():
        line = raw.strip()
        if not line or line.startswith("#"):
            continue
        if "|" in line:
            name, url = line.split("|", 1)
            sources.append((name.strip(), url.strip()))
            continue
        parts = shlex.split(line)
        if len(parts) >= 2:
            sources.append((parts[0], parts[1]))
    return sources


def find_input_files(in_dir: Path) -> List[Path]:
    return sorted(in_dir.rglob("*.conversations.json"))


# ---------------------------
# CLI
# ---------------------------

def main() -> None:
    parser = argparse.ArgumentParser(
        description="Auto-apply high-confidence fixes from 06b verification to stage 06 output"
    )
    parser.add_argument(
        "name", nargs="?",
        help="Source name (folder under data/06.video-type/)"
    )
    parser.add_argument(
        "youtube_url", nargs="?",
        help="YouTube URL (unused, accepted for pipeline compatibility)"
    )
    parser.add_argument(
        "--input",
        help="Input .conversations.json file or directory"
    )
    parser.add_argument(
        "--output",
        help="Output directory (defaults to data/06c.patched/)"
    )
    parser.add_argument(
        "--test",
        action="store_true",
        help="Process test videos (data/test/06.video-type/)"
    )
    parser.add_argument(
        "--sources",
        nargs="?",
        const="docs/pipeline/sources.txt",
        help="Process all sources from sources.txt file"
    )
    parser.add_argument(
        "--manifest",
        help="Manifest file: only process videos listed (docs/pipeline/batches/P001.txt)."
    )
    parser.add_argument(
        "--dry-run",
        action="store_true",
        help="Preview what would be patched without writing"
    )
    parser.add_argument(
        "--overwrite",
        action="store_true",
        help="Overwrite existing output files"
    )
    parser.add_argument(
        "--debug",
        action="store_true",
        help="Enable debug logging"
    )

    args = parser.parse_args()

    # Route to appropriate mode
    if args.test:
        _run_directory(
            test_s06_root(), test_s06b_root(), test_output_root(), args
        )
    elif args.manifest:
        _run_manifest(args)
    elif args.input:
        _run_input(args)
    elif args.sources:
        _run_sources(args)
    elif args.name:
        _run_named_source(args)
    else:
        raise SystemExit("Provide a source name, --input, --test, --manifest, or --sources")


def _run_input(args) -> None:
    input_path = Path(args.input)
    if not input_path.exists():
        input_path = repo_root() / args.input
    if not input_path.exists():
        raise SystemExit(f"Input not found: {args.input}")

    if input_path.is_file():
        out_dir = Path(args.output) if args.output else output_root()
        output_path = compute_output_path(input_path, out_dir)

        if output_path.exists() and not args.overwrite:
            print(f"{LOG_PREFIX} Output exists, skipping: {output_path}")
            return

        verification_path = find_verification_for(input_path, s06_root(), s06b_root())
        result = patch_file(input_path, verification_path, output_path, dry_run=args.dry_run)
        print(f"\n{LOG_PREFIX} Done. Verdict: {result.get('verdict')}, "
              f"Fixes: {result.get('fixes', 0)}, Flags: {result.get('flags', 0)}")
        return

    out_dir = Path(args.output) if args.output else output_root()
    _run_directory(input_path, s06b_root(), out_dir, args)


def _run_sources(args) -> None:
    sources_path = repo_root() / args.sources
    if not sources_path.exists():
        raise SystemExit(f"Sources file not found: {sources_path}")

    total_fixes = 0
    total_files = 0

    for src_name, _ in parse_sources_file(sources_path):
        src_in_dir = s06_root() / src_name
        if not src_in_dir.exists():
            print(f"{LOG_PREFIX} Skipping {src_name}: no 06.video-type output")
            continue

        src_s06b_dir = s06b_root() / src_name
        src_out_dir = output_root() / src_name
        files = find_input_files(src_in_dir)

        for input_file in files:
            output_file = compute_output_path(input_file, src_out_dir)
            if output_file.exists() and not args.overwrite:
                continue
            try:
                verification_path = find_verification_for(input_file, s06_root(), s06b_root())
                result = patch_file(input_file, verification_path, output_file, dry_run=args.dry_run)
                total_fixes += result.get("fixes", 0)
                total_files += 1
            except Exception as e:
                print(f"{LOG_PREFIX} Error: {e}")

    print(f"\n{LOG_PREFIX} Done. Patched {total_files} files, {total_fixes} total fixes")


def _run_manifest(args) -> None:
    """Run for videos listed in a manifest file."""
    manifest_path = Path(args.manifest)
    if not manifest_path.is_absolute():
        manifest_path = repo_root() / manifest_path
    if not manifest_path.exists():
        raise SystemExit(f"Manifest file not found: {manifest_path}")

    sources_map = load_manifest_sources(manifest_path)

    for src_name, vid_ids in sorted(sources_map.items()):
        src_in_dir = s06_root() / src_name
        if not src_in_dir.exists():
            print(f"{LOG_PREFIX} Skipping {src_name}: no 06.video-type output")
            continue

        src_s06b_dir = s06b_root() / src_name
        src_out_dir = output_root() / src_name
        files = manifest_filter_files(find_input_files(src_in_dir), vid_ids)
        if not files:
            print(f"{LOG_PREFIX} Skipping {src_name}: no manifest videos found in input")
            continue

        print(f"{LOG_PREFIX} Manifest: {src_name} ({len(files)} videos)")
        _run_directory_with_files(files, src_in_dir, s06b_root(), src_out_dir, args)


def _run_directory_with_files(
    files: List[Path], in_dir: Path, verify_dir: Path, out_dir: Path, args
) -> None:
    """Process a specific list of files."""
    if not files:
        return

    print(f"{LOG_PREFIX} Input : {in_dir}")
    print(f"{LOG_PREFIX} Output: {out_dir}")
    print(f"{LOG_PREFIX} Files : {len(files)}")

    state_path = out_dir / ".06c_patch_state.json"
    state = load_state(state_path)

    processed = 0
    skipped = 0
    failed = 0
    consecutive_failures = 0
    total_fixes = 0
    total_flags = 0

    for input_file in files:
        file_key = str(input_file.relative_to(in_dir))

        if file_key in state.completed_files and not args.overwrite:
            skipped += 1
            continue

        output_file = compute_output_path(input_file, out_dir)

        if output_file.exists() and not args.overwrite:
            skipped += 1
            state.completed_files.append(file_key)
            save_state(state_path, state)
            continue

        state.in_progress = file_key
        save_state(state_path, state)

        try:
            verification_path = find_verification_for(input_file, s06_root(), verify_dir)
            result = patch_file(input_file, verification_path, output_file, dry_run=args.dry_run)
            processed += 1
            consecutive_failures = 0
            total_fixes += result.get("fixes", 0)
            total_flags += result.get("flags", 0)

            if not args.dry_run:
                state.completed_files.append(file_key)
                state.in_progress = None
                save_state(state_path, state)

        except Exception as e:
            print(f"{LOG_PREFIX} Error processing {input_file}: {e}")
            state.failures.append({"file": file_key, "error": str(e)})
            state.in_progress = None
            save_state(state_path, state)
            failed += 1
            consecutive_failures += 1

            if consecutive_failures >= MAX_CONSECUTIVE_FAILURES:
                print(f"\n{LOG_PREFIX} HALTING: {consecutive_failures} consecutive failures")
                break

    print(f"\n{LOG_PREFIX} Done.")
    print(f"  Processed: {processed}")
    print(f"  Skipped:   {skipped}")
    print(f"  Failed:    {failed}")
    print(f"  Fixes:     {total_fixes}")
    print(f"  Flags:     {total_flags}")

    total_attempted = processed + failed
    if total_attempted > 0:
        failure_rate = failed / total_attempted
        if failure_rate > MAX_FAILURE_RATE and total_attempted >= 5:
            print(f"\n{LOG_PREFIX} HALTING: Failure rate {failure_rate:.0%} exceeds {MAX_FAILURE_RATE:.0%} threshold")
            sys.exit(1)


def _run_named_source(args) -> None:
    """Run for a named source (from pipeline: ./06c.patch source_name url)."""
    name = args.name
    in_dir = s06_root() / name
    if not in_dir.exists():
        raise SystemExit(f"Input directory not found: {in_dir}")

    verify_dir = s06b_root() / name
    out_dir = Path(args.output) if args.output else output_root() / name
    _run_directory(in_dir, verify_dir, out_dir, args)


def _run_directory(in_dir: Path, verify_dir: Path, out_dir: Path, args) -> None:
    files = find_input_files(in_dir)
    if not files:
        print(f"{LOG_PREFIX} No .conversations.json files found in: {in_dir}")
        return
    _run_directory_with_files(files, in_dir, verify_dir, out_dir, args)


if __name__ == "__main__":
    main()
