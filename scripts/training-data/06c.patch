#!/usr/bin/env python3
"""
scripts/training-data/06c.patch

Auto-apply high-confidence fixes from 06b verification reports.

Pure deterministic JSON patching — no LLM calls.

Reads:
  - Stage 06 output:
      data/06.video-type/<source>/<video>/*.conversations.json
  - Stage 06b verification reports:
      data/06b.verify/<source>/<video>/*.verification.json

Writes:
  - Patched conversations (same filename pattern as stage 06):
      data/06c.patched/<source>/<video>/*.conversations.json

Auto-applies:
  - misattributions: changes speaker_role on matched segments
  - collapse_issues: changes speaker_role + speaker_role_override
  - boundary_issues: applies merge/split/reclassify when suggested_fix is present and confidence is high
    (and recomputes conversations[] summary)

Flags but does NOT fix:
  - other_flags

Use:

  A) From pipeline:
     ./scripts/training-data/06c.patch "source_name" "youtube_url"

  B) Test videos:
     ./scripts/training-data/06c.patch --test

  C) Single file:
     ./scripts/training-data/06c.patch --input data/06.video-type/video.conversations.json

  D) Batch from sources file:
     ./scripts/training-data/06c.patch --sources

  E) Dry run (show what would be patched):
     ./scripts/training-data/06c.patch --input data/06.video-type/video.conversations.json --dry-run
"""

from __future__ import annotations

import argparse
import copy
import json
import shlex
import sys
import time
from dataclasses import dataclass, asdict
from pathlib import Path
from typing import Any, Dict, List, Optional, Set, Tuple

import jsonschema

from batch.manifest_parser import load_manifest, load_manifest_sources, manifest_filter_files


# ---------------------------
# Configuration
# ---------------------------

PIPELINE_VERSION = "06c.patch-v1.5"  # v1.5: validate 06b verification schema before patching

LOG_PREFIX = "[06c.patch]"

# Failure budget: halt batch if too many failures
MAX_CONSECUTIVE_FAILURES = 3
MAX_FAILURE_RATE = 0.20  # halt if >20% of videos fail

# Confidence thresholds: only auto-apply high-confidence fixes
MIN_MISATTRIBUTION_CONFIDENCE = 0.7
MIN_VIDEO_TYPE_CONFIDENCE = 0.85
MIN_BOUNDARY_CONFIDENCE = 0.90  # Higher threshold - structural changes

SCHEMA_PATH = Path(__file__).parent / "schemas" / "conversations.schema.json"
_CACHED_SCHEMA: Optional[Dict[str, Any]] = None
VERIFICATION_SCHEMA_PATH = Path(__file__).parent / "schemas" / "verification.schema.json"
_CACHED_VERIFICATION_SCHEMA: Optional[Dict[str, Any]] = None


# ---------------------------
# State Management
# ---------------------------

@dataclass
class ProcessingState:
    version: int
    completed_files: List[str]
    in_progress: Optional[str]
    failures: List[Dict[str, str]]


def load_state(state_path: Path) -> ProcessingState:
    if state_path.exists():
        try:
            data = json.loads(state_path.read_text())
            return ProcessingState(
                version=data.get("version", 1),
                completed_files=data.get("completed_files", []),
                in_progress=data.get("in_progress"),
                failures=data.get("failures", []),
            )
        except (json.JSONDecodeError, KeyError):
            pass
    return ProcessingState(version=1, completed_files=[], in_progress=None, failures=[])


def save_state(state_path: Path, state: ProcessingState) -> None:
    state_path.parent.mkdir(parents=True, exist_ok=True)
    state_path.write_text(json.dumps(asdict(state), indent=2))

def _load_schema() -> Optional[Dict[str, Any]]:
    if not SCHEMA_PATH.exists():
        print(f"{LOG_PREFIX} WARNING: Schema not found at {SCHEMA_PATH}")
        return None
    return json.loads(SCHEMA_PATH.read_text(encoding="utf-8"))


def _get_schema() -> Optional[Dict[str, Any]]:
    global _CACHED_SCHEMA
    if _CACHED_SCHEMA is None:
        _CACHED_SCHEMA = _load_schema()
    return _CACHED_SCHEMA


def _validate_conversations_schema(conversations_data: Dict[str, Any]) -> None:
    """Validate patched output against conversations.schema.json. Raises on failure."""
    schema = _get_schema()
    if not schema:
        return
    try:
        jsonschema.validate(instance=conversations_data, schema=schema)
    except jsonschema.ValidationError as e:
        path = ".".join(str(p) for p in e.absolute_path) if e.absolute_path else "(root)"
        raise RuntimeError(f"Patched conversations schema invalid at {path}: {e.message[:200]}")


def _load_verification_schema() -> Optional[Dict[str, Any]]:
    if not VERIFICATION_SCHEMA_PATH.exists():
        print(f"{LOG_PREFIX} WARNING: Verification schema not found at {VERIFICATION_SCHEMA_PATH}")
        return None
    return json.loads(VERIFICATION_SCHEMA_PATH.read_text(encoding="utf-8"))


def _get_verification_schema() -> Optional[Dict[str, Any]]:
    global _CACHED_VERIFICATION_SCHEMA
    if _CACHED_VERIFICATION_SCHEMA is None:
        _CACHED_VERIFICATION_SCHEMA = _load_verification_schema()
    return _CACHED_VERIFICATION_SCHEMA


def _validate_verification_schema(verification_data: Dict[str, Any]) -> None:
    """Validate 06b verification report against verification.schema.json. Raises on failure."""
    schema = _get_verification_schema()
    if not schema:
        return
    try:
        jsonschema.validate(instance=verification_data, schema=schema)
    except jsonschema.ValidationError as e:
        path = ".".join(str(p) for p in e.absolute_path) if e.absolute_path else "(root)"
        raise RuntimeError(f"Verification schema invalid at {path}: {e.message[:200]}")


def _recompute_conversations_summary(
    segments: List[Dict[str, Any]],
    old_conversations: Any,
) -> List[Dict[str, Any]]:
    """Rebuild conversations[] from segments[].conversation_id while preserving extra fields when possible."""
    old_by_id: Dict[int, Dict[str, Any]] = {}
    if isinstance(old_conversations, list):
        for c in old_conversations:
            if not isinstance(c, dict):
                continue
            cid = c.get("conversation_id")
            if isinstance(cid, int):
                old_by_id[cid] = c

    conv_segments: Dict[int, List[Dict[str, Any]]] = {}
    for seg in segments:
        if not isinstance(seg, dict):
            continue
        cid = seg.get("conversation_id", 0)
        if isinstance(cid, int) and cid > 0:
            conv_segments.setdefault(cid, []).append(seg)

    out: List[Dict[str, Any]] = []
    for cid, segs in sorted(conv_segments.items()):
        base = dict(old_by_id.get(cid) or {})
        target_speaker_ids: Set[str] = set()
        has_uncertain_roles = False
        for seg in segs:
            role = str(seg.get("speaker_role", "")).strip().lower()
            if role == "target":
                speaker_id = str(seg.get("speaker_id", "")).strip()
                if speaker_id:
                    target_speaker_ids.add(speaker_id)
            elif role in {"unknown", "collapsed"}:
                has_uncertain_roles = True

        target_label = "unknown"
        target_reason = "No reliable target turns detected in this conversation."
        target_confidence = 0.4
        if len(target_speaker_ids) >= 2:
            target_label = "multiple_women"
            target_reason = (
                f"Detected target turns from {len(target_speaker_ids)} distinct speaker IDs."
            )
            target_confidence = 0.95
        elif len(target_speaker_ids) == 1:
            target_label = "single_woman"
            target_reason = "Detected target turns from one consistent speaker ID."
            target_confidence = 0.9
        elif has_uncertain_roles:
            target_reason = (
                "Conversation includes unknown/collapsed role segments, so target participation is uncertain."
            )
            target_confidence = 0.3

        base["conversation_id"] = cid
        base["segment_ids"] = [
            s.get("id") for s in segs if isinstance(s.get("id"), int)
        ]
        base["start_time"] = segs[0].get("start", 0)
        base["end_time"] = segs[-1].get("end", 0)
        base["target_participation"] = {
            "label": target_label,
            "target_speaker_ids": sorted(target_speaker_ids),
            "confidence": round(target_confidence, 2),
            "reasoning": target_reason,
        }
        out.append(base)

    return out


# ---------------------------
# Patching Logic
# ---------------------------

def _merge_conversations(segments: List[Dict], from_conv: int, to_conv: int) -> int:
    """Reassign all segments from from_conv to to_conv. Returns count of segments moved."""
    count = 0
    for seg in segments:
        if seg.get("conversation_id") == from_conv:
            seg["conversation_id"] = to_conv
            count += 1
    return count


def _split_conversation(
    segments: List[Dict], conv_id: int, split_seg_id: int, new_conv_id: int
) -> int:
    """Split conversation at segment, assigning split_seg and after to new_conv_id.

    Returns count of segments reassigned.
    """
    count = 0
    in_split = False
    for seg in segments:
        if seg.get("conversation_id") != conv_id:
            continue
        if seg.get("id") == split_seg_id:
            in_split = True
        if in_split:
            seg["conversation_id"] = new_conv_id
            count += 1
    return count


def _get_next_conversation_id(segments: List[Dict]) -> int:
    """Get the next available conversation ID."""
    max_id = 0
    for seg in segments:
        conv_id = seg.get("conversation_id", 0)
        if conv_id > max_id:
            max_id = conv_id
    return max_id + 1


def apply_patches(
    conversations_data: Dict[str, Any],
    verification_data: Dict[str, Any],
) -> Tuple[List[Dict], List[str]]:
    """Apply high-confidence fixes from verification report to conversations data.

    Returns (fixes_applied, flags_not_fixed).
    """
    segments = conversations_data.get("segments", [])
    # Build segment index by id for fast lookup
    seg_by_id: Dict[int, Dict] = {}
    for seg in segments:
        seg_by_id[seg.get("id", -1)] = seg

    fixes_applied: List[Dict] = []
    flags_not_fixed: List[str] = []

    # --- Apply misattribution fixes ---
    for entry in verification_data.get("misattributions", []):
        seg_id = entry.get("segment_id")
        current_role = entry.get("current_role")
        suggested_role = entry.get("suggested_role")
        evidence = entry.get("evidence", "")
        confidence = entry.get("confidence", 1.0)  # Default 1.0 for backwards compat

        if seg_id is None or suggested_role is None:
            flags_not_fixed.append(
                f"misattribution: incomplete entry (seg_id={seg_id}, suggested={suggested_role})"
            )
            continue

        # Skip prose suggested_roles (should be schema-valid role values)
        valid_roles = {"coach", "student", "target", "other", "voiceover", "unknown"}
        if suggested_role not in valid_roles:
            flags_not_fixed.append(
                f"misattribution: segment {seg_id} suggested_role '{suggested_role}' is not a valid role — skipping"
            )
            continue

        seg = seg_by_id.get(seg_id)
        if seg is None:
            flags_not_fixed.append(
                f"misattribution: segment {seg_id} not found in conversations data"
            )
            continue

        actual_role = seg.get("speaker_role")
        if current_role and actual_role != current_role:
            flags_not_fixed.append(
                f"misattribution: segment {seg_id} current_role mismatch "
                f"(expected '{current_role}', found '{actual_role}') — skipping"
            )
            continue

        if confidence < MIN_MISATTRIBUTION_CONFIDENCE:
            flags_not_fixed.append(
                f"misattribution: segment {seg_id} confidence {confidence:.0%} below threshold — skipping"
            )
            continue

        old_role = seg["speaker_role"]
        old_override = seg.get("speaker_role_override")
        seg["speaker_role"] = suggested_role
        if old_override is not None:
            seg["speaker_role_override"] = suggested_role
        fixes_applied.append({
            "segment_id": seg_id,
            "field": "speaker_role" + ("+speaker_role_override" if old_override else ""),
            "old": f"role={old_role}" + (f", override={old_override}" if old_override else ""),
            "new": suggested_role,
            "source": "misattribution",
            "evidence": evidence[:200],
        })

    # --- Apply collapse issue fixes ---
    for entry in verification_data.get("collapse_issues", []):
        seg_id = entry.get("segment_id")
        reported_speaker_id = entry.get("speaker_id")
        suggested_override = entry.get("suggested_override")
        current_override = entry.get("current_override")
        evidence = entry.get("evidence", "")
        confidence = entry.get("confidence", 1.0)  # Default 1.0 for backwards compat

        if seg_id is None or suggested_override is None:
            flags_not_fixed.append(
                f"collapse_issue: incomplete entry (seg_id={seg_id}, suggested={suggested_override})"
            )
            continue

        # Skip prose suggested_overrides (should be schema-valid role values)
        valid_roles = {"coach", "student", "target", "other", "voiceover", "unknown"}
        if suggested_override not in valid_roles:
            flags_not_fixed.append(
                f"collapse_issue: segment {seg_id} suggested_override '{suggested_override}' is not a valid role — skipping"
            )
            continue

        seg = seg_by_id.get(seg_id)
        if seg is None:
            flags_not_fixed.append(
                f"collapse_issue: segment {seg_id} not found in conversations data"
            )
            continue

        # Only apply collapse_issues to segments whose speaker is actually marked as collapsed.
        seg_speaker_id = seg.get("speaker_id")
        speaker_labels = conversations_data.get("speaker_labels", {}) or {}
        collapse_meta = conversations_data.get("speaker_collapse") or {}
        collapsed_ids = set(collapse_meta.get("collapsed_speakers", [])) if isinstance(collapse_meta, dict) and collapse_meta.get("detected") else set()
        is_collapsed_speaker = False
        if seg_speaker_id and seg_speaker_id in collapsed_ids:
            is_collapsed_speaker = True
        elif seg_speaker_id and isinstance(speaker_labels, dict):
            is_collapsed_speaker = speaker_labels.get(seg_speaker_id, {}).get("role") == "collapsed"

        if not is_collapsed_speaker:
            flags_not_fixed.append(
                f"collapse_issue: segment {seg_id} speaker_id '{seg_speaker_id}' is not marked collapsed — skipping"
            )
            continue

        if reported_speaker_id and seg_speaker_id and reported_speaker_id != seg_speaker_id:
            flags_not_fixed.append(
                f"collapse_issue: segment {seg_id} speaker_id mismatch "
                f"(expected '{reported_speaker_id}', found '{seg_speaker_id}') — skipping"
            )
            continue

        actual_override = seg.get("speaker_role_override")
        actual_role = seg.get("speaker_role")

        # Lenient current_override matching: treat null, None, "none", "none (raw label)", "" as equivalent
        def normalize_override(val):
            if val is None or val == "" or (isinstance(val, str) and val.lower().startswith("none")):
                return None
            return val

        normalized_current = normalize_override(current_override)
        normalized_actual = normalize_override(actual_override)

        # Only check mismatch if current_override was specified and non-null
        if normalized_current is not None and normalized_actual != normalized_current and actual_role != normalized_current:
            flags_not_fixed.append(
                f"collapse_issue: segment {seg_id} current_override mismatch "
                f"(expected '{current_override}', found override='{actual_override}', role='{actual_role}') — skipping"
            )
            continue

        if confidence < MIN_MISATTRIBUTION_CONFIDENCE:
            flags_not_fixed.append(
                f"collapse_issue: segment {seg_id} confidence {confidence:.0%} below threshold — skipping"
            )
            continue

        old_role = seg.get("speaker_role")
        old_override = seg.get("speaker_role_override")
        seg["speaker_role"] = suggested_override
        seg["speaker_role_override"] = suggested_override
        fixes_applied.append({
            "segment_id": seg_id,
            "field": "speaker_role+speaker_role_override",
            "old": f"role={old_role}, override={old_override}",
            "new": suggested_override,
            "source": "collapse_issue",
            "evidence": evidence[:200],
        })

    # --- Apply video type fix ---
    video_type_check = verification_data.get("video_type_check", {})
    if not video_type_check.get("agrees", True):
        suggested_type = video_type_check.get("suggested_type")
        confidence = video_type_check.get("confidence", 0)
        reasoning = video_type_check.get("reasoning", "")

        if suggested_type and confidence >= MIN_VIDEO_TYPE_CONFIDENCE:
            video_type_obj = conversations_data.get("video_type", {})
            old_type = video_type_obj.get("type") if isinstance(video_type_obj, dict) else video_type_obj
            if isinstance(video_type_obj, dict):
                conversations_data["video_type"]["type"] = suggested_type
            else:
                conversations_data["video_type"] = {"type": suggested_type}
            fixes_applied.append({
                "field": "video_type",
                "old": old_type,
                "new": suggested_type,
                "source": "video_type_check",
                "confidence": confidence,
                "reasoning": reasoning[:200],
            })
        elif suggested_type:
            flags_not_fixed.append(
                f"video_type: suggested '{suggested_type}' but confidence {confidence:.0%} "
                f"below {MIN_VIDEO_TYPE_CONFIDENCE:.0%} threshold"
            )

    # --- Apply boundary fixes (or log as unfixed) ---
    for entry in verification_data.get("boundary_issues", []):
        conv_id = entry.get("conversation_id")
        severity = entry.get("severity", "unknown")
        issue = entry.get("issue", "")
        suggested_fix = entry.get("suggested_fix")
        confidence = entry.get("confidence", 0)

        # Skip if no fix suggested or confidence too low
        if not suggested_fix or confidence < MIN_BOUNDARY_CONFIDENCE:
            reason = "no fix suggested" if not suggested_fix else f"confidence {confidence:.0%} below {MIN_BOUNDARY_CONFIDENCE:.0%}"
            flags_not_fixed.append(
                f"boundary_issue [{severity}]: Conv {conv_id} — {issue[:100]} ({reason})"
            )
            continue

        # Apply the fix
        if suggested_fix == "merge_with_next":
            target_conv = conv_id + 1
            count = _merge_conversations(segments, conv_id, target_conv)
            if count > 0:
                fixes_applied.append({
                    "field": "conversation_id",
                    "action": "merge",
                    "from_conv": conv_id,
                    "to_conv": target_conv,
                    "segments_moved": count,
                    "source": "boundary_issue",
                    "confidence": confidence,
                    "issue": issue[:150],
                })
            else:
                flags_not_fixed.append(
                    f"boundary_issue: merge_with_next for conv {conv_id} found no segments"
                )

        elif suggested_fix == "merge_with_previous":
            target_conv = conv_id - 1
            if target_conv < 1:
                flags_not_fixed.append(
                    f"boundary_issue: merge_with_previous for conv {conv_id} invalid (no previous)"
                )
                continue
            count = _merge_conversations(segments, conv_id, target_conv)
            if count > 0:
                fixes_applied.append({
                    "field": "conversation_id",
                    "action": "merge",
                    "from_conv": conv_id,
                    "to_conv": target_conv,
                    "segments_moved": count,
                    "source": "boundary_issue",
                    "confidence": confidence,
                    "issue": issue[:150],
                })
            else:
                flags_not_fixed.append(
                    f"boundary_issue: merge_with_previous for conv {conv_id} found no segments"
                )

        elif suggested_fix.startswith("split_at_segment_"):
            try:
                split_seg_id = int(suggested_fix.split("_")[-1])
            except ValueError:
                flags_not_fixed.append(
                    f"boundary_issue: invalid split format '{suggested_fix}'"
                )
                continue

            new_conv_id = _get_next_conversation_id(segments)
            count = _split_conversation(segments, conv_id, split_seg_id, new_conv_id)
            if count > 0:
                fixes_applied.append({
                    "field": "conversation_id",
                    "action": "split",
                    "original_conv": conv_id,
                    "split_at_segment": split_seg_id,
                    "new_conv_id": new_conv_id,
                    "segments_moved": count,
                    "source": "boundary_issue",
                    "confidence": confidence,
                    "issue": issue[:150],
                })
            else:
                flags_not_fixed.append(
                    f"boundary_issue: split_at_segment_{split_seg_id} for conv {conv_id} found no segments"
                )

        elif suggested_fix == "reclassify_as_commentary":
            # Move all segments from this conversation to commentary (conv_id=0)
            count = _merge_conversations(segments, conv_id, 0)
            if count > 0:
                fixes_applied.append({
                    "field": "conversation_id",
                    "action": "reclassify_as_commentary",
                    "from_conv": conv_id,
                    "to_conv": 0,
                    "segments_moved": count,
                    "source": "boundary_issue",
                    "confidence": confidence,
                    "issue": issue[:150],
                })
            else:
                flags_not_fixed.append(
                    f"boundary_issue: reclassify_as_commentary for conv {conv_id} found no segments"
                )

        else:
            flags_not_fixed.append(
                f"boundary_issue [{severity}]: Conv {conv_id} — unknown fix type '{suggested_fix}'"
            )

    # --- Log other_flags as unfixed ---
    for flag in verification_data.get("other_flags", []):
        flags_not_fixed.append(f"other_flag: {str(flag)[:150]}")

    return fixes_applied, flags_not_fixed


def patch_file(
    conversations_path: Path,
    verification_path: Optional[Path],
    output_path: Path,
    dry_run: bool = False,
    allow_reject: bool = False,
    allow_invalid_verification: bool = False,
) -> Dict[str, Any]:
    """Patch a single conversations file using its verification report."""

    print(f"\n{LOG_PREFIX} Patching: {conversations_path.name}")

    with conversations_path.open("r", encoding="utf-8") as f:
        conversations_data = json.load(f)

    segment_count = len(conversations_data.get("segments", []))
    print(f"{LOG_PREFIX}   Segments: {segment_count}")

    # Deep copy so we don't modify original
    patched = copy.deepcopy(conversations_data)

    if verification_path is None or not verification_path.exists():
        # No verification file — copy unchanged
        print(f"{LOG_PREFIX}   No verification report found — copying unchanged")

        patched["patch_metadata"] = {
            "patched_at": time.strftime("%Y-%m-%dT%H:%M:%SZ"),
            "pipeline_version": PIPELINE_VERSION,
            "verification_verdict": None,
            "verification_file": None,
            "fixes_applied": [],
            "fixes_applied_count": 0,
            "flags_not_fixed": [],
            "flags_not_fixed_count": 0,
            "note": "No verification report found — copied unchanged",
        }

        # Even when copying unchanged, ensure the output is schema-valid.
        _validate_conversations_schema(patched)

        if not dry_run:
            output_path.parent.mkdir(parents=True, exist_ok=True)
            with output_path.open("w", encoding="utf-8") as f:
                json.dump(patched, f, indent=2, ensure_ascii=False)
            print(f"{LOG_PREFIX}   Output: {output_path}")
        else:
            print(f"{LOG_PREFIX}   [DRY RUN] Would copy unchanged to {output_path}")

        return {"verdict": None, "fixes": 0, "flags": 0}

    # Load verification report
    with verification_path.open("r", encoding="utf-8") as f:
        verification_data = json.load(f)
    verification_schema_valid = True
    try:
        _validate_verification_schema(verification_data)
    except RuntimeError as e:
        verification_schema_valid = False
        if not allow_invalid_verification:
            raise RuntimeError(
                f"Verification report schema invalid for {conversations_path.name}: {e}. "
                "Fix/re-run 06b.verify, or use --allow-invalid-verification for legacy reports."
            )
        print(f"{LOG_PREFIX}   WARNING: {e}")
        print(f"{LOG_PREFIX}   WARNING: Continuing due to --allow-invalid-verification")

    verdict = verification_data.get("verdict", "FLAG")
    print(f"{LOG_PREFIX}   Verification verdict: {verdict}")
    print(f"{LOG_PREFIX}   Verification file: {verification_path}")

    if verdict == "REJECT" and not allow_reject:
        raise RuntimeError(
            f"Verification verdict is REJECT for {conversations_path.name}. "
            "By default 06c refuses to patch REJECT outputs. Re-run with --allow-reject if you want to apply "
            "high-confidence deterministic fixes anyway (useful for iteration / salvage)."
        )

    if verdict == "APPROVE":
        # Copy unchanged, just add metadata
        fixes_applied: List[Dict] = []
        flags_not_fixed: List[str] = []
    else:
        # FLAG/REJECT — apply patches
        fixes_applied, flags_not_fixed = apply_patches(patched, verification_data)

    # Always recompute conversations[] from segments after patching to keep semantic consistency,
    # especially when boundary fixes are auto-applied.
    patched["conversations"] = _recompute_conversations_summary(
        patched.get("segments", []),
        patched.get("conversations"),
    )

    patched["patch_metadata"] = {
        "patched_at": time.strftime("%Y-%m-%dT%H:%M:%SZ"),
        "pipeline_version": PIPELINE_VERSION,
        "verification_verdict": verdict,
        "verification_file": str(verification_path),
        "verification_schema_valid": verification_schema_valid,
        "fixes_applied": fixes_applied,
        "fixes_applied_count": len(fixes_applied),
        "flags_not_fixed": flags_not_fixed,
        "flags_not_fixed_count": len(flags_not_fixed),
    }

    # Fail fast if patched output violates the Stage 06 conversations schema.
    _validate_conversations_schema(patched)

    # Print summary
    if fixes_applied:
        print(f"{LOG_PREFIX}   Fixes applied: {len(fixes_applied)}")
        for fix in fixes_applied:
            src = fix.get("source", "unknown")
            if "segment_id" in fix:
                seg_id = fix.get("segment_id")
                print(f"{LOG_PREFIX}     seg {seg_id}: {fix.get('old')} → {fix.get('new')} ({src})")
                continue

            action = fix.get("action")
            if action == "merge":
                print(
                    f"{LOG_PREFIX}     merge conv {fix.get('from_conv')} → {fix.get('to_conv')} "
                    f"({fix.get('segments_moved')} segs) ({src})"
                )
                continue
            if action == "split":
                print(
                    f"{LOG_PREFIX}     split conv {fix.get('original_conv')} at seg {fix.get('split_at_segment')} "
                    f"→ conv {fix.get('new_conv_id')} ({fix.get('segments_moved')} segs) ({src})"
                )
                continue
            if action == "reclassify_as_commentary":
                print(
                    f"{LOG_PREFIX}     reclassify conv {fix.get('from_conv')} → commentary "
                    f"({fix.get('segments_moved')} segs) ({src})"
                )
                continue

            field = fix.get("field", "unknown_field")
            if "old" in fix or "new" in fix:
                print(f"{LOG_PREFIX}     {field}: {fix.get('old')} → {fix.get('new')} ({src})")
            else:
                print(f"{LOG_PREFIX}     {field}: {fix} ({src})")
    if flags_not_fixed:
        print(f"{LOG_PREFIX}   Flags not fixed: {len(flags_not_fixed)}")
        for flag in flags_not_fixed:
            print(f"{LOG_PREFIX}     {flag[:120]}")

    if not dry_run:
        output_path.parent.mkdir(parents=True, exist_ok=True)
        with output_path.open("w", encoding="utf-8") as f:
            json.dump(patched, f, indent=2, ensure_ascii=False)
        print(f"{LOG_PREFIX}   Output: {output_path}")
    else:
        print(f"{LOG_PREFIX}   [DRY RUN] Would write to {output_path}")

    return {
        "verdict": verdict,
        "fixes": len(fixes_applied),
        "flags": len(flags_not_fixed),
    }


# ---------------------------
# Path helpers
# ---------------------------

def repo_root() -> Path:
    return Path(__file__).resolve().parents[2]


def s06_root() -> Path:
    return repo_root() / "data" / "06.video-type"


def s06b_root() -> Path:
    return repo_root() / "data" / "06b.verify"


def output_root() -> Path:
    return repo_root() / "data" / "06c.patched"


def test_s06_root() -> Path:
    return repo_root() / "data" / "test" / "06.video-type"


def test_s06b_root() -> Path:
    return repo_root() / "data" / "test" / "06b.verify"


def test_output_root() -> Path:
    return repo_root() / "data" / "test" / "06c.patched"


def resolve_root_path(raw_path: Optional[str], default_root: Path) -> Path:
    if not raw_path:
        return default_root
    path = Path(raw_path)
    if not path.is_absolute():
        path = repo_root() / path
    return path


def compute_output_path(input_path: Path, output_dir: Path) -> Path:
    """Compute output path — same *.conversations.json pattern as stage 06."""
    stem = input_path.stem
    # Ensure the output has .conversations.json extension
    if stem.endswith(".conversations"):
        filename = f"{stem}.json"
    else:
        filename = f"{stem}.conversations.json"
    return output_dir / filename


def compute_output_path_with_layout(
    input_path: Path,
    output_dir: Path,
    input_root: Optional[Path] = None,
) -> Path:
    canonical = compute_output_path(input_path, output_dir)
    if input_root is not None:
        try:
            rel_parent = input_path.parent.relative_to(input_root)
            if rel_parent != Path("."):
                return output_dir / rel_parent / canonical.name
        except ValueError:
            pass
    return canonical


def find_existing_output_path(
    input_path: Path,
    preferred_output_dir: Path,
    output_root_dir: Optional[Path] = None,
    input_root: Optional[Path] = None,
) -> Optional[Path]:
    preferred = compute_output_path_with_layout(
        input_path,
        preferred_output_dir,
        input_root=input_root,
    )
    if preferred.exists():
        return preferred
    # Legacy source-flat fallback.
    source_flat = compute_output_path(input_path, preferred_output_dir)
    if source_flat.exists():
        return source_flat
    # Legacy root-flat fallback.
    if output_root_dir is not None:
        root_flat = compute_output_path(input_path, output_root_dir)
        if root_flat.exists():
            return root_flat
    return None


def find_verification_for(
    conversations_path: Path, s06_dir: Path, s06b_dir: Path
) -> Optional[Path]:
    """Find the matching verification JSON for a conversations file.

    Tries two strategies:
    1. Mirror the relative path from s06_dir into s06b_dir, swapping extension
    2. Flat lookup in s06b_dir root (for files not in source subdirectories)
    """
    stem = conversations_path.stem
    if stem.endswith(".conversations"):
        stem = stem[:-len(".conversations")]
    verification_name = f"{stem}.verification.json"

    # Strategy 1: Mirror relative path
    try:
        rel = conversations_path.parent.relative_to(s06_dir)
        candidate = s06b_dir / rel / verification_name
        if candidate.exists():
            return candidate
    except ValueError:
        pass

    # Strategy 2: Flat lookup at s06b root
    candidate = s06b_dir / verification_name
    if candidate.exists():
        return candidate

    # Strategy 3: Search recursively in s06b_dir
    for found in s06b_dir.rglob(verification_name):
        return found

    return None


def parse_sources_file(path: Path) -> List[Tuple[str, str]]:
    sources: List[Tuple[str, str]] = []
    for raw in path.read_text(encoding="utf-8").splitlines():
        line = raw.strip()
        if not line or line.startswith("#"):
            continue
        if "|" in line:
            name, url = line.split("|", 1)
            sources.append((name.strip(), url.strip()))
            continue
        parts = shlex.split(line)
        if len(parts) >= 2:
            sources.append((parts[0], parts[1]))
    return sources


def find_input_files(in_dir: Path) -> List[Path]:
    return sorted(in_dir.rglob("*.conversations.json"))


# ---------------------------
# CLI
# ---------------------------

def main() -> None:
    parser = argparse.ArgumentParser(
        description="Auto-apply high-confidence fixes from 06b verification to stage 06 output"
    )
    parser.add_argument(
        "name", nargs="?",
        help="Source name (folder under data/06.video-type/)"
    )
    parser.add_argument(
        "youtube_url", nargs="?",
        help="YouTube URL (unused, accepted for pipeline compatibility)"
    )
    parser.add_argument(
        "--input",
        help="Input .conversations.json file or directory"
    )
    parser.add_argument(
        "--input-root",
        help=(
            "Root directory for source/manifest runs "
            "(default: data/06.video-type, or data/test/06.video-type with --test)"
        ),
    )
    parser.add_argument(
        "--verification-root",
        help=(
            "Root directory containing Stage 06b verification reports "
            "(default: data/06b.verify, or data/test/06b.verify with --test)"
        ),
    )
    parser.add_argument(
        "--output",
        help="Output directory (defaults to data/06c.patched/)"
    )
    parser.add_argument(
        "--test",
        action="store_true",
        help="Process test videos (data/test/06.video-type/)"
    )
    parser.add_argument(
        "--sources",
        nargs="?",
        const="docs/pipeline/sources.txt",
        help="Process all sources from sources.txt file"
    )
    parser.add_argument(
        "--manifest",
        help="Manifest file: only process videos listed (docs/pipeline/batches/P001.txt)."
    )
    parser.add_argument(
        "--dry-run",
        action="store_true",
        help="Preview what would be patched without writing"
    )
    parser.add_argument(
        "--overwrite",
        action="store_true",
        help="Overwrite existing output files"
    )
    parser.add_argument(
        "--allow-reject",
        action="store_true",
        help="Allow patching when 06b verdict is REJECT (apply high-confidence deterministic fixes anyway)"
    )
    parser.add_argument(
        "--allow-invalid-verification",
        action="store_true",
        help="Allow patching with schema-invalid 06b verification reports (legacy fallback; unsafe)",
    )
    parser.add_argument(
        "--debug",
        action="store_true",
        help="Enable debug logging"
    )

    args = parser.parse_args()

    if args.test:
        in_root = resolve_root_path(args.input_root, test_s06_root())
        verify_root = resolve_root_path(args.verification_root, test_s06b_root())
        out_root = Path(args.output) if args.output else test_output_root()
    else:
        in_root = resolve_root_path(args.input_root, s06_root())
        verify_root = resolve_root_path(args.verification_root, s06b_root())
        out_root = Path(args.output) if args.output else output_root()

    # Route to appropriate mode
    if args.test:
        _run_directory(
            in_root,
            verify_root,
            out_root,
            args,
            verification_input_root=in_root,
        )
    elif args.manifest:
        _run_manifest(args, in_root, verify_root, out_root)
    elif args.input:
        _run_input(args, in_root, verify_root, out_root)
    elif args.sources:
        _run_sources(args, in_root, verify_root, out_root)
    elif args.name:
        _run_named_source(args, in_root, verify_root, out_root)
    else:
        raise SystemExit("Provide a source name, --input, --test, --manifest, or --sources")


def _run_input(args, in_base: Path, verify_root: Path, out_base: Path) -> None:
    input_path = Path(args.input)
    if not input_path.exists():
        input_path = repo_root() / args.input
    if not input_path.exists():
        raise SystemExit(f"Input not found: {args.input}")

    if input_path.is_file():
        out_dir = Path(args.output) if args.output else out_base
        output_path = compute_output_path_with_layout(input_path, out_dir, input_root=in_base)

        if output_path.exists() and not args.overwrite:
            print(f"{LOG_PREFIX} Output exists, skipping: {output_path}")
            return

        verification_path = find_verification_for(input_path, in_base, verify_root)
        result = patch_file(
            input_path,
            verification_path,
            output_path,
            dry_run=args.dry_run,
            allow_reject=args.allow_reject,
            allow_invalid_verification=args.allow_invalid_verification,
        )
        print(f"\n{LOG_PREFIX} Done. Verdict: {result.get('verdict')}, "
              f"Fixes: {result.get('fixes', 0)}, Flags: {result.get('flags', 0)}")
        return

    out_dir = Path(args.output) if args.output else out_base
    _run_directory(
        input_path,
        verify_root,
        out_dir,
        args,
        verification_input_root=in_base,
    )


def _run_sources(args, in_base: Path, verify_root: Path, out_base: Path) -> None:
    sources_path = repo_root() / args.sources
    if not sources_path.exists():
        raise SystemExit(f"Sources file not found: {sources_path}")

    total_fixes = 0
    total_files = 0

    for src_name, _ in parse_sources_file(sources_path):
        src_in_dir = in_base / src_name
        if not src_in_dir.exists():
            print(f"{LOG_PREFIX} Skipping {src_name}: no 06.video-type output")
            continue

        src_out_dir = out_base / src_name
        files = find_input_files(src_in_dir)

        for input_file in files:
            preferred_output = compute_output_path_with_layout(
                input_file,
                src_out_dir,
                input_root=src_in_dir,
            )
            existing_output = find_existing_output_path(
                input_file,
                src_out_dir,
                output_root_dir=out_base,
                input_root=src_in_dir,
            )
            if existing_output and not args.overwrite:
                continue
            try:
                verification_path = find_verification_for(input_file, in_base, verify_root)
                result = patch_file(
                    input_file,
                    verification_path,
                    preferred_output,
                    dry_run=args.dry_run,
                    allow_reject=args.allow_reject,
                    allow_invalid_verification=args.allow_invalid_verification,
                )
                total_fixes += result.get("fixes", 0)
                total_files += 1
            except Exception as e:
                print(f"{LOG_PREFIX} Error: {e}")

    print(f"\n{LOG_PREFIX} Done. Patched {total_files} files, {total_fixes} total fixes")


def _run_manifest(args, in_base: Path, verify_root: Path, out_base: Path) -> None:
    """Run for videos listed in a manifest file."""
    manifest_path = Path(args.manifest)
    if not manifest_path.is_absolute():
        manifest_path = repo_root() / manifest_path
    if not manifest_path.exists():
        raise SystemExit(f"Manifest file not found: {manifest_path}")

    sources_map = load_manifest_sources(manifest_path)

    for src_name, vid_ids in sorted(sources_map.items()):
        src_in_dir = in_base / src_name
        if not src_in_dir.exists():
            print(f"{LOG_PREFIX} Skipping {src_name}: no 06.video-type output")
            continue

        src_out_dir = out_base / src_name
        files = manifest_filter_files(find_input_files(src_in_dir), vid_ids)
        if not files:
            print(f"{LOG_PREFIX} Skipping {src_name}: no manifest videos found in input")
            continue

        print(f"{LOG_PREFIX} Manifest: {src_name} ({len(files)} videos)")
        _run_directory_with_files(
            files,
            src_in_dir,
            verify_root,
            src_out_dir,
            args,
            output_root_dir=out_base,
            verification_input_root=in_base,
        )


def _run_directory_with_files(
    files: List[Path],
    in_dir: Path,
    verify_dir: Path,
    out_dir: Path,
    args,
    output_root_dir: Optional[Path] = None,
    verification_input_root: Optional[Path] = None,
) -> None:
    """Process a specific list of files."""
    if not files:
        return

    print(f"{LOG_PREFIX} Input : {in_dir}")
    print(f"{LOG_PREFIX} Output: {out_dir}")
    print(f"{LOG_PREFIX} Files : {len(files)}")

    state_path = out_dir / ".06c_patch_state.json"
    state = load_state(state_path)

    processed = 0
    skipped = 0
    failed = 0
    consecutive_failures = 0
    total_fixes = 0
    total_flags = 0

    for input_file in files:
        file_key = str(input_file.relative_to(in_dir))

        if file_key in state.completed_files and not args.overwrite:
            skipped += 1
            continue

        preferred_output = compute_output_path_with_layout(
            input_file,
            out_dir,
            input_root=in_dir,
        )
        existing_output = find_existing_output_path(
            input_file,
            out_dir,
            output_root_dir=output_root_dir,
            input_root=in_dir,
        )

        if existing_output and not args.overwrite:
            skipped += 1
            state.completed_files.append(file_key)
            save_state(state_path, state)
            continue

        state.in_progress = file_key
        save_state(state_path, state)

        try:
            verification_path = find_verification_for(
                input_file,
                verification_input_root or in_dir,
                verify_dir,
            )
            result = patch_file(
                input_file,
                verification_path,
                preferred_output,
                dry_run=args.dry_run,
                allow_reject=args.allow_reject,
                allow_invalid_verification=args.allow_invalid_verification,
            )
            processed += 1
            consecutive_failures = 0
            total_fixes += result.get("fixes", 0)
            total_flags += result.get("flags", 0)

            if not args.dry_run:
                state.completed_files.append(file_key)
                state.in_progress = None
                save_state(state_path, state)

        except Exception as e:
            print(f"{LOG_PREFIX} Error processing {input_file}: {e}")
            state.failures.append({"file": file_key, "error": str(e)})
            state.in_progress = None
            save_state(state_path, state)
            failed += 1
            consecutive_failures += 1

            if consecutive_failures >= MAX_CONSECUTIVE_FAILURES:
                print(f"\n{LOG_PREFIX} HALTING: {consecutive_failures} consecutive failures")
                break

    print(f"\n{LOG_PREFIX} Done.")
    print(f"  Processed: {processed}")
    print(f"  Skipped:   {skipped}")
    print(f"  Failed:    {failed}")
    print(f"  Fixes:     {total_fixes}")
    print(f"  Flags:     {total_flags}")

    total_attempted = processed + failed
    if total_attempted > 0:
        failure_rate = failed / total_attempted
        if failure_rate > MAX_FAILURE_RATE and total_attempted >= 5:
            print(f"\n{LOG_PREFIX} HALTING: Failure rate {failure_rate:.0%} exceeds {MAX_FAILURE_RATE:.0%} threshold")
            sys.exit(1)


def _run_named_source(args, in_base: Path, verify_root: Path, out_base: Path) -> None:
    """Run for a named source (from pipeline: ./06c.patch source_name url)."""
    name = args.name
    in_dir = in_base / name
    if not in_dir.exists():
        raise SystemExit(f"Input directory not found: {in_dir}")

    out_dir = Path(args.output) if args.output else out_base / name
    _run_directory(
        in_dir,
        verify_root,
        out_dir,
        args,
        verification_input_root=in_base,
    )


def _run_directory(
    in_dir: Path,
    verify_dir: Path,
    out_dir: Path,
    args,
    verification_input_root: Optional[Path] = None,
) -> None:
    files = find_input_files(in_dir)
    if not files:
        print(f"{LOG_PREFIX} No .conversations.json files found in: {in_dir}")
        return
    _run_directory_with_files(
        files,
        in_dir,
        verify_dir,
        out_dir,
        args,
        verification_input_root=verification_input_root,
    )


if __name__ == "__main__":
    main()
