#!/usr/bin/env python3
"""
scripts/training-data/06c.patch

Auto-apply high-confidence fixes from 06b verification reports.

Pure deterministic JSON patching — no LLM calls.

Reads:
  - Stage 06 output:
      data/06.video-type/<source>/<video>/*.conversations.json
  - Stage 06b verification reports:
      data/06b.verify/<source>/<video>/*.verification.json

Writes:
  - Patched conversations (same filename pattern as stage 06):
      data/06c.patched/<source>/<video>/*.conversations.json

Auto-applies:
  - misattributions: changes speaker_role on matched segments
  - collapse_issues: changes speaker_role + speaker_role_override

Flags but does NOT fix:
  - boundary_issues
  - other_flags

Use:

  A) From pipeline:
     ./scripts/training-data/06c.patch "source_name" "youtube_url"

  B) Test videos:
     ./scripts/training-data/06c.patch --test

  C) Single file:
     ./scripts/training-data/06c.patch --input data/06.video-type/video.conversations.json

  D) Batch from sources file:
     ./scripts/training-data/06c.patch --sources

  E) Dry run (show what would be patched):
     ./scripts/training-data/06c.patch --input data/06.video-type/video.conversations.json --dry-run
"""

from __future__ import annotations

import argparse
import copy
import json
import shlex
import sys
import time
from dataclasses import dataclass, asdict
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple

from pipeline_manifest import load_manifest, load_manifest_sources, manifest_filter_files


# ---------------------------
# Configuration
# ---------------------------

PIPELINE_VERSION = "06c.patch-v1.0"

LOG_PREFIX = "[06c.patch]"

# Failure budget: halt batch if too many failures
MAX_CONSECUTIVE_FAILURES = 3
MAX_FAILURE_RATE = 0.20  # halt if >20% of videos fail


# ---------------------------
# State Management
# ---------------------------

@dataclass
class ProcessingState:
    version: int
    completed_files: List[str]
    in_progress: Optional[str]
    failures: List[Dict[str, str]]


def load_state(state_path: Path) -> ProcessingState:
    if state_path.exists():
        try:
            data = json.loads(state_path.read_text())
            return ProcessingState(
                version=data.get("version", 1),
                completed_files=data.get("completed_files", []),
                in_progress=data.get("in_progress"),
                failures=data.get("failures", []),
            )
        except (json.JSONDecodeError, KeyError):
            pass
    return ProcessingState(version=1, completed_files=[], in_progress=None, failures=[])


def save_state(state_path: Path, state: ProcessingState) -> None:
    state_path.parent.mkdir(parents=True, exist_ok=True)
    state_path.write_text(json.dumps(asdict(state), indent=2))


# ---------------------------
# Patching Logic
# ---------------------------

def apply_patches(
    conversations_data: Dict[str, Any],
    verification_data: Dict[str, Any],
) -> Tuple[List[Dict], List[str]]:
    """Apply high-confidence fixes from verification report to conversations data.

    Returns (fixes_applied, flags_not_fixed).
    """
    segments = conversations_data.get("segments", [])
    # Build segment index by id for fast lookup
    seg_by_id: Dict[int, Dict] = {}
    for seg in segments:
        seg_by_id[seg.get("id", -1)] = seg

    fixes_applied: List[Dict] = []
    flags_not_fixed: List[str] = []

    # --- Apply misattribution fixes ---
    for entry in verification_data.get("misattributions", []):
        seg_id = entry.get("segment_id")
        current_role = entry.get("current_role")
        suggested_role = entry.get("suggested_role")
        evidence = entry.get("evidence", "")

        if seg_id is None or suggested_role is None:
            flags_not_fixed.append(
                f"misattribution: incomplete entry (seg_id={seg_id}, suggested={suggested_role})"
            )
            continue

        seg = seg_by_id.get(seg_id)
        if seg is None:
            flags_not_fixed.append(
                f"misattribution: segment {seg_id} not found in conversations data"
            )
            continue

        actual_role = seg.get("speaker_role")
        if current_role and actual_role != current_role:
            flags_not_fixed.append(
                f"misattribution: segment {seg_id} current_role mismatch "
                f"(expected '{current_role}', found '{actual_role}') — skipping"
            )
            continue

        old_role = seg["speaker_role"]
        seg["speaker_role"] = suggested_role
        fixes_applied.append({
            "segment_id": seg_id,
            "field": "speaker_role",
            "old": old_role,
            "new": suggested_role,
            "source": "misattribution",
            "evidence": evidence[:200],
        })

    # --- Apply collapse issue fixes ---
    for entry in verification_data.get("collapse_issues", []):
        seg_id = entry.get("segment_id")
        suggested_override = entry.get("suggested_override")
        current_override = entry.get("current_override")
        evidence = entry.get("evidence", "")

        if seg_id is None or suggested_override is None:
            flags_not_fixed.append(
                f"collapse_issue: incomplete entry (seg_id={seg_id}, suggested={suggested_override})"
            )
            continue

        seg = seg_by_id.get(seg_id)
        if seg is None:
            flags_not_fixed.append(
                f"collapse_issue: segment {seg_id} not found in conversations data"
            )
            continue

        actual_override = seg.get("speaker_role_override")
        actual_role = seg.get("speaker_role")
        if current_override and actual_override != current_override and actual_role != current_override:
            flags_not_fixed.append(
                f"collapse_issue: segment {seg_id} current_override mismatch "
                f"(expected '{current_override}', found override='{actual_override}', role='{actual_role}') — skipping"
            )
            continue

        old_role = seg.get("speaker_role")
        old_override = seg.get("speaker_role_override")
        seg["speaker_role"] = suggested_override
        seg["speaker_role_override"] = suggested_override
        fixes_applied.append({
            "segment_id": seg_id,
            "field": "speaker_role+speaker_role_override",
            "old": f"role={old_role}, override={old_override}",
            "new": suggested_override,
            "source": "collapse_issue",
            "evidence": evidence[:200],
        })

    # --- Log boundary_issues as unfixed ---
    for entry in verification_data.get("boundary_issues", []):
        conv_id = entry.get("conversation_id", "?")
        severity = entry.get("severity", "unknown")
        issue = entry.get("issue", "")
        flags_not_fixed.append(
            f"boundary_issue [{severity}]: Conv {conv_id} — {issue[:150]}"
        )

    # --- Log other_flags as unfixed ---
    for flag in verification_data.get("other_flags", []):
        flags_not_fixed.append(f"other_flag: {str(flag)[:150]}")

    return fixes_applied, flags_not_fixed


def patch_file(
    conversations_path: Path,
    verification_path: Optional[Path],
    output_path: Path,
    dry_run: bool = False,
) -> Dict[str, Any]:
    """Patch a single conversations file using its verification report."""

    print(f"\n{LOG_PREFIX} Patching: {conversations_path.name}")

    with conversations_path.open("r", encoding="utf-8") as f:
        conversations_data = json.load(f)

    segment_count = len(conversations_data.get("segments", []))
    print(f"{LOG_PREFIX}   Segments: {segment_count}")

    # Deep copy so we don't modify original
    patched = copy.deepcopy(conversations_data)

    if verification_path is None or not verification_path.exists():
        # No verification file — copy unchanged
        print(f"{LOG_PREFIX}   No verification report found — copying unchanged")

        patched["patch_metadata"] = {
            "patched_at": time.strftime("%Y-%m-%dT%H:%M:%SZ"),
            "pipeline_version": PIPELINE_VERSION,
            "verification_verdict": None,
            "verification_file": None,
            "fixes_applied": [],
            "fixes_applied_count": 0,
            "flags_not_fixed": [],
            "flags_not_fixed_count": 0,
            "note": "No verification report found — copied unchanged",
        }

        if not dry_run:
            output_path.parent.mkdir(parents=True, exist_ok=True)
            with output_path.open("w", encoding="utf-8") as f:
                json.dump(patched, f, indent=2, ensure_ascii=False)
            print(f"{LOG_PREFIX}   Output: {output_path}")
        else:
            print(f"{LOG_PREFIX}   [DRY RUN] Would copy unchanged to {output_path}")

        return {"verdict": None, "fixes": 0, "flags": 0}

    # Load verification report
    with verification_path.open("r", encoding="utf-8") as f:
        verification_data = json.load(f)

    verdict = verification_data.get("verdict", "FLAG")
    print(f"{LOG_PREFIX}   Verification verdict: {verdict}")
    print(f"{LOG_PREFIX}   Verification file: {verification_path}")

    if verdict == "REJECT":
        raise RuntimeError(
            f"Verification verdict is REJECT for {conversations_path.name}. "
            "06b should have halted the pipeline before reaching 06c."
        )

    if verdict == "APPROVE":
        # Copy unchanged, just add metadata
        fixes_applied: List[Dict] = []
        flags_not_fixed: List[str] = []
    else:
        # FLAG — apply patches
        fixes_applied, flags_not_fixed = apply_patches(patched, verification_data)

    patched["patch_metadata"] = {
        "patched_at": time.strftime("%Y-%m-%dT%H:%M:%SZ"),
        "pipeline_version": PIPELINE_VERSION,
        "verification_verdict": verdict,
        "verification_file": str(verification_path),
        "fixes_applied": fixes_applied,
        "fixes_applied_count": len(fixes_applied),
        "flags_not_fixed": flags_not_fixed,
        "flags_not_fixed_count": len(flags_not_fixed),
    }

    # Print summary
    if fixes_applied:
        print(f"{LOG_PREFIX}   Fixes applied: {len(fixes_applied)}")
        for fix in fixes_applied:
            print(f"{LOG_PREFIX}     seg {fix['segment_id']}: {fix['old']} → {fix['new']} ({fix['source']})")
    if flags_not_fixed:
        print(f"{LOG_PREFIX}   Flags not fixed: {len(flags_not_fixed)}")
        for flag in flags_not_fixed:
            print(f"{LOG_PREFIX}     {flag[:120]}")

    if not dry_run:
        output_path.parent.mkdir(parents=True, exist_ok=True)
        with output_path.open("w", encoding="utf-8") as f:
            json.dump(patched, f, indent=2, ensure_ascii=False)
        print(f"{LOG_PREFIX}   Output: {output_path}")
    else:
        print(f"{LOG_PREFIX}   [DRY RUN] Would write to {output_path}")

    return {
        "verdict": verdict,
        "fixes": len(fixes_applied),
        "flags": len(flags_not_fixed),
    }


# ---------------------------
# Path helpers
# ---------------------------

def repo_root() -> Path:
    return Path(__file__).resolve().parents[2]


def s06_root() -> Path:
    return repo_root() / "data" / "06.video-type"


def s06b_root() -> Path:
    return repo_root() / "data" / "06b.verify"


def output_root() -> Path:
    return repo_root() / "data" / "06c.patched"


def test_s06_root() -> Path:
    return repo_root() / "data" / "test" / "06.video-type"


def test_s06b_root() -> Path:
    return repo_root() / "data" / "test" / "06b.verify"


def test_output_root() -> Path:
    return repo_root() / "data" / "test" / "06c.patched"


def compute_output_path(input_path: Path, output_dir: Path) -> Path:
    """Compute output path — same *.conversations.json pattern as stage 06."""
    stem = input_path.stem
    # Ensure the output has .conversations.json extension
    if stem.endswith(".conversations"):
        return output_dir / f"{stem}.json"
    return output_dir / f"{stem}.conversations.json"


def find_verification_for(
    conversations_path: Path, s06_dir: Path, s06b_dir: Path
) -> Optional[Path]:
    """Find the matching verification JSON for a conversations file.

    Tries two strategies:
    1. Mirror the relative path from s06_dir into s06b_dir, swapping extension
    2. Flat lookup in s06b_dir root (for files not in source subdirectories)
    """
    stem = conversations_path.stem
    if stem.endswith(".conversations"):
        stem = stem[:-len(".conversations")]
    verification_name = f"{stem}.verification.json"

    # Strategy 1: Mirror relative path
    try:
        rel = conversations_path.parent.relative_to(s06_dir)
        candidate = s06b_dir / rel / verification_name
        if candidate.exists():
            return candidate
    except ValueError:
        pass

    # Strategy 2: Flat lookup at s06b root
    candidate = s06b_dir / verification_name
    if candidate.exists():
        return candidate

    # Strategy 3: Search recursively in s06b_dir
    for found in s06b_dir.rglob(verification_name):
        return found

    return None


def parse_sources_file(path: Path) -> List[Tuple[str, str]]:
    sources: List[Tuple[str, str]] = []
    for raw in path.read_text(encoding="utf-8").splitlines():
        line = raw.strip()
        if not line or line.startswith("#"):
            continue
        if "|" in line:
            name, url = line.split("|", 1)
            sources.append((name.strip(), url.strip()))
            continue
        parts = shlex.split(line)
        if len(parts) >= 2:
            sources.append((parts[0], parts[1]))
    return sources


def find_input_files(in_dir: Path) -> List[Path]:
    return sorted(in_dir.rglob("*.conversations.json"))


# ---------------------------
# CLI
# ---------------------------

def main() -> None:
    parser = argparse.ArgumentParser(
        description="Auto-apply high-confidence fixes from 06b verification to stage 06 output"
    )
    parser.add_argument(
        "name", nargs="?",
        help="Source name (folder under data/06.video-type/)"
    )
    parser.add_argument(
        "youtube_url", nargs="?",
        help="YouTube URL (unused, accepted for pipeline compatibility)"
    )
    parser.add_argument(
        "--input",
        help="Input .conversations.json file or directory"
    )
    parser.add_argument(
        "--output",
        help="Output directory (defaults to data/06c.patched/)"
    )
    parser.add_argument(
        "--test",
        action="store_true",
        help="Process test videos (data/test/06.video-type/)"
    )
    parser.add_argument(
        "--sources",
        nargs="?",
        const="docs/sources.txt",
        help="Process all sources from sources.txt file"
    )
    parser.add_argument(
        "--manifest",
        help="Manifest file: only process videos listed (docs/pipeline/batches/P001.txt)."
    )
    parser.add_argument(
        "--dry-run",
        action="store_true",
        help="Preview what would be patched without writing"
    )
    parser.add_argument(
        "--overwrite",
        action="store_true",
        help="Overwrite existing output files"
    )
    parser.add_argument(
        "--debug",
        action="store_true",
        help="Enable debug logging"
    )

    args = parser.parse_args()

    # Route to appropriate mode
    if args.test:
        _run_directory(
            test_s06_root(), test_s06b_root(), test_output_root(), args
        )
    elif args.manifest:
        _run_manifest(args)
    elif args.input:
        _run_input(args)
    elif args.sources:
        _run_sources(args)
    elif args.name:
        _run_named_source(args)
    else:
        raise SystemExit("Provide a source name, --input, --test, --manifest, or --sources")


def _run_input(args) -> None:
    input_path = Path(args.input)
    if not input_path.exists():
        input_path = repo_root() / args.input
    if not input_path.exists():
        raise SystemExit(f"Input not found: {args.input}")

    if input_path.is_file():
        out_dir = Path(args.output) if args.output else output_root()
        output_path = compute_output_path(input_path, out_dir)

        if output_path.exists() and not args.overwrite:
            print(f"{LOG_PREFIX} Output exists, skipping: {output_path}")
            return

        verification_path = find_verification_for(input_path, s06_root(), s06b_root())
        result = patch_file(input_path, verification_path, output_path, dry_run=args.dry_run)
        print(f"\n{LOG_PREFIX} Done. Verdict: {result.get('verdict')}, "
              f"Fixes: {result.get('fixes', 0)}, Flags: {result.get('flags', 0)}")
        return

    out_dir = Path(args.output) if args.output else output_root()
    _run_directory(input_path, s06b_root(), out_dir, args)


def _run_sources(args) -> None:
    sources_path = repo_root() / args.sources
    if not sources_path.exists():
        raise SystemExit(f"Sources file not found: {sources_path}")

    total_fixes = 0
    total_files = 0

    for src_name, _ in parse_sources_file(sources_path):
        src_in_dir = s06_root() / src_name
        if not src_in_dir.exists():
            print(f"{LOG_PREFIX} Skipping {src_name}: no 06.video-type output")
            continue

        src_s06b_dir = s06b_root() / src_name
        src_out_dir = output_root() / src_name
        files = find_input_files(src_in_dir)

        for input_file in files:
            output_file = compute_output_path(input_file, src_out_dir)
            if output_file.exists() and not args.overwrite:
                continue
            try:
                verification_path = find_verification_for(input_file, s06_root(), s06b_root())
                result = patch_file(input_file, verification_path, output_file, dry_run=args.dry_run)
                total_fixes += result.get("fixes", 0)
                total_files += 1
            except Exception as e:
                print(f"{LOG_PREFIX} Error: {e}")

    print(f"\n{LOG_PREFIX} Done. Patched {total_files} files, {total_fixes} total fixes")


def _run_manifest(args) -> None:
    """Run for videos listed in a manifest file."""
    manifest_path = Path(args.manifest)
    if not manifest_path.is_absolute():
        manifest_path = repo_root() / manifest_path
    if not manifest_path.exists():
        raise SystemExit(f"Manifest file not found: {manifest_path}")

    sources_map = load_manifest_sources(manifest_path)

    for src_name, vid_ids in sorted(sources_map.items()):
        src_in_dir = s06_root() / src_name
        if not src_in_dir.exists():
            print(f"{LOG_PREFIX} Skipping {src_name}: no 06.video-type output")
            continue

        src_s06b_dir = s06b_root() / src_name
        src_out_dir = output_root() / src_name
        files = manifest_filter_files(find_input_files(src_in_dir), vid_ids)
        if not files:
            print(f"{LOG_PREFIX} Skipping {src_name}: no manifest videos found in input")
            continue

        print(f"{LOG_PREFIX} Manifest: {src_name} ({len(files)} videos)")
        _run_directory_with_files(files, src_in_dir, s06b_root(), src_out_dir, args)


def _run_directory_with_files(
    files: List[Path], in_dir: Path, verify_dir: Path, out_dir: Path, args
) -> None:
    """Process a specific list of files."""
    if not files:
        return

    print(f"{LOG_PREFIX} Input : {in_dir}")
    print(f"{LOG_PREFIX} Output: {out_dir}")
    print(f"{LOG_PREFIX} Files : {len(files)}")

    state_path = out_dir / ".06c_patch_state.json"
    state = load_state(state_path)

    processed = 0
    skipped = 0
    failed = 0
    consecutive_failures = 0
    total_fixes = 0
    total_flags = 0

    for input_file in files:
        file_key = str(input_file.relative_to(in_dir))

        if file_key in state.completed_files and not args.overwrite:
            skipped += 1
            continue

        output_file = compute_output_path(input_file, out_dir)

        if output_file.exists() and not args.overwrite:
            skipped += 1
            state.completed_files.append(file_key)
            save_state(state_path, state)
            continue

        state.in_progress = file_key
        save_state(state_path, state)

        try:
            verification_path = find_verification_for(input_file, s06_root(), verify_dir)
            result = patch_file(input_file, verification_path, output_file, dry_run=args.dry_run)
            processed += 1
            consecutive_failures = 0
            total_fixes += result.get("fixes", 0)
            total_flags += result.get("flags", 0)

            if not args.dry_run:
                state.completed_files.append(file_key)
                state.in_progress = None
                save_state(state_path, state)

        except Exception as e:
            print(f"{LOG_PREFIX} Error processing {input_file}: {e}")
            state.failures.append({"file": file_key, "error": str(e)})
            state.in_progress = None
            save_state(state_path, state)
            failed += 1
            consecutive_failures += 1

            if consecutive_failures >= MAX_CONSECUTIVE_FAILURES:
                print(f"\n{LOG_PREFIX} HALTING: {consecutive_failures} consecutive failures")
                break

    print(f"\n{LOG_PREFIX} Done.")
    print(f"  Processed: {processed}")
    print(f"  Skipped:   {skipped}")
    print(f"  Failed:    {failed}")
    print(f"  Fixes:     {total_fixes}")
    print(f"  Flags:     {total_flags}")

    total_attempted = processed + failed
    if total_attempted > 0:
        failure_rate = failed / total_attempted
        if failure_rate > MAX_FAILURE_RATE and total_attempted >= 5:
            print(f"\n{LOG_PREFIX} HALTING: Failure rate {failure_rate:.0%} exceeds {MAX_FAILURE_RATE:.0%} threshold")
            sys.exit(1)


def _run_named_source(args) -> None:
    """Run for a named source (from pipeline: ./06c.patch source_name url)."""
    name = args.name
    in_dir = s06_root() / name
    if not in_dir.exists():
        raise SystemExit(f"Input directory not found: {in_dir}")

    verify_dir = s06b_root() / name
    out_dir = Path(args.output) if args.output else output_root() / name
    _run_directory(in_dir, verify_dir, out_dir, args)


def _run_directory(in_dir: Path, verify_dir: Path, out_dir: Path, args) -> None:
    files = find_input_files(in_dir)
    if not files:
        print(f"{LOG_PREFIX} No .conversations.json files found in: {in_dir}")
        return
    _run_directory_with_files(files, in_dir, verify_dir, out_dir, args)


if __name__ == "__main__":
    main()
