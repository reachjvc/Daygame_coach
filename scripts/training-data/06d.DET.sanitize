#!/usr/bin/env python3
"""
scripts/training-data/06d.DET.sanitize

Deterministic sanitization pass between 06c.patch and Stage 07.

Reads:
  - data/06c.DET.patched/<source>/<video>/*.conversations.json

Writes:
  - data/06d.DET.sanitized/<source>/<video>/*.conversations.json
  - data/06d.DET.sanitized/<source>/<video>/*.sanitize.report.json

Current deterministic rules (D1):
  1) teaser_duplicate detection (early duplicated preview lines)
  2) mixed_mode detection (live interaction + commentary merged in one segment)
  3) Stage 07 evidence allowlist generation per conversation

No LLM calls.
"""

from __future__ import annotations

import argparse
import copy
import json
import re
import shlex
import sys
import time
from pathlib import Path
from typing import Any, Dict, List, Optional, Set, Tuple

from batch.manifest_parser import load_manifest_sources, manifest_filter_files
from batch.quarantine_helpers import get_quarantine_block_reason, load_quarantine_video_ids


LOG_PREFIX = "[06d.DET.sanitize]"
PIPELINE_VERSION = "06d.DET.sanitize-v1.0"


def repo_root() -> Path:
    return Path(__file__).resolve().parents[2]


def input_root() -> Path:
    return repo_root() / "data" / "06c.DET.patched"


def output_root() -> Path:
    return repo_root() / "data" / "06d.DET.sanitized"


def test_input_root() -> Path:
    return repo_root() / "data" / "test" / "06c.DET.patched"


def test_output_root() -> Path:
    return repo_root() / "data" / "test" / "06d.DET.sanitized"


def resolve_root_path(raw_path: Optional[str], default_root: Path) -> Path:
    if not raw_path:
        return default_root
    path = Path(raw_path)
    if not path.is_absolute():
        path = repo_root() / path
    return path


def parse_sources_file(path: Path) -> List[Tuple[str, str]]:
    sources: List[Tuple[str, str]] = []
    for raw in path.read_text(encoding="utf-8").splitlines():
        line = raw.strip()
        if not line or line.startswith("#"):
            continue
        if "|" in line:
            name, url = line.split("|", 1)
            sources.append((name.strip(), url.strip()))
            continue
        parts = shlex.split(line)
        if len(parts) >= 2:
            sources.append((parts[0], parts[1]))
    return sources


def find_input_files(in_dir: Path) -> List[Path]:
    return sorted(in_dir.rglob("*.conversations.json"))


def compute_output_path(input_path: Path, output_dir: Path) -> Path:
    stem = input_path.stem
    if stem.endswith(".conversations"):
        filename = f"{stem}.json"
    else:
        filename = f"{stem}.conversations.json"
    return output_dir / filename


def compute_output_path_with_layout(
    input_path: Path,
    output_dir: Path,
    input_root_dir: Optional[Path] = None,
) -> Path:
    canonical = compute_output_path(input_path, output_dir)
    if input_root_dir is not None:
        try:
            rel_parent = input_path.parent.relative_to(input_root_dir)
            if rel_parent != Path("."):
                return output_dir / rel_parent / canonical.name
        except ValueError:
            pass
    return canonical


def find_existing_output_path(
    input_path: Path,
    preferred_output_dir: Path,
    output_root_dir: Optional[Path] = None,
    input_root_dir: Optional[Path] = None,
) -> Optional[Path]:
    preferred = compute_output_path_with_layout(
        input_path,
        preferred_output_dir,
        input_root_dir=input_root_dir,
    )
    if preferred.exists():
        return preferred
    return None


def normalize_text_for_match(text: str) -> str:
    lowered = text.lower()
    lowered = re.sub(r"[^a-z0-9\s]", " ", lowered)
    lowered = re.sub(r"\s+", " ", lowered).strip()
    return lowered


def _unique_flags(existing: Any, new_flag: str) -> List[str]:
    out: List[str] = []
    if isinstance(existing, list):
        for item in existing:
            if isinstance(item, str):
                out.append(item)
    if new_flag not in out:
        out.append(new_flag)
    return out


def _detect_teaser_duplicates(
    segments: List[Dict[str, Any]],
    teaser_scan_limit: int = 30,
    min_distance: int = 20,
    min_run_len: int = 3,
    min_norm_chars: int = 6,
) -> Tuple[List[Dict[str, int]], Set[int], Dict[int, int]]:
    """
    Find repeated early transcript windows that appear later.

    Returns:
      - runs: list of dicts with early/later start/end segment IDs
      - early_duplicate_segment_ids: IDs to tag as teaser duplicates
      - early_to_later_map: early segment ID -> matched later segment ID
    """
    if not segments:
        return [], set(), {}

    norms = [normalize_text_for_match(str(seg.get("text", ""))) for seg in segments]
    early_limit = min(teaser_scan_limit, len(segments))

    idx_pair: Dict[int, int] = {}
    for early_idx in range(early_limit):
        early_norm = norms[early_idx]
        if len(early_norm) < min_norm_chars:
            continue
        for later_idx in range(early_idx + min_distance, len(segments)):
            if norms[later_idx] == early_norm:
                idx_pair[early_idx] = later_idx
                break

    if not idx_pair:
        return [], set(), {}

    runs_idx: List[Tuple[int, int, int, int]] = []
    keys = sorted(idx_pair.keys())
    i = 0
    while i < len(keys):
        start_e = keys[i]
        start_l = idx_pair[start_e]
        end_e = start_e
        end_l = start_l
        i += 1
        while i < len(keys):
            k = keys[i]
            expected_e = end_e + 1
            expected_l = end_l + 1
            if k == expected_e and idx_pair[k] == expected_l:
                end_e = k
                end_l = idx_pair[k]
                i += 1
                continue
            break
        if (end_e - start_e + 1) >= min_run_len:
            runs_idx.append((start_e, end_e, start_l, end_l))

    # Optional one-step extension for merged text like:
    # early: "X ... and Y"
    # later: "X ..." + "Y"
    extended_runs: List[Tuple[int, int, int, int]] = []
    for start_e, end_e, start_l, end_l in runs_idx:
        next_e = end_e + 1
        next_l = end_l + 1
        while next_e < early_limit and next_l < len(segments):
            early_norm = norms[next_e]
            if len(early_norm) < min_norm_chars:
                break
            cand_single = norms[next_l]
            cand_double = cand_single
            if next_l + 1 < len(segments):
                cand_double = f"{cand_single} {norms[next_l + 1]}".strip()
            matched = (
                early_norm in cand_single
                or cand_single in early_norm
                or early_norm in cand_double
                or cand_double in early_norm
            )
            if not matched:
                break
            end_e = next_e
            # if matched only with double, widen later window
            if next_l + 1 < len(segments) and (early_norm in cand_double or cand_double in early_norm):
                end_l = next_l + 1
                next_l = end_l + 1
            else:
                end_l = next_l
                next_l = end_l + 1
            next_e = end_e + 1
        extended_runs.append((start_e, end_e, start_l, end_l))

    early_ids: Set[int] = set()
    early_to_later: Dict[int, int] = {}
    runs: List[Dict[str, int]] = []
    for start_e, end_e, start_l, end_l in extended_runs:
        early_start_id = int(segments[start_e].get("id", start_e))
        early_end_id = int(segments[end_e].get("id", end_e))
        later_start_id = int(segments[start_l].get("id", start_l))
        later_end_id = int(segments[end_l].get("id", end_l))
        runs.append(
            {
                "early_start_segment_id": early_start_id,
                "early_end_segment_id": early_end_id,
                "later_start_segment_id": later_start_id,
                "later_end_segment_id": later_end_id,
            }
        )
        e_idx = start_e
        l_idx = start_l
        while e_idx <= end_e and l_idx <= end_l:
            early_id = int(segments[e_idx].get("id", e_idx))
            later_id = int(segments[l_idx].get("id", l_idx))
            early_ids.add(early_id)
            early_to_later[early_id] = later_id
            e_idx += 1
            l_idx += 1

    return runs, early_ids, early_to_later


def _detect_mixed_mode_segments(segments: List[Dict[str, Any]]) -> Set[int]:
    markers = (
        "so rather than me",
        "to begin with",
        "as we can see",
        "what am i showing here",
        "the important thing to note",
        "i decided to wrap it up",
        "you'll notice",
        "this is where",
    )
    question_re = re.compile(
        r"\b(did you|do you|are you|would you|can you|what do you|where are you|what kind of)\b",
        re.IGNORECASE,
    )
    mixed_ids: Set[int] = set()
    for seg in segments:
        if int(seg.get("conversation_id", 0)) <= 0:
            continue
        if str(seg.get("segment_type", "")).strip().lower() != "approach":
            continue
        text = str(seg.get("text", ""))
        low = text.lower()
        has_interaction = bool(question_re.search(text)) or "?" in text
        has_commentary = any(m in low for m in markers)
        if has_interaction and has_commentary:
            seg_id = seg.get("id")
            if isinstance(seg_id, int):
                mixed_ids.add(seg_id)
    return mixed_ids


def _split_mixed_mode_text(text: str) -> Optional[Dict[str, Any]]:
    """Attempt deterministic boundary split of mixed interaction/commentary text.

    Returns a dict with split details when confidence is high enough, otherwise None.
    """
    if not isinstance(text, str):
        return None
    raw = text.strip()
    if len(raw) < 24:
        return None

    markers = (
        " so rather than me ",
        " to begin with ",
        " as we can see ",
        " what am i showing here ",
        " the important thing to note ",
        " i decided to wrap it up ",
        " you'll notice ",
        " this is where ",
    )
    low = f" {raw.lower()} "

    best_idx: Optional[int] = None
    best_marker = ""
    for marker in markers:
        idx = low.find(marker)
        if idx < 0:
            continue
        # convert from padded string position to original string position
        orig_idx = max(0, idx - 1)
        if best_idx is None or orig_idx < best_idx:
            best_idx = orig_idx
            best_marker = marker.strip()

    if best_idx is None:
        return None

    if best_idx < 10 or (len(raw) - best_idx) < 14:
        return None

    interaction_text = raw[:best_idx].strip(" ,.;:-")
    commentary_text = raw[best_idx:].strip(" ,.;:-")
    if len(interaction_text) < 8 or len(commentary_text) < 12:
        return None

    has_interaction_cue = (
        "?" in interaction_text
        or bool(
            re.search(
                r"\b(did you|do you|are you|would you|can you|what do you|where are you|what kind of)\b",
                interaction_text,
                re.IGNORECASE,
            )
        )
    )
    has_commentary_cue = any(
        cue in commentary_text.lower()
        for cue in (
            "rather than me",
            "to begin with",
            "as we can see",
            "what am i showing here",
            "the important thing to note",
            "you'll notice",
            "this is where",
        )
    )
    if not has_interaction_cue or not has_commentary_cue:
        return None

    return {
        "boundary_char_index": best_idx,
        "boundary_marker": best_marker,
        "interaction_text": interaction_text,
        "commentary_text": commentary_text,
        "confidence": 0.95,
        "split_method": "marker_boundary_v1",
    }


def _recompute_conversations(segments: List[Dict[str, Any]], old_conversations: Any) -> List[Dict[str, Any]]:
    old_by_id: Dict[int, Dict[str, Any]] = {}
    if isinstance(old_conversations, list):
        for c in old_conversations:
            if isinstance(c, dict) and isinstance(c.get("conversation_id"), int):
                old_by_id[int(c["conversation_id"])] = c

    by_conv: Dict[int, List[Dict[str, Any]]] = {}
    for seg in segments:
        cid = seg.get("conversation_id", 0)
        if isinstance(cid, int) and cid > 0:
            by_conv.setdefault(cid, []).append(seg)

    out: List[Dict[str, Any]] = []
    for cid in sorted(by_conv.keys()):
        segs = by_conv[cid]
        seg_ids = [int(s["id"]) for s in segs if isinstance(s.get("id"), int)]
        base = copy.deepcopy(old_by_id.get(cid, {}))
        base["conversation_id"] = cid
        base["segment_ids"] = seg_ids
        base["start_time"] = float(segs[0].get("start", 0.0))
        base["end_time"] = float(segs[-1].get("end", 0.0))
        out.append(base)
    return out


def _build_conv_speaker_stats(segments: List[Dict[str, Any]]) -> Dict[int, Dict[str, Dict[str, int]]]:
    """Build per-conversation, per-speaker role counters from segments."""
    stats: Dict[int, Dict[str, Dict[str, int]]] = {}
    for seg in segments:
        if not isinstance(seg, dict):
            continue
        conv_id = seg.get("conversation_id")
        speaker_id = seg.get("speaker_id")
        if not isinstance(conv_id, int) or conv_id <= 0:
            continue
        if not isinstance(speaker_id, str) or not speaker_id.strip():
            continue
        role = str(seg.get("speaker_role", "")).strip().lower()
        if not role:
            role = "unknown"
        by_speaker = stats.setdefault(conv_id, {})
        speaker_stats = by_speaker.setdefault(
            speaker_id,
            {"target_turns": 0, "coach_turns": 0, "other_turns": 0},
        )
        if role == "target":
            speaker_stats["target_turns"] += 1
        elif role in ("coach", "student", "voiceover"):
            speaker_stats["coach_turns"] += 1
        else:
            speaker_stats["other_turns"] += 1
    return stats


def _derive_fixed_coach_speaker_ids(
    speaker_labels: Any,
    *,
    min_confidence: float = 0.75,
) -> Set[str]:
    """Derive high-confidence non-collapsed coach speaker IDs."""
    fixed: Set[str] = set()
    if not isinstance(speaker_labels, dict):
        return fixed
    for speaker_id, meta in speaker_labels.items():
        if not isinstance(speaker_id, str) or not isinstance(meta, dict):
            continue
        role = str(meta.get("role", "")).strip().lower()
        conf_raw = meta.get("confidence", 0.0)
        try:
            confidence = float(conf_raw)
        except Exception:
            confidence = 0.0
        if role == "coach" and confidence >= min_confidence:
            fixed.add(speaker_id)
    return fixed


def _derive_confident_target_speaker_ids(
    *,
    conv: Dict[str, Any],
    speaker_labels: Any,
    fixed_coach_speaker_ids: Set[str],
    conv_speaker_stats: Dict[str, Dict[str, int]],
    min_target_confidence: float = 0.60,
) -> List[str]:
    """Conservative confident-target subset for downstream hook/investment gating."""
    tp = conv.get("target_participation")
    raw_ids: List[str] = []
    if isinstance(tp, dict):
        raw = tp.get("target_speaker_ids")
        if isinstance(raw, list):
            raw_ids.extend([sid for sid in raw if isinstance(sid, str) and sid.strip()])

    # Include any speaker with observed target turns even if missing from raw list.
    for speaker_id, sstats in conv_speaker_stats.items():
        if isinstance(sstats, dict) and int(sstats.get("target_turns", 0)) > 0:
            raw_ids.append(speaker_id)

    # Preserve order while deduping.
    dedup_raw: List[str] = []
    seen: Set[str] = set()
    for sid in raw_ids:
        if sid in seen:
            continue
        seen.add(sid)
        dedup_raw.append(sid)

    confident: List[str] = []
    labels = speaker_labels if isinstance(speaker_labels, dict) else {}
    for sid in dedup_raw:
        if sid in fixed_coach_speaker_ids:
            continue
        if sid.strip().upper() == "UNKNOWN":
            continue

        meta = labels.get(sid, {})
        role = str(meta.get("role", "")).strip().lower() if isinstance(meta, dict) else ""
        conf_raw = meta.get("confidence", 0.0) if isinstance(meta, dict) else 0.0
        try:
            confidence = float(conf_raw)
        except Exception:
            confidence = 0.0

        if role == "collapsed":
            continue
        if role == "target" and confidence >= min_target_confidence:
            confident.append(sid)
            continue

        # Fallback: allow if segment-level evidence strongly supports target role
        # and there is no coach-role evidence for this speaker ID in this conversation.
        sstats = conv_speaker_stats.get(sid, {})
        target_turns = int(sstats.get("target_turns", 0)) if isinstance(sstats, dict) else 0
        coach_turns = int(sstats.get("coach_turns", 0)) if isinstance(sstats, dict) else 0
        if target_turns >= 2 and coach_turns == 0:
            confident.append(sid)

    return confident


def _apply_target_participation_hygiene(
    conversations: List[Dict[str, Any]],
    segments: List[Dict[str, Any]],
    speaker_labels: Any,
) -> Dict[str, int]:
    """Populate confident target speaker IDs and remove fixed coach IDs from target lists."""
    fixed_coach_speaker_ids = _derive_fixed_coach_speaker_ids(speaker_labels)
    stats_by_conv = _build_conv_speaker_stats(segments)
    polluted_target_lists = 0
    confident_empty = 0

    for conv in conversations:
        if not isinstance(conv, dict):
            continue
        conv_id = conv.get("conversation_id")
        if not isinstance(conv_id, int) or conv_id <= 0:
            continue

        tp = conv.get("target_participation")
        if not isinstance(tp, dict):
            tp = {}
            conv["target_participation"] = tp

        raw_ids = tp.get("target_speaker_ids")
        if not isinstance(raw_ids, list):
            raw_ids = []

        filtered_raw = [
            sid
            for sid in raw_ids
            if isinstance(sid, str) and sid not in fixed_coach_speaker_ids
        ]
        if len(filtered_raw) != len(raw_ids):
            polluted_target_lists += 1
        tp["target_speaker_ids"] = filtered_raw

        confident_ids = _derive_confident_target_speaker_ids(
            conv=conv,
            speaker_labels=speaker_labels,
            fixed_coach_speaker_ids=fixed_coach_speaker_ids,
            conv_speaker_stats=stats_by_conv.get(conv_id, {}),
        )
        if not confident_ids:
            confident_empty += 1

        tp["target_speaker_ids_confident"] = confident_ids
        tp["fixed_coach_speaker_ids"] = sorted(fixed_coach_speaker_ids)
        tp["confidence_hygiene"] = {
            "pipeline_version": PIPELINE_VERSION,
            "min_target_label_confidence": 0.60,
            "fixed_coach_min_confidence": 0.75,
        }

    return {
        "fixed_coach_speaker_ids_count": len(fixed_coach_speaker_ids),
        "target_lists_filtered_count": polluted_target_lists,
        "conversations_with_empty_confident_targets": confident_empty,
    }


def sanitize_conversations(
    data: Dict[str, Any],
    source_file: str,
) -> Tuple[Dict[str, Any], Dict[str, Any]]:
    out = copy.deepcopy(data)
    segments = out.get("segments")
    if not isinstance(segments, list):
        raise RuntimeError("Invalid conversations JSON: missing 'segments' list")

    # Ensure deterministic segment ordering by id
    segments.sort(key=lambda s: int(s.get("id", 0)))

    teaser_runs, teaser_ids, teaser_map = _detect_teaser_duplicates(segments)
    mixed_mode_ids = _detect_mixed_mode_segments(segments)

    id_to_seg: Dict[int, Dict[str, Any]] = {
        int(seg["id"]): seg for seg in segments if isinstance(seg.get("id"), int)
    }

    flagged_details: List[Dict[str, Any]] = []
    mixed_mode_split_candidates = 0
    mixed_mode_split_materialized = 0
    mixed_mode_synthetic_child_segments = 0
    used_segment_ids: Set[int] = set(id_to_seg.keys())
    next_synthetic_segment_id = (max(used_segment_ids) + 1) if used_segment_ids else 0
    synthetic_children_by_parent: Dict[int, List[Dict[str, Any]]] = {}

    for seg_id in sorted(teaser_ids):
        seg = id_to_seg.get(seg_id)
        if not seg:
            continue
        seg["segment_flags"] = _unique_flags(seg.get("segment_flags"), "teaser_duplicate")
        seg["exclude_from_stage07_evidence"] = True
        seg["is_teaser"] = True
        later_id = teaser_map.get(seg_id)
        later_seg = id_to_seg.get(later_id) if later_id is not None else None
        teaser_of = None
        if later_seg is not None:
            later_cid = later_seg.get("conversation_id", 0)
            if isinstance(later_cid, int) and later_cid > 0:
                teaser_of = later_cid
        seg["teaser_of_conversation_id"] = teaser_of
        seg["segment_type"] = "commentary"
        seg["conversation_id"] = 0
        flagged_details.append(
            {
                "segment_id": seg_id,
                "flags": ["teaser_duplicate"],
                "text_preview": str(seg.get("text", ""))[:140],
            }
        )

    for seg_id in sorted(mixed_mode_ids):
        seg = id_to_seg.get(seg_id)
        if not seg:
            continue
        seg["segment_flags"] = _unique_flags(seg.get("segment_flags"), "mixed_mode")
        split = _split_mixed_mode_text(str(seg.get("text", "")))
        if split:
            original_text = str(seg.get("text", ""))
            original_start = float(seg.get("start", 0.0))
            original_end = float(seg.get("end", original_start))
            duration = max(0.0, original_end - original_start)
            boundary_ratio = float(split["boundary_char_index"]) / max(float(len(original_text)), 1.0)
            interaction_end = original_start + (duration * boundary_ratio)
            if duration > 0.0:
                min_end = original_start + min(0.05, duration * 0.2)
                max_end = original_end - min(0.05, duration * 0.2)
                interaction_end = max(min_end, min(max_end, interaction_end))
            else:
                interaction_end = original_end

            child_id = next_synthetic_segment_id
            while child_id in used_segment_ids:
                child_id += 1
            next_synthetic_segment_id = child_id + 1
            used_segment_ids.add(child_id)

            original_speaker_role = str(seg.get("speaker_role", ""))
            original_speaker_id = seg.get("speaker_id")
            seg["mixed_mode_split"] = {
                "status": "materialized",
                "parent_segment_id": seg_id,
                "child_segment_ids": [child_id],
                "split_method": split["split_method"],
                "confidence": split["confidence"],
                "boundary_char_index": split["boundary_char_index"],
                "boundary_marker": split["boundary_marker"],
                "interaction_text": split["interaction_text"],
                "commentary_text": split["commentary_text"],
                "interaction_end_time": interaction_end,
                "original_start_time": original_start,
                "original_end_time": original_end,
                "original_speaker_role": original_speaker_role,
                "original_speaker_id": original_speaker_id,
                "original_text": original_text,
            }

            # Parent becomes the interaction child (keeps original segment id for compatibility).
            seg["text"] = split["interaction_text"]
            seg["start"] = original_start
            seg["end"] = interaction_end
            seg["speaker_role"] = "coach"
            seg["speaker_role_override"] = "coach"
            seg["exclude_from_stage07_evidence"] = False
            seg.pop("exclude_from_stage07_reason", None)
            seg["segment_flags"] = _unique_flags(seg.get("segment_flags"), "mixed_mode_split_candidate")
            seg["segment_flags"] = _unique_flags(seg.get("segment_flags"), "mixed_mode_split_materialized")

            child_segment: Dict[str, Any] = {
                "id": child_id,
                "start": interaction_end,
                "end": original_end,
                "text": split["commentary_text"],
                "speaker_id": original_speaker_id,
                "speaker_role": "coach",
                "segment_type": "commentary",
                "conversation_id": 0,
                "is_conversation_start": False,
                "synthetic_segment": True,
                "synthetic_type": "mixed_mode_commentary_tail",
                "parent_segment_id": seg_id,
                "split_method": split["split_method"],
                "exclude_from_stage07_evidence": True,
                "exclude_from_stage07_reason": "synthetic_commentary_child_from_mixed_mode",
                "segment_flags": ["mixed_mode_split_child", "synthetic_commentary"],
            }
            synthetic_children_by_parent.setdefault(seg_id, []).append(child_segment)

            mixed_mode_split_candidates += 1
            mixed_mode_split_materialized += 1
            mixed_mode_synthetic_child_segments += 1
            flagged_details.append(
                {
                    "segment_id": seg_id,
                    "flags": ["mixed_mode", "mixed_mode_split_candidate", "mixed_mode_split_materialized"],
                    "materialized": True,
                    "synthetic_child_segment_ids": [child_id],
                    "boundary_marker": split["boundary_marker"],
                    "interaction_preview": split["interaction_text"][:120],
                    "commentary_preview": split["commentary_text"][:120],
                }
            )
        else:
            seg["exclude_from_stage07_evidence"] = True
            seg["exclude_from_stage07_reason"] = "mixed_mode_no_deterministic_boundary"
            flagged_details.append(
                {
                    "segment_id": seg_id,
                    "flags": ["mixed_mode"],
                    "exclude_reason": "mixed_mode_no_deterministic_boundary",
                    "text_preview": str(seg.get("text", ""))[:140],
                }
            )

    if synthetic_children_by_parent:
        rebuilt_segments: List[Dict[str, Any]] = []
        for seg in segments:
            rebuilt_segments.append(seg)
            seg_id = seg.get("id")
            if isinstance(seg_id, int) and seg_id in synthetic_children_by_parent:
                rebuilt_segments.extend(synthetic_children_by_parent[seg_id])
        segments = rebuilt_segments
        out["segments"] = segments
        id_to_seg = {
            int(seg["id"]): seg
            for seg in segments
            if isinstance(seg, dict) and isinstance(seg.get("id"), int)
        }

    # Recompute conversations and add Stage 07 evidence allowlist
    old_conversations = out.get("conversations", [])
    conversations = _recompute_conversations(segments, old_conversations)
    speaker_labels = out.get("speaker_labels", {})
    hygiene_counts = _apply_target_participation_hygiene(
        conversations,
        segments,
        speaker_labels,
    )
    for conv in conversations:
        seg_ids = [sid for sid in conv.get("segment_ids", []) if isinstance(sid, int)]
        allow_ids: List[int] = []
        excluded_ids: List[int] = []
        for sid in seg_ids:
            seg = id_to_seg.get(sid)
            if not seg:
                continue
            excluded = bool(seg.get("exclude_from_stage07_evidence"))
            if str(seg.get("segment_type", "")).strip().lower() != "approach":
                excluded = True
            if excluded:
                excluded_ids.append(sid)
            else:
                allow_ids.append(sid)
        conv["stage07_evidence_segment_ids"] = allow_ids
        conv["excluded_stage07_evidence_segment_ids"] = excluded_ids
        conv["sanitizer_version"] = PIPELINE_VERSION

    out["conversations"] = conversations
    out["segments"] = segments

    metadata = out.get("metadata")
    if not isinstance(metadata, dict):
        metadata = {}
        out["metadata"] = metadata
    metadata["sanitized_at"] = time.strftime("%Y-%m-%dT%H:%M:%SZ")
    metadata["sanitizer_pipeline_version"] = PIPELINE_VERSION

    out["sanitize_metadata"] = {
        "pipeline_version": PIPELINE_VERSION,
        "source_file": source_file,
        "generated_at": time.strftime("%Y-%m-%dT%H:%M:%SZ"),
        "counts": {
            "segments_total": len(segments),
            "teaser_duplicate_segments": len(teaser_ids),
            "mixed_mode_segments": len(mixed_mode_ids),
            "mixed_mode_split_candidates": mixed_mode_split_candidates,
            "mixed_mode_split_materialized_segments": mixed_mode_split_materialized,
            "mixed_mode_synthetic_child_segments": mixed_mode_synthetic_child_segments,
            "excluded_from_stage07_evidence": sum(
                1 for s in segments if bool(s.get("exclude_from_stage07_evidence"))
            ),
            "fixed_coach_speaker_ids_count": hygiene_counts["fixed_coach_speaker_ids_count"],
            "target_lists_filtered_count": hygiene_counts["target_lists_filtered_count"],
            "conversations_with_empty_confident_targets": hygiene_counts["conversations_with_empty_confident_targets"],
        },
    }

    report = {
        "video_id": out.get("video_id"),
        "generated_at": time.strftime("%Y-%m-%dT%H:%M:%SZ"),
        "pipeline_version": PIPELINE_VERSION,
        "source_file": source_file,
        "rules": {
            "teaser_duplicate": {
                "enabled": True,
                "description": "Early duplicated preview lines are marked and excluded from Stage 07 evidence.",
            },
            "mixed_mode": {
                "enabled": True,
                "description": "Approach segments containing both interaction and commentary markers are excluded from Stage 07 evidence.",
            },
            "stage07_evidence_allowlist": {
                "enabled": True,
                "description": "Per-conversation stage07_evidence_segment_ids generated from sanitized segments.",
            },
        },
        "counts": {
            "segments_total": len(segments),
            "teaser_duplicate_segments": len(teaser_ids),
            "mixed_mode_segments": len(mixed_mode_ids),
            "mixed_mode_split_candidates": mixed_mode_split_candidates,
            "mixed_mode_split_materialized_segments": mixed_mode_split_materialized,
            "mixed_mode_synthetic_child_segments": mixed_mode_synthetic_child_segments,
            "excluded_from_stage07_evidence": sum(
                1 for s in segments if bool(s.get("exclude_from_stage07_evidence"))
            ),
            "conversations_total": len(conversations),
            "fixed_coach_speaker_ids_count": hygiene_counts["fixed_coach_speaker_ids_count"],
            "target_lists_filtered_count": hygiene_counts["target_lists_filtered_count"],
            "conversations_with_empty_confident_targets": hygiene_counts["conversations_with_empty_confident_targets"],
        },
        "teaser_runs": teaser_runs,
        "teaser_duplicate_segment_ids": sorted(teaser_ids),
        "mixed_mode_segment_ids": sorted(mixed_mode_ids),
        "mixed_mode_split_candidate_segment_ids": sorted(
            [
                int(s.get("id"))
                for s in segments
                if isinstance(s, dict)
                and isinstance(s.get("id"), int)
                and "mixed_mode_split_candidate" in (s.get("segment_flags") or [])
            ]
        ),
        "mixed_mode_split_materialized_segment_ids": sorted(
            [
                int(s.get("id"))
                for s in segments
                if isinstance(s, dict)
                and isinstance(s.get("id"), int)
                and "mixed_mode_split_materialized" in (s.get("segment_flags") or [])
            ]
        ),
        "mixed_mode_split_child_segment_ids": sorted(
            [
                int(s.get("id"))
                for s in segments
                if isinstance(s, dict)
                and isinstance(s.get("id"), int)
                and "mixed_mode_split_child" in (s.get("segment_flags") or [])
            ]
        ),
        "flagged_segments": flagged_details,
    }

    return out, report


def process_file(input_path: Path, output_path: Path, dry_run: bool = False) -> Dict[str, int]:
    with input_path.open("r", encoding="utf-8") as f:
        data = json.load(f)

    sanitized, report = sanitize_conversations(data, source_file=str(input_path))
    report_path = output_path.with_suffix(".sanitize.report.json")

    if dry_run:
        print(
            f"{LOG_PREFIX} [DRY RUN] {input_path.name}: "
            f"teaser={report['counts']['teaser_duplicate_segments']}, "
            f"mixed_mode={report['counts']['mixed_mode_segments']}, "
            f"excluded={report['counts']['excluded_from_stage07_evidence']}"
        )
        return report["counts"]

    output_path.parent.mkdir(parents=True, exist_ok=True)
    with output_path.open("w", encoding="utf-8") as f:
        json.dump(sanitized, f, indent=2, ensure_ascii=False)

    with report_path.open("w", encoding="utf-8") as f:
        json.dump(report, f, indent=2, ensure_ascii=False)

    print(
        f"{LOG_PREFIX} {input_path.name}: "
        f"teaser={report['counts']['teaser_duplicate_segments']}, "
        f"mixed_mode={report['counts']['mixed_mode_segments']}, "
        f"excluded={report['counts']['excluded_from_stage07_evidence']}"
    )
    print(f"{LOG_PREFIX}   Output: {output_path}")
    print(f"{LOG_PREFIX}   Report: {report_path}")
    return report["counts"]


def _run_directory_with_files(
    files: List[Path],
    in_dir: Path,
    out_dir: Path,
    args: argparse.Namespace,
    output_root_dir: Optional[Path] = None,
) -> None:
    if not files:
        return
    print(f"{LOG_PREFIX} Input : {in_dir}")
    print(f"{LOG_PREFIX} Output: {out_dir}")
    print(f"{LOG_PREFIX} Files : {len(files)}")

    processed = 0
    skipped = 0
    skipped_quarantine = 0
    failed = 0
    totals = {
        "teaser_duplicate_segments": 0,
        "mixed_mode_segments": 0,
        "excluded_from_stage07_evidence": 0,
    }
    quarantine_ids: Set[str] = getattr(args, "_quarantine_ids", set())

    for input_file in files:
        quarantine_reason = get_quarantine_block_reason(input_file, quarantine_ids)
        if quarantine_reason:
            print(f"{LOG_PREFIX} SKIP: {input_file.name} - {quarantine_reason}")
            skipped_quarantine += 1
            continue
        preferred_output = compute_output_path_with_layout(
            input_file,
            out_dir,
            input_root_dir=in_dir,
        )
        existing_output = find_existing_output_path(
            input_file,
            out_dir,
            output_root_dir=output_root_dir,
            input_root_dir=in_dir,
        )
        if existing_output and not args.overwrite:
            skipped += 1
            continue
        try:
            counts = process_file(input_file, preferred_output, dry_run=args.dry_run)
            processed += 1
            totals["teaser_duplicate_segments"] += counts.get("teaser_duplicate_segments", 0)
            totals["mixed_mode_segments"] += counts.get("mixed_mode_segments", 0)
            totals["excluded_from_stage07_evidence"] += counts.get(
                "excluded_from_stage07_evidence", 0
            )
        except Exception as e:
            failed += 1
            print(f"{LOG_PREFIX} ERROR: {input_file} -> {e}")

    print(f"\n{LOG_PREFIX} Done.")
    print(f"  Processed: {processed}")
    print(f"  Skipped:   {skipped}")
    print(f"  Skipped (quarantine): {skipped_quarantine}")
    print(f"  Failed:    {failed}")
    print(f"  Teaser duplicates: {totals['teaser_duplicate_segments']}")
    print(f"  Mixed-mode:        {totals['mixed_mode_segments']}")
    print(f"  Excluded evidence: {totals['excluded_from_stage07_evidence']}")

    if failed > 0:
        sys.exit(1)


def _run_directory(in_dir: Path, out_dir: Path, args: argparse.Namespace) -> None:
    files = find_input_files(in_dir)
    if not files:
        print(f"{LOG_PREFIX} No .conversations.json files found in: {in_dir}")
        return
    _run_directory_with_files(files, in_dir, out_dir, args)


def _run_input(args: argparse.Namespace, in_base: Path, out_base: Path) -> None:
    input_path = Path(args.input)
    if not input_path.exists():
        input_path = repo_root() / args.input
    if not input_path.exists():
        raise SystemExit(f"Input not found: {args.input}")
    input_path = input_path.resolve()

    if input_path.is_file():
        quarantine_ids: Set[str] = getattr(args, "_quarantine_ids", set())
        quarantine_reason = get_quarantine_block_reason(input_path, quarantine_ids)
        if quarantine_reason:
            print(f"{LOG_PREFIX} SKIP: {input_path.name} - {quarantine_reason}")
            return
        out_dir = Path(args.output) if args.output else out_base
        output_path = compute_output_path_with_layout(input_path, out_dir, input_root_dir=in_base)
        if output_path.exists() and not args.overwrite:
            print(f"{LOG_PREFIX} Output exists, skipping: {output_path}")
            return
        process_file(input_path, output_path, dry_run=args.dry_run)
        return

    out_dir = Path(args.output) if args.output else out_base
    _run_directory(input_path, out_dir, args)


def _run_sources(args: argparse.Namespace, in_base: Path, out_base: Path) -> None:
    sources_path = repo_root() / args.sources
    if not sources_path.exists():
        raise SystemExit(f"Sources file not found: {sources_path}")
    for src_name, _ in parse_sources_file(sources_path):
        src_in_dir = in_base / src_name
        if not src_in_dir.exists():
            print(f"{LOG_PREFIX} Skipping {src_name}: no 06c.DET.patched output")
            continue
        src_out_dir = out_base / src_name
        files = find_input_files(src_in_dir)
        _run_directory_with_files(files, src_in_dir, src_out_dir, args, output_root_dir=out_base)


def _run_manifest(args: argparse.Namespace, in_base: Path, out_base: Path) -> None:
    manifest_path = Path(args.manifest)
    if not manifest_path.is_absolute():
        manifest_path = repo_root() / manifest_path
    if not manifest_path.exists():
        raise SystemExit(f"Manifest file not found: {manifest_path}")

    sources_map = load_manifest_sources(manifest_path)
    for src_name, vid_ids in sorted(sources_map.items()):
        src_in_dir = in_base / src_name
        if not src_in_dir.exists():
            print(f"{LOG_PREFIX} Skipping {src_name}: no 06c.DET.patched output")
            continue
        src_out_dir = out_base / src_name
        files = manifest_filter_files(find_input_files(src_in_dir), vid_ids)
        if not files:
            print(f"{LOG_PREFIX} Skipping {src_name}: no manifest videos found in input")
            continue
        print(f"{LOG_PREFIX} Manifest: {src_name} ({len(files)} videos)")
        _run_directory_with_files(files, src_in_dir, src_out_dir, args, output_root_dir=out_base)


def _run_named_source(args: argparse.Namespace, in_base: Path, out_base: Path) -> None:
    name = args.name
    in_dir = in_base / name
    if not in_dir.exists():
        raise SystemExit(f"Input directory not found: {in_dir}")
    out_dir = Path(args.output) if args.output else out_base / name
    _run_directory(in_dir, out_dir, args)


def main() -> None:
    parser = argparse.ArgumentParser(
        description="Deterministic Stage 06d sanitization pass"
    )
    parser.add_argument(
        "name", nargs="?",
        help="Source name (folder under data/06c.DET.patched/)"
    )
    parser.add_argument(
        "youtube_url", nargs="?",
        help="YouTube URL (unused, accepted for pipeline compatibility)"
    )
    parser.add_argument("--input", help="Input .conversations.json file or directory")
    parser.add_argument(
        "--input-root",
        help=(
            "Root directory for source/manifest runs "
            "(default: data/06c.DET.patched, or data/test/06c.DET.patched with --test)"
        ),
    )
    parser.add_argument("--output", help="Output directory (defaults to data/06d.DET.sanitized/)")
    parser.add_argument(
        "--test",
        action="store_true",
        help="Process test videos (data/test/06c.DET.patched/)"
    )
    parser.add_argument(
        "--sources",
        nargs="?",
        const="docs/pipeline/sources.txt",
        help="Process all sources from sources.txt file"
    )
    parser.add_argument(
        "--manifest",
        help="Manifest file: only process videos listed"
    )
    parser.add_argument(
        "--dry-run",
        action="store_true",
        help="Preview sanitization without writing"
    )
    parser.add_argument(
        "--overwrite",
        action="store_true",
        help="Overwrite existing output files"
    )
    parser.add_argument(
        "--quarantine-file",
        help="Optional JSON file listing quarantined video IDs to skip",
    )
    args = parser.parse_args()

    args._quarantine_ids = set()
    if args.quarantine_file:
        quarantine_path = Path(args.quarantine_file)
        if not quarantine_path.is_absolute():
            quarantine_path = repo_root() / quarantine_path
        if not quarantine_path.exists():
            raise SystemExit(f"Quarantine file not found: {quarantine_path}")
        args._quarantine_ids = load_quarantine_video_ids(quarantine_path)
        print(f"{LOG_PREFIX} Loaded quarantine file: {quarantine_path} ({len(args._quarantine_ids)} video ids)")

    if args.test:
        in_root = resolve_root_path(args.input_root, test_input_root())
        out_root = Path(args.output) if args.output else test_output_root()
    else:
        in_root = resolve_root_path(args.input_root, input_root())
        out_root = Path(args.output) if args.output else output_root()

    if args.test:
        _run_directory(in_root, out_root, args)
    elif args.manifest:
        _run_manifest(args, in_root, out_root)
    elif args.input:
        _run_input(args, in_root, out_root)
    elif args.sources:
        _run_sources(args, in_root, out_root)
    elif args.name:
        _run_named_source(args, in_root, out_root)
    else:
        raise SystemExit("Provide a source name, --input, --test, --manifest, or --sources")


if __name__ == "__main__":
    main()
