#!/usr/bin/env python3
"""
scripts/training-data/04.segment-enrich

Phase 1: Speaker labeling (LLM) + Tone classification (audio thresholds)

Input:  data/03.audio-features/<source>/<video>/*.audio_features.json
Output: data/04.segment-enrich/<source>/<video>/*.segment_enriched.json

Use:
  ./04.segment-enrich --input PATH           # Process single file
  ./04.segment-enrich --sources              # Process all sources
  ./04.segment-enrich --force                # Reprocess all
  ./04.segment-enrich --dry-run              # Show what would be processed

================================================================================
TONE CLASSIFICATION - THE POINT OF IT ALL
================================================================================

We classify each segment into exactly ONE of 5 tones based on audio features.
Research: docs/overviews/PIPELINE/research/tones/TONES_RESEARCH_SUMMARY.md

THE 5 TONES:
┌────────────┬─────┬─────────────────────────────────────────────────────────┐
│ Tone       │  %  │ What it sounds like                                     │
├────────────┼─────┼─────────────────────────────────────────────────────────┤
│ playful    │ 13% │ Animated, dynamic delivery - pitch varies, high energy  │
│ confident  │ 14% │ Measured, steady delivery - controlled pace and energy  │
│ nervous    │ 14% │ Fast, monotone delivery - rushed speech, flat pitch     │
│ energetic  │ 12% │ High vocal effort/arousal - bright, loud                │
│ neutral    │ 47% │ Modal baseline - none of the above                      │
└────────────┴─────┴─────────────────────────────────────────────────────────┘

DROPPED TONES (and why):
- warm      → No acoustic cluster exists (timbre-based, we don't measure timbre)
- grounded  → Same as confident
- direct    → Semantic, not acoustic
- flirty    → Same as playful

FEATURES USED FOR TONE DETECTION:
- pitch_std (Hz)      : Pitch variability - high = animated, low = monotone
- energy_dyn (dB)     : Loudness dynamics - high = expressive, low = flat
- syllable_rate (/s)  : Speaking speed - high = rushed, low = measured
- brightness (Hz)     : Spectral centroid - high = bright/effortful

================================================================================
"""

from __future__ import annotations

import argparse
import hashlib
import json
import os
import re
import sys
from datetime import datetime, timezone
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple
import requests
import numpy as np

# === Configuration ===
REPO_ROOT = Path(__file__).parent.parent.parent
DATA_ROOT = REPO_ROOT / "data"
INPUT_DIR = DATA_ROOT / "03.audio-features"
OUTPUT_DIR = DATA_ROOT / "04.segment-enrich"
STATE_FILE = DATA_ROOT / ".state" / ".04.segment-enrich.state.json"

OLLAMA_URL = "http://localhost:11434/api/generate"
OLLAMA_MODEL = "llama3.1:latest"
OLLAMA_TIMEOUT = 120

# Version tracking
PIPELINE_VERSION = "2026-01-31"
PROMPT_VERSION = "1.2.0"  # Using speaker embeddings for clustering
SCHEMA_VERSION = "1.0.0"

# =============================================================================
# TONE THRESHOLDS - How we detect each of the 5 tones
# =============================================================================
# See docstring above for the full explanation of why these 5 tones.
#
# Detection logic:
#   playful   = pitch varies a lot (std > 22) AND loud dynamics (dyn > 13)
#   confident = pitch steady (std < 18) AND moderate dynamics AND measured pace
#   nervous   = fast speech (rate > 6.8) AND monotone (std < 16)
#   energetic = bright voice (> 1700Hz) OR very loud dynamics (> 15dB)
#   neutral   = none of the above (this is the default, ~47% of segments)
#
TONE_THRESHOLDS = {
    "playful":   {"pitch_std_min": 22, "energy_dyn_min": 13},
    "confident": {"pitch_std_max": 18, "energy_dyn_min": 8, "energy_dyn_max": 13, "syl_rate_min": 5, "syl_rate_max": 6.5},
    "nervous":   {"syl_rate_min": 6.8, "pitch_std_max": 16},
    "energetic": {"brightness_min": 1700, "energy_dyn_min": 15},  # OR condition (either triggers it)
}


def file_checksum(path: Path) -> str:
    """SHA256 checksum (first 16 chars)."""
    return hashlib.sha256(path.read_bytes()).hexdigest()[:16]


def load_state() -> Dict:
    """Load processing state."""
    if STATE_FILE.exists():
        return json.loads(STATE_FILE.read_text())
    return {"processed": {}, "failed": {}}


def save_state(state: Dict):
    """Save processing state."""
    STATE_FILE.parent.mkdir(parents=True, exist_ok=True)
    STATE_FILE.write_text(json.dumps(state, indent=2))


# === Video Type Pre-Detection ===

# Keywords indicating talking_head (single speaker)
TALKING_HEAD_KEYWORDS = [
    "tips", "how to", "breakdown", "mindset", "analysis", "commentary",
    "mistakes", "hack", "hacks", "advice", "guide", "tutorial",
    "feel good", "confidence", "inner game", "psychology", "lesson",
    "conversations", "conversation", "secret", "secrets", "why", "what is",
    "explained", "simple", "steps", "step", "minutes", "seconds"
]

# Keywords indicating infield (multi-speaker)
INFIELD_KEYWORDS = [
    "infield", "approach", "set", "pickup", "interaction", "number close",
    "kiss close", "pull", "date", "daygame pickup"
]


def detect_video_type_from_title(title: str) -> Tuple[str, float]:
    """
    Pre-detect video type from title keywords.
    Returns (video_type, confidence)

    This runs BEFORE speaker clustering to determine if we should
    expect single or multiple speakers.
    """
    title_lower = title.lower()

    # Check for infield keywords (stronger signal)
    for kw in INFIELD_KEYWORDS:
        if kw in title_lower:
            return "infield", 0.85

    # Check for talking_head keywords
    for kw in TALKING_HEAD_KEYWORDS:
        if kw in title_lower:
            return "talking_head", 0.80

    # Default: assume talking_head (most videos are coach-to-camera)
    # Infield videos almost always have "infield" or "approach" in title
    return "talking_head", 0.60


# =============================================================================
# GLOBAL SPEAKER IDENTIFICATION
# =============================================================================
#
# Single source of truth for speaker labeling. Uses:
# 1. Global clustering (sees all embeddings before deciding)
# 2. Text pattern classification (data-driven, not hardcoded phrases)
# 3. Context-based assignment for segments without embeddings
#
# This replaces the previous split architecture where 03.audio-features did
# incremental clustering and 04 did LLM labeling on broken clusters.
# =============================================================================

# Distance threshold for agglomerative clustering (1 - cosine_similarity)
# Higher threshold = more segments per cluster (less fragmentation)
# 0.5 means segments need ~50% similarity to be in same cluster
# Street audio is noisy, so we need a lenient threshold
CLUSTERING_DISTANCE_THRESHOLD = 0.5


def classify_cluster_by_patterns(texts: List[str]) -> Tuple[str, float]:
    """
    Classify a cluster as coach/target using linguistic patterns.
    Returns (label, confidence)

    Coach indicators: initiates, compliments, leads conversation
    Target indicators: responds, short answers, reactive
    """
    all_text = " ".join(t.lower() for t in texts)
    total_segments = len(texts)

    if total_segments == 0:
        return "other", 0.3

    # Coach signals: initiates, compliments, leads
    coach_signals = 0
    if "you look" in all_text:
        coach_signals += 1
    if "you're beautiful" in all_text or "you're gorgeous" in all_text or "you're cute" in all_text:
        coach_signals += 1
    if "i had to say" in all_text or "i had to come" in all_text:
        coach_signals += 1
    if "what's your name" in all_text or "what is your name" in all_text:
        coach_signals += 1
    if "can i get your" in all_text or "let me get your" in all_text:
        coach_signals += 1
    if "excuse me" in all_text or "sorry to bother" in all_text:
        coach_signals += 1

    # Coach asks questions - count question marks
    question_count = all_text.count("?")
    if question_count > total_segments * 0.3:
        coach_signals += 1

    # Target signals: short responses, reactive
    target_signals = 0

    # Many short responses (< 25 chars)
    short_count = sum(1 for t in texts if len(t.strip()) < 25)
    if short_count > total_segments * 0.5:
        target_signals += 1

    # Common target responses
    target_phrases = ["thank you", "thanks", "okay", "yeah", "oh", "really"]
    for phrase in target_phrases:
        matches = sum(1 for t in texts if t.lower().strip().startswith(phrase))
        if matches >= 2:
            target_signals += 1
            break

    # Few questions (targets answer, not ask)
    if question_count < total_segments * 0.1:
        target_signals += 1

    # Decide based on signals
    if coach_signals >= 2 and coach_signals > target_signals:
        confidence = min(0.7 + coach_signals * 0.05, 0.95)
        return "coach", confidence
    elif target_signals >= 2 and target_signals > coach_signals:
        confidence = min(0.7 + target_signals * 0.05, 0.95)
        return "target", confidence
    elif coach_signals > 0 and target_signals == 0:
        return "coach", 0.6
    elif target_signals > 0 and coach_signals == 0:
        return "target", 0.6
    else:
        # Ambiguous - will be resolved by cluster size (coach talks more)
        return "ambiguous", 0.4


def infer_speaker_from_context(segments: List[Dict], labels: List[str], i: int) -> str:
    """
    Infer speaker for segment without embedding using surrounding context.
    """
    text = segments[i].get("text", "").strip().lower()
    prev_label = labels[i - 1] if i > 0 else None
    prev_text = segments[i - 1].get("text", "") if i > 0 else ""

    # Short response after question = likely target
    if prev_text.strip().endswith("?") and len(text) < 30:
        return "target"

    # Thank you after compliment = target
    compliment_words = ["beautiful", "cute", "pretty", "gorgeous", "stunning"]
    if any(word in prev_text.lower() for word in compliment_words):
        if text.startswith(("oh", "thank", "thanks", "aww")):
            return "target"

    # Name response after "I'm [name]" = target
    if "i'm " in prev_text.lower() and len(text.split()) <= 3 and len(text) < 20:
        if not text.startswith(("i'm", "i am", "nice to", "how")):
            return "target"

    # Continuity: same as previous (if not "other")
    if prev_label and prev_label not in ("other", "ambiguous"):
        return prev_label

    return "other"


def identify_speakers_global(
    segments: List[Dict],
    video_type: str,
    video_title: str
) -> Tuple[List[Dict], Dict[str, str]]:
    """
    Speaker identification using pyannote speaker IDs from transcription.

    Uses pyannote_speaker field (SPEAKER_00, SPEAKER_01, etc.) from 02.transcribe,
    then maps to coach/target/voiceover based on:
    - Speaking time (majority speaker = coach)
    - Text patterns for verification/refinement

    Returns:
        (speaker_info_list, cluster_labels)

    speaker_info_list: List of dicts with {label, confidence, method, cluster_id}
    cluster_labels: Dict mapping pyannote speaker to semantic label
    """
    n_segments = len(segments)

    # Single speaker mode (talking_head)
    if video_type == "talking_head":
        print("    Video type: talking_head - all segments labeled as coach")
        speaker_info = [{
            "label": "coach",
            "confidence": 0.90,
            "method": "video_type_single_speaker",
            "cluster_id": "SPEAKER_00"
        } for _ in range(n_segments)]
        return speaker_info, {"SPEAKER_00": "coach"}

    print(f"    Video type: {video_type} - using pyannote speaker IDs")

    # Count speaking time per pyannote speaker
    speaker_durations: Dict[str, float] = {}
    speaker_segment_counts: Dict[str, int] = {}
    speaker_texts: Dict[str, List[str]] = {}

    for seg in segments:
        pyannote_spk = seg.get("pyannote_speaker") or "UNKNOWN"
        duration = float(seg.get("duration_sec", seg.get("end", 0) - seg.get("start", 0)))
        text = seg.get("text", "")

        speaker_durations[pyannote_spk] = speaker_durations.get(pyannote_spk, 0.0) + duration
        speaker_segment_counts[pyannote_spk] = speaker_segment_counts.get(pyannote_spk, 0) + 1
        if pyannote_spk not in speaker_texts:
            speaker_texts[pyannote_spk] = []
        speaker_texts[pyannote_spk].append(text)

    print(f"    Pyannote speakers: {dict(speaker_segment_counts)}")
    print(f"    Speaking time (s): {dict(speaker_durations)}")

    # Sort speakers by speaking time (descending)
    sorted_speakers = sorted(speaker_durations.keys(), key=lambda s: -speaker_durations[s])

    # Map pyannote speakers to semantic labels
    # Majority speaker = coach, minority speaker(s) = target
    # Use text pattern analysis to verify/refine
    speaker_label_map: Dict[str, Tuple[str, float]] = {}

    for i, spk in enumerate(sorted_speakers):
        texts = speaker_texts.get(spk, [])
        pattern_label, pattern_conf = classify_cluster_by_patterns(texts)

        if i == 0:  # Majority speaker
            if pattern_label == "target":
                # Text patterns strongly suggest target, but this is majority speaker
                # This is unusual - might be a compilation video. Use text analysis.
                label = pattern_label
                confidence = pattern_conf * 0.8  # Lower confidence due to conflict
                print(f"      {spk}: majority but text suggests target (unusual)")
            else:
                # Majority speaker is coach (expected)
                label = "coach"
                confidence = max(0.85, pattern_conf)
        elif i == 1:  # Second speaker (minority)
            if pattern_label == "coach":
                # Text patterns suggest coach, but this is minority speaker
                # Could be voiceover or secondary commentary
                label = "voiceover"
                confidence = 0.6
                print(f"      {spk}: minority but text suggests coach -> voiceover")
            else:
                # Minority speaker is target (expected for infield)
                label = "target"
                confidence = max(0.75, pattern_conf)
        else:  # Third+ speakers
            # Rare - label as "other"
            label = "other"
            confidence = 0.5

        speaker_label_map[spk] = (label, confidence)
        print(f"      {spk}: {len(texts)} segments, {speaker_durations[spk]:.1f}s -> {label} ({confidence:.2f})")

    # Handle UNKNOWN speaker
    if "UNKNOWN" in speaker_label_map:
        # UNKNOWN segments - use text patterns or default to coach
        speaker_label_map["UNKNOWN"] = ("coach", 0.4)

    # Build speaker_info for each segment
    speaker_info: List[Dict] = []
    for seg in segments:
        pyannote_spk = seg.get("pyannote_speaker") or "UNKNOWN"
        label, confidence = speaker_label_map.get(pyannote_spk, ("coach", 0.4))

        # Apply text-based refinement for short segments
        text = seg.get("text", "").lower()
        duration = float(seg.get("duration_sec", seg.get("end", 0) - seg.get("start", 0)))

        # Short segments with typical target responses should be target
        # even if pyannote assigned them to majority speaker
        target_phrases = [
            "thank you", "thanks", "okay", "oh okay", "yeah", "yes",
            "nice to meet you", "nice meeting you", "i'm ", "my name is",
            "it's good", "i do", "i don't", "i'm from", "i live"
        ]

        if duration < 2.0 and label == "coach":
            for phrase in target_phrases:
                if phrase in text and len(text) < 50:
                    # This looks like a target response
                    label = "target"
                    confidence = 0.70
                    break

        speaker_info.append({
            "label": label,
            "confidence": round(confidence, 2),
            "method": "pyannote_speaker_id",
            "cluster_id": pyannote_spk
        })

    # Build final cluster_labels dict
    final_cluster_labels = {
        spk: lbl for spk, (lbl, _) in speaker_label_map.items()
    }

    return speaker_info, final_cluster_labels


# === Tone Classification (Audio Thresholds) ===

def classify_tone_window(features: Dict) -> Tuple[str, float]:
    """
    Classify tone using audio feature thresholds.
    Returns (tone, confidence)
    """
    pitch = features.get("pitch", {})
    energy = features.get("energy", {})
    tempo = features.get("tempo", {})
    spectral = features.get("spectral", {})

    pitch_std = pitch.get("std_hz", 0)
    energy_dyn = energy.get("dynamics_db", 0)
    syl_rate = tempo.get("syllable_rate", 0)
    brightness = spectral.get("brightness_hz", 0)

    scores = {}

    # Playful: pitch_std > 22 AND energy_dyn > 13
    if pitch_std > 22 and energy_dyn > 13:
        score = min((pitch_std - 22) / 10, 1) * 0.5 + min((energy_dyn - 13) / 5, 1) * 0.5
        scores["playful"] = score

    # Confident: pitch_std < 18 AND energy_dyn 8-13 AND syl_rate 5-6.5
    if pitch_std < 18 and 8 <= energy_dyn <= 13 and 5 <= syl_rate <= 6.5:
        score = (1 - pitch_std / 18) * 0.4 + 0.3 + 0.3  # Simplified
        scores["confident"] = min(score, 1.0)

    # Nervous: syl_rate > 6.8 AND pitch_std < 16
    if syl_rate > 6.8 and pitch_std < 16:
        score = min((syl_rate - 6.8) / 2, 1) * 0.5 + (1 - pitch_std / 16) * 0.5
        scores["nervous"] = score

    # Energetic: brightness > 1700 OR energy_dyn > 15
    if brightness > 1700 or energy_dyn > 15:
        b_score = min((brightness - 1700) / 500, 1) if brightness > 1700 else 0
        e_score = min((energy_dyn - 15) / 5, 1) if energy_dyn > 15 else 0
        scores["energetic"] = max(b_score, e_score)

    # Default to neutral if no strong signals
    if not scores or max(scores.values()) < 0.3:
        return "neutral", 0.7

    # Return highest scoring tone
    best_tone = max(scores.keys(), key=lambda k: scores[k])
    return best_tone, min(scores[best_tone], 1.0)


def build_tone_windows(segments: List[Dict], window_sec: float = 30, hop_sec: float = 10) -> List[Dict]:
    """
    Build tone windows with aggregated features.
    """
    if not segments:
        return []

    total_duration = segments[-1]["end"]
    windows = []
    window_id = 0

    start = 0
    while start < total_duration:
        end = min(start + window_sec, total_duration)

        # Get segments in this window
        window_segs = [s for s in segments if s["start"] < end and s["end"] > start]

        if window_segs:
            # Aggregate features (weighted by duration in window)
            agg_features = aggregate_features(window_segs, start, end)
            tone, confidence = classify_tone_window(agg_features)

            windows.append({
                "id": window_id,
                "start": round(start, 2),
                "end": round(end, 2),
                "tone": {
                    "primary": tone,
                    "confidence": round(confidence, 2),
                    "method": "audio_threshold"
                }
            })
            window_id += 1

        start += hop_sec

    return windows


def aggregate_features(segments: List[Dict], win_start: float, win_end: float) -> Dict:
    """Aggregate features across segments in a window."""
    pitch_stds = []
    energy_dyns = []
    syl_rates = []
    brightnesses = []

    for seg in segments:
        f = seg.get("features", {})
        if f.get("pitch", {}).get("std_hz", 0) > 0:
            pitch_stds.append(f["pitch"]["std_hz"])
        if f.get("energy", {}).get("dynamics_db", 0) > 0:
            energy_dyns.append(f["energy"]["dynamics_db"])
        if f.get("tempo", {}).get("syllable_rate", 0) > 0:
            syl_rates.append(f["tempo"]["syllable_rate"])
        if f.get("spectral", {}).get("brightness_hz", 0) > 0:
            brightnesses.append(f["spectral"]["brightness_hz"])

    return {
        "pitch": {"std_hz": sum(pitch_stds) / len(pitch_stds) if pitch_stds else 0},
        "energy": {"dynamics_db": sum(energy_dyns) / len(energy_dyns) if energy_dyns else 0},
        "tempo": {"syllable_rate": sum(syl_rates) / len(syl_rates) if syl_rates else 0},
        "spectral": {"brightness_hz": sum(brightnesses) / len(brightnesses) if brightnesses else 0},
    }


# NOTE: Turn-taking corrections removed - now integrated into identify_speakers_global()
# via the infer_speaker_from_context() function which handles short responses,
# name exchanges, and compliment responses as part of the main speaker ID flow.


# === Main Processing ===

def process_file(input_path: Path, force: bool = False) -> Optional[Path]:
    """Process a single audio features file."""
    # Determine output path
    rel_path = input_path.relative_to(INPUT_DIR)
    output_path = OUTPUT_DIR / rel_path.parent / rel_path.name.replace(".audio_features.json", ".segment_enriched.json")

    # Skip if already processed (unless force)
    if output_path.exists() and not force:
        print(f"  Skipping (exists): {output_path.name}")
        return output_path

    # Load input
    data = json.loads(input_path.read_text())
    segments = data.get("segments", [])

    if not segments:
        print(f"  Skipping (no segments): {input_path.name}")
        return None

    # Extract video title from path
    video_title = input_path.parent.name.split(" [")[0]

    print(f"  Processing: {video_title[:50]}...")
    print(f"    Segments: {len(segments)}")

    # 1. Pre-detect video type from title
    detected_video_type, type_confidence = detect_video_type_from_title(video_title)

    # 2. Global speaker identification (single source of truth)
    # Uses embeddings + text patterns, handles context inference for missing embeddings
    speaker_info_list, cluster_labels = identify_speakers_global(
        segments, detected_video_type, video_title
    )

    # Count labels for summary
    label_counts = {}
    for info in speaker_info_list:
        lbl = info["label"]
        label_counts[lbl] = label_counts.get(lbl, 0) + 1
    print(f"    Speaker labels: {label_counts}")

    # 3. Build enriched segments
    enriched_segments = []
    for i, seg in enumerate(segments):
        info = speaker_info_list[i]
        enriched_segments.append({
            "id": i,
            "start": seg["start"],
            "end": seg["end"],
            "text": seg.get("text", ""),
            "speaker": {
                "label": info["label"],
                "confidence": info["confidence"],
                "method": info["method"]
            },
            "speaker_id": info["cluster_id"]
        })

    # 4. Build tone windows
    tone_windows = build_tone_windows(segments)
    print(f"    Tone windows: {len(tone_windows)}")

    # 5. Build output
    output = {
        "video_id": input_path.parent.name,
        "source_file": str(input_path),
        "processed_at": datetime.now(timezone.utc).isoformat(),
        "segments": enriched_segments,
        "tone_windows": tone_windows,
        "speaker_cluster_labels": cluster_labels,
        "detected_video_type": detected_video_type,
        "metadata": {
            "pipeline_version": PIPELINE_VERSION,
            "prompt_version": PROMPT_VERSION,
            "speaker_id_version": "2.0.0-global-clustering",  # New versioning
            "schema_version": SCHEMA_VERSION,
            "input_checksum": file_checksum(input_path)
        }
    }

    # Write output
    output_path.parent.mkdir(parents=True, exist_ok=True)
    output_path.write_text(json.dumps(output, indent=2))

    print(f"    Output: {output_path}")
    return output_path


def find_input_files(source: Optional[str] = None) -> List[Path]:
    """Find all audio feature files to process."""
    if source:
        source_dir = INPUT_DIR / source
    else:
        source_dir = INPUT_DIR

    files = []
    for json_file in source_dir.rglob("*.audio_features.json"):
        files.append(json_file)

    return sorted(files)


def main():
    parser = argparse.ArgumentParser(description="Speaker + Tone enrichment for segments")
    parser.add_argument("--input", type=Path, help="Process single file")
    parser.add_argument("--source", type=str, help="Process specific source folder")
    parser.add_argument("--sources", action="store_true", help="Process all sources")
    parser.add_argument("--force", action="store_true", help="Reprocess all files")
    parser.add_argument("--dry-run", action="store_true", help="Show what would be processed")
    parser.add_argument("--limit", type=int, help="Limit number of files to process")

    args = parser.parse_args()

    # Determine files to process
    if args.input:
        files = [args.input]
    elif args.source:
        files = find_input_files(args.source)
    elif args.sources:
        files = find_input_files()
    else:
        parser.print_help()
        return

    if args.limit:
        files = files[:args.limit]

    print(f"Found {len(files)} files to process")

    if args.dry_run:
        for f in files:
            print(f"  Would process: {f}")
        return

    # Process files
    success = 0
    failed = 0

    for f in files:
        try:
            result = process_file(f, force=args.force)
            if result:
                success += 1
        except Exception as e:
            print(f"  ERROR: {f.name}: {e}", file=sys.stderr)
            failed += 1

    print(f"\nComplete: {success} success, {failed} failed")


if __name__ == "__main__":
    main()
