#!/usr/bin/env python3
# scripts/training-data/02b.clean-transcribed
#
# STEP 2B — CONSENSUS MERGE + QC (ROVER-style voting + Ollama + audio rescoring)
#
# SAFETY GUARANTEE:
# - This script NEVER deletes anything under <repo>/data/.
# - Temporary segment WAVs are created ONLY under a dedicated temp base (default: /tmp/daygame-coach-02b)
#   and cleaned up automatically.
#
# Goal:
# - Take TWO transcript engine outputs (from Step 02):
#     1) openai-whisper   -> *.full.whisper.json
#     2) faster-whisper   -> *.full.faster.json
#
# - Produce ONE merged transcript per video folder using:
#     - token alignment (ROVER-ish) when engines broadly agree
#     - whole-text selection fallback when engines disagree strongly
#     - NO disfluency cleanup (um/uh/repeats are preserved)
#
# - Run Ollama llama3.2 for QC:
#     - flag weird / low-agreement spans
#     - propose candidates (DO NOT auto-apply)
#
# - Do audio-based rescoring per segment:
#     - slice segment audio (with padding)
#     - run faster-whisper on the slice
#     - compute mismatch score vs consensus
#     - flag suspicious segments
#
# Reads (from STEP 02 output):
#   data/02.transcribe/<source>/<video_dir>/
#     <video_dir>.full.whisper.json
#     <video_dir>.full.faster.json
#
# Also reads original audio (for rescoring):
#   data/01.download/<source>/<video_dir>/
#     *.audio.asr.clean16k.wav   (preferred)
#     *.audio.asr.raw16k.wav
#     *.wav
#
# Writes:
#   data/02b.clean-transcribed/<source>/<video_dir>/<video_dir>.consensus.json
#   data/02b.clean-transcribed/<source>/<video_dir>/<video_dir>.consensus.txt
#   data/02b.clean-transcribed/<source>/<video_dir>/<video_dir>.scores.json
#   data/02b.clean-transcribed/<source>/<video_dir>/<video_dir>.qc_report.json
#
# Notes:
# - Consensus merge happens FIRST.
# - Ollama only flags + proposes candidates AFTER merge.
# - QC suggestions are NOT applied automatically.

import argparse
import json
import os
import re
import shlex
import subprocess
import tempfile
import urllib.request
import urllib.error
from contextlib import contextmanager
from dataclasses import dataclass
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple, Iterator

import numpy as np

try:
    import torch  # type: ignore
except Exception:
    torch = None

try:
    import torchaudio  # type: ignore
except Exception:
    torchaudio = None

try:
    import soundfile as sf  # type: ignore
except Exception:
    sf = None

try:
    import librosa  # type: ignore
except Exception:
    librosa = None


# --------------------------
# Utilities
# --------------------------

def repo_root() -> Path:
    # scripts/training-data/02b.clean-transcribed -> parents[2] == repo root
    root = Path(__file__).resolve().parents[2]
    if root == Path("/"):
        raise SystemExit("[02b] Refusing to run: computed repo_root is '/'.")
    return root


def safe_name(name: str) -> str:
    cleaned = re.sub(r"[^A-Za-z0-9._-]+", "_", (name or "").strip())
    return cleaned.strip("_") or "source"


def extract_video_id(url: str) -> Optional[str]:
    url = (url or "").strip()
    if not url:
        return None
    m = re.search(r"[?&]v=([^&]+)", url)
    if m:
        return m.group(1)
    m = re.search(r"youtu\.be/([^?&/]+)", url)
    if m:
        return m.group(1)
    return None


def parse_sources_file(path: Path) -> List[Tuple[str, str]]:
    sources: List[Tuple[str, str]] = []
    for raw in path.read_text(encoding="utf-8").splitlines():
        line = raw.strip()
        if not line or line.startswith("#"):
            continue
        if "|" in line:
            name, url = line.split("|", 1)
            name = name.strip()
            url = url.strip()
            if name and url:
                sources.append((name, url))
            continue
        parts = shlex.split(line)
        if len(parts) >= 2:
            sources.append((parts[0], parts[1]))
    return sources


def read_json(path: Path) -> Dict[str, Any]:
    return json.loads(path.read_text(encoding="utf-8"))


def write_json(path: Path, data: Any) -> None:
    path.parent.mkdir(parents=True, exist_ok=True)
    path.write_text(json.dumps(data, ensure_ascii=False, indent=2) + "\n", encoding="utf-8")


def write_txt(path: Path, text: str) -> None:
    path.parent.mkdir(parents=True, exist_ok=True)
    t = (text or "").strip()
    path.write_text(t + ("\n" if t else ""), encoding="utf-8")


def now_iso() -> str:
    return datetime.utcnow().replace(microsecond=0).isoformat() + "Z"


def segments_ok(transcript: Dict[str, Any]) -> bool:
    segs = transcript.get("segments")
    return isinstance(segs, list) and len(segs) > 0


def clamp_float(x: Any, lo: float, hi: float, default: float) -> float:
    try:
        v = float(x)
    except Exception:
        return default
    return max(lo, min(hi, v))


def shutil_which(cmd: str) -> Optional[str]:
    for p in os.environ.get("PATH", "").split(os.pathsep):
        exe = Path(p) / cmd
        if exe.exists() and os.access(str(exe), os.X_OK):
            return str(exe)
    return None


def ensure_under_dir(path: Path, parent: Path, label: str) -> None:
    """
    Safety helper: guarantees `path` is under `parent` (after resolve).
    """
    try:
        pr = parent.resolve()
        rr = path.resolve()
    except Exception:
        raise SystemExit(f"[02b] Refusing: could not resolve paths for {label}.")
    if pr == rr or pr not in rr.parents:
        raise SystemExit(
            f"[02b] Refusing: {label} is not under expected dir.\n  {label}={rr}\n  parent={pr}"
        )


# --------------------------
# Audio helpers
# --------------------------

def wav_duration_sec_fast(path: Path) -> Optional[float]:
    try:
        import wave
        with wave.open(str(path), "rb") as wf:
            frames = wf.getnframes()
            rate = wf.getframerate()
            if rate <= 0:
                return None
            return float(frames) / float(rate)
    except Exception:
        return None


def load_audio_mono(path: str) -> Tuple[np.ndarray, int]:
    """
    Mono loader:
      1) torchaudio
      2) soundfile
      3) librosa
    """
    if torchaudio is not None and torch is not None:
        try:
            waveform, sr = torchaudio.load(path)
            waveform = waveform.to(torch.float32)
            if waveform.ndim == 2 and waveform.shape[0] > 1:
                waveform = waveform.mean(dim=0, keepdim=True)
            if waveform.ndim == 2:
                waveform = waveform.squeeze(0)
            y = waveform.detach().cpu().numpy().astype(np.float32)
            return y, int(sr)
        except Exception:
            pass

    if sf is not None:
        y, sr = sf.read(path, dtype="float32", always_2d=True)
        if y.shape[1] > 1:
            y = y.mean(axis=1, keepdims=True)
        y = y[:, 0].astype(np.float32)
        return y, int(sr)

    if librosa is not None:
        y, sr = librosa.load(path, sr=None, mono=True)
        return y.astype(np.float32), int(sr)

    raise SystemExit("No audio loader available. Install torchaudio OR soundfile OR librosa.")


def write_wav_mono_16k(path: Path, y: np.ndarray, sr: int) -> None:
    """
    Writes WAV PCM16 mono @ 16k.
    """
    if librosa is None:
        raise SystemExit("librosa required for resampling. Install librosa.")
    if sr != 16000:
        y = librosa.resample(y.astype(np.float32), orig_sr=sr, target_sr=16000).astype(np.float32)
        sr = 16000

    y = np.clip(y, -1.0, 1.0)
    pcm16 = (y * 32767.0).astype(np.int16)

    if sf is None:
        raise SystemExit("soundfile required for wav writing. Install soundfile.")
    sf.write(str(path), pcm16, sr, subtype="PCM_16")


def pick_best_audio_in_download_dir(video_download_dir: Path, prefer: str = "clean") -> Optional[Path]:
    clean = sorted(video_download_dir.glob("*.audio.asr.clean16k.wav"))
    raw = sorted(video_download_dir.glob("*.audio.asr.raw16k.wav"))
    legacy = sorted(video_download_dir.glob("*.wav"))

    if prefer == "raw" and raw:
        return raw[0]
    if prefer == "legacy" and legacy:
        return legacy[0]

    if clean:
        clean_path = clean[0]
        if raw:
            raw_path = raw[0]
            d_clean = wav_duration_sec_fast(clean_path)
            d_raw = wav_duration_sec_fast(raw_path)
            if d_clean is not None and d_raw is not None and d_clean < (0.95 * d_raw):
                return raw_path
        return clean_path

    if raw:
        return raw[0]
    if legacy:
        return legacy[0]
    return None


def default_temp_base() -> Path:
    """
    Force temp base away from repo/data.
    Default to /tmp/daygame-coach-02b, ignoring TMPDIR.
    Override with DAYGAME_TMP_BASE if desired.
    """
    base = Path(os.environ.get("DAYGAME_TMP_BASE", "/tmp")) / "daygame-coach-02b"
    base.mkdir(parents=True, exist_ok=True)
    return base


@contextmanager
def temp_segment_wav_16k(
    y_full: np.ndarray,
    sr_full: int,
    start_sec: float,
    end_sec: float,
    temp_base: Path,
) -> Iterator[Path]:
    """
    Creates a temporary WAV (mono, 16k) for [start_sec, end_sec] under temp_base,
    then deletes it automatically.

    SAFETY: temp_base is never under repo/data unless explicitly overridden.
    """
    start_sec = max(0.0, float(start_sec))
    end_sec = max(start_sec, float(end_sec))
    i0 = int(round(start_sec * sr_full))
    i1 = int(round(end_sec * sr_full))
    i0 = max(0, min(i0, len(y_full)))
    i1 = max(0, min(i1, len(y_full)))
    chunk = y_full[i0:i1].astype(np.float32)

    with tempfile.TemporaryDirectory(prefix="seg_audio_", dir=str(temp_base)) as td:
        out_wav = Path(td) / "seg.wav"
        write_wav_mono_16k(out_wav, chunk, sr_full)
        yield out_wav


# --------------------------
# Overlap helpers
# --------------------------

def overlap(a0: float, a1: float, b0: float, b1: float) -> float:
    return max(0.0, min(a1, b1) - max(a0, b0))


# --------------------------
# Tokenization + alignment voting (ROVER-ish)
# --------------------------

# Keep contractions together: don't / I'm / we'll etc (supports ASCII ' and unicode ’)
_TOKEN_RE = re.compile(r"\w+(?:[’']\w+)*|[^\w\s]", re.UNICODE)

def tokenize(text: str) -> List[str]:
    text = (text or "").strip()
    if not text:
        return []
    return _TOKEN_RE.findall(text)


def detokenize(tokens: List[str]) -> str:
    if not tokens:
        return ""
    out: List[str] = []
    for t in tokens:
        if not out:
            out.append(t)
            continue
        if re.match(r"^[,.;:!?)]$", t):
            out[-1] = out[-1] + t
        elif re.match(r"^[(]$", out[-1]):
            out.append(t)
        else:
            out.append(" " + t)
    return "".join(out).strip()


def needleman_wunsch(a: List[str], b: List[str]) -> Tuple[List[Optional[str]], List[Optional[str]]]:
    MATCH = 2
    MISMATCH = -1
    GAP = -1

    n = len(a)
    m = len(b)

    dp = np.zeros((n + 1, m + 1), dtype=np.int32)
    bt = np.zeros((n + 1, m + 1), dtype=np.int8)  # 0 diag, 1 up, 2 left

    for i in range(1, n + 1):
        dp[i, 0] = dp[i - 1, 0] + GAP
        bt[i, 0] = 1
    for j in range(1, m + 1):
        dp[0, j] = dp[0, j - 1] + GAP
        bt[0, j] = 2

    for i in range(1, n + 1):
        ai = a[i - 1]
        for j in range(1, m + 1):
            bj = b[j - 1]
            score_diag = dp[i - 1, j - 1] + (MATCH if ai == bj else MISMATCH)
            score_up = dp[i - 1, j] + GAP
            score_left = dp[i, j - 1] + GAP
            best = score_diag
            move = 0
            if score_up > best:
                best = score_up
                move = 1
            if score_left > best:
                best = score_left
                move = 2
            dp[i, j] = best
            bt[i, j] = move

    i, j = n, m
    aa: List[Optional[str]] = []
    bb: List[Optional[str]] = []

    while i > 0 or j > 0:
        move = bt[i, j]
        if i > 0 and j > 0 and move == 0:
            aa.append(a[i - 1])
            bb.append(b[j - 1])
            i -= 1
            j -= 1
        elif i > 0 and (j == 0 or move == 1):
            aa.append(a[i - 1])
            bb.append(None)
            i -= 1
        else:
            aa.append(None)
            bb.append(b[j - 1])
            j -= 1

    aa.reverse()
    bb.reverse()
    return aa, bb


def progressive_rover_vote(
    engine_tokens: Dict[str, List[str]],
    weights: Dict[str, float],
    backbone_engine: str,
) -> Tuple[List[str], float]:
    if not engine_tokens:
        return [], 0.0

    if backbone_engine not in engine_tokens:
        backbone_engine = next(iter(engine_tokens.keys()))

    backbone = engine_tokens[backbone_engine]

    columns: List[Dict[str, Optional[str]]] = []
    for t in backbone:
        columns.append({"backbone": t, backbone_engine: t})

    def current_backbone_tokens(cols: List[Dict[str, Optional[str]]]) -> List[str]:
        return [str(c.get("backbone") or "") for c in cols]

    for eng, toks in engine_tokens.items():
        if eng == backbone_engine:
            continue

        bb = current_backbone_tokens(columns)
        aligned_bb, aligned_eng = needleman_wunsch(bb, toks)

        col_idx = 0
        new_columns: List[Dict[str, Optional[str]]] = []
        for bb_tok, e_tok in zip(aligned_bb, aligned_eng):
            if bb_tok is None and e_tok is not None:
                new_columns.append({"backbone": "", eng: e_tok})
            elif bb_tok is not None:
                col = dict(columns[col_idx])
                col[eng] = e_tok
                new_columns.append(col)
                col_idx += 1

        columns = new_columns

    consensus: List[str] = []
    agreeing = 0
    considered = 0
    engines = list(engine_tokens.keys())

    for col in columns:
        cand_weights: Dict[str, float] = {}
        present = 0
        for eng in engines:
            tok = col.get(eng)
            if tok is None or tok == "":
                continue
            present += 1
            w = float(weights.get(eng, 1.0))
            cand_weights[tok] = cand_weights.get(tok, 0.0) + w

        if present == 0:
            continue

        best_tok = max(cand_weights.items(), key=lambda kv: kv[1])[0]
        consensus.append(best_tok)

        col_agree = 0
        for eng in engines:
            tok = col.get(eng)
            if tok is None or tok == "":
                continue
            if tok == best_tok:
                col_agree += 1

        considered += present
        agreeing += col_agree

    agreement_score = float(agreeing / considered) if considered > 0 else 0.0
    return consensus, agreement_score


# --------------------------
# Window text extraction (CRITICAL FIX)
# --------------------------

def gather_text_for_window(segments: List[Dict[str, Any]], s: float, e: float) -> str:
    """
    Extract text for a [s,e] window.

    Priority:
      1) Word-level timestamps if available (segments[*].words)
      2) Otherwise select ONE best matching segment (no concatenation)
         using a heuristic that prefers:
           - segments containing the window midpoint
           - shorter segment duration
           - higher overlap fraction
    """
    s = float(s)
    e = float(e)
    mid = 0.5 * (s + e)
    win_len = max(1e-6, e - s)

    # 1) Word-level path (best)
    word_tokens: List[str] = []
    saw_words = False
    for seg in segments:
        wlist = seg.get("words")
        if isinstance(wlist, list) and wlist:
            saw_words = True
            for w in wlist:
                try:
                    ws = float(w.get("start", 0.0))
                    we = float(w.get("end", ws))
                except Exception:
                    continue
                if overlap(s, e, ws, we) <= 0.0:
                    continue
                wt = (w.get("word") or w.get("text") or "").strip()
                if wt:
                    word_tokens.append(wt)

    if saw_words:
        return detokenize(tokenize(" ".join(word_tokens))).strip()

    # 2) Segment-level fallback: choose ONE best segment (NO concatenation)
    candidates: List[Tuple[float, float, float, str]] = []
    # tuple = (contains_mid(0/1), overlap_fraction, -seg_len, text)
    for seg in segments:
        try:
            fs = float(seg.get("start", 0.0))
            fe = float(seg.get("end", 0.0))
        except Exception:
            continue
        ov = overlap(s, e, fs, fe)
        if ov <= 0.0:
            continue
        seg_len = max(1e-6, fe - fs)
        contains_mid = 1.0 if (fs <= mid <= fe) else 0.0
        ov_frac = ov / max(seg_len, win_len)  # normalized-ish
        txt = (seg.get("text") or "").strip()
        if txt:
            candidates.append((contains_mid, ov_frac, -seg_len, txt))

    if not candidates:
        return ""

    candidates.sort(reverse=True)
    return candidates[0][3]


# --------------------------
# Audio rescoring
# --------------------------

def _norm_token(w: str) -> str:
    w = (w or "").lower().strip()
    # simple synonym normalization for ultra-short backchannels
    mapping = {
        "yeah": "yes",
        "yep": "yes",
        "yah": "yes",
        "yea": "yes",
        "yup": "yes",
        "mm": "mhm",
        "mhm": "mhm",
        "uhhuh": "mhm",
        "uh-huh": "mhm",
        "uh": "uh",
        "um": "um",
        "ok": "okay",
    }
    return mapping.get(w, w)


def normalized_word_edit_distance(a: str, b: str) -> float:
    wa = [_norm_token(w) for w in re.findall(r"\w+", a or "")]
    wb = [_norm_token(w) for w in re.findall(r"\w+", b or "")]
    if not wa and not wb:
        return 0.0
    if not wa or not wb:
        return 1.0

    # ultra-short guard: treat yes/yeah etc as equal after normalization
    if len(wa) == 1 and len(wb) == 1 and wa[0] == wb[0]:
        return 0.0

    n = len(wa)
    m = len(wb)
    dp = np.zeros((n + 1, m + 1), dtype=np.int32)
    for i in range(n + 1):
        dp[i, 0] = i
    for j in range(m + 1):
        dp[0, j] = j

    for i in range(1, n + 1):
        for j in range(1, m + 1):
            cost = 0 if wa[i - 1] == wb[j - 1] else 1
            dp[i, j] = min(
                dp[i - 1, j] + 1,
                dp[i, j - 1] + 1,
                dp[i - 1, j - 1] + cost,
            )

    dist = dp[n, m]
    denom = max(n, m)
    return float(dist / denom) if denom > 0 else 0.0


class SegmentRescorer:
    def __init__(self, model_name: str = "base", device: str = "cuda", compute_type: str = "float16"):
        try:
            from faster_whisper import WhisperModel  # type: ignore
        except Exception:
            raise SystemExit("Audio rescoring requires faster-whisper. Install: pip install -U faster-whisper")

        self.model_name = model_name
        self.device = device
        self.compute_type = compute_type
        self._model = WhisperModel(model_name, device=device, compute_type=compute_type)

    def transcribe_wav(self, wav_path: str, language: str = "en") -> Tuple[str, float]:
        segments, _info = self._model.transcribe(
            wav_path,
            language=language,
            task="transcribe",
            beam_size=5,
            temperature=0.0,
            condition_on_previous_text=False,
        )

        parts: List[str] = []
        avg_logprobs: List[float] = []
        for seg in segments:
            txt = (seg.text or "").strip()
            if txt:
                parts.append(txt)
            al = getattr(seg, "avg_logprob", None)
            if isinstance(al, (int, float)):
                avg_logprobs.append(float(al))

        text = " ".join(parts).strip()
        avg_lp = float(np.mean(avg_logprobs)) if avg_logprobs else 0.0
        return text, avg_lp


# --------------------------
# Ollama QC prompts
# --------------------------

QC_SYSTEM = """You are a transcript QC assistant.
You do NOT rewrite the transcript automatically.
You only:
1) FLAG weird spans (garbled, contradictions, obvious mishears)
2) PROPOSE candidate text options (prefer engine options, do not invent)

Rules:
- Keep disfluencies (um/uh/repeats) if they exist.
- Do NOT "polish" grammar.
- If unsure, just flag and give low confidence.
Return strict JSON only (no markdown, no extra text).
"""

QC_USER_TEMPLATE = """You are checking merged transcript segments.

For each segment you receive:
- idx, start, end
- consensus_text (merged output)
- engine_texts: whisper / faster
- agreement_score (0..1)
- audio_rescore_text (if available)
- audio_mismatch (0..1) where 1 is bad mismatch

Return JSON in this schema:

{{
  "segments": [
    {{
      "idx": 0,
      "issue_level": "none" | "low" | "medium" | "high",
      "reasons": ["short strings..."],
      "candidates": [
        {{
          "text": "candidate transcript text for this segment",
          "source": "whisper|faster|audio_rescore|mixed",
          "confidence": 0-100
        }}
      ]
    }}
  ]
}}

Constraints:
- If agreement_score is high and audio_mismatch is low, issue_level should be "none" or "low".
- Prefer candidates that match one of the engine texts exactly.
- If you propose a "mixed" candidate, it must only use words already present in the provided texts.

Here are the segments:
{items_json}
"""


def ollama_generate_json(
    api_url: str,
    model: str,
    system: str,
    prompt: str,
    timeout_sec: int,
    temperature: float = 0.0,
) -> Dict[str, Any]:
    url = api_url.rstrip("/") + "/api/generate"
    payload = {
        "model": model,
        "prompt": prompt,
        "system": system,
        "stream": False,
        "format": "json",
        "options": {"temperature": float(temperature)},
    }

    data = json.dumps(payload).encode("utf-8")
    req = urllib.request.Request(
        url=url,
        data=data,
        headers={"Content-Type": "application/json"},
        method="POST",
    )

    try:
        with urllib.request.urlopen(req, timeout=timeout_sec) as resp:
            body = resp.read().decode("utf-8", errors="replace")
    except urllib.error.URLError as e:
        raise RuntimeError(f"Ollama API error: {e}") from e

    obj = json.loads(body)
    raw = (obj.get("response") or "").strip()
    if not raw:
        raise RuntimeError("Ollama API returned empty response")
    return json.loads(raw)


def run_ollama_cli(model: str, full_prompt: str, timeout_sec: int) -> str:
    cmd = ["ollama", "run", model]
    proc = subprocess.run(
        cmd,
        input=full_prompt,
        stdout=subprocess.PIPE,
        stderr=subprocess.PIPE,
        text=True,
        timeout=timeout_sec,
    )
    if proc.returncode != 0:
        raise RuntimeError(f"ollama CLI failed (code={proc.returncode})\nSTDERR:\n{proc.stderr.strip()}")
    return proc.stdout.strip()


def extract_json_object(s: str) -> Dict[str, Any]:
    start = s.find("{")
    end = s.rfind("}")
    if start == -1 or end == -1 or end <= start:
        raise ValueError("No JSON object found in Ollama output")
    blob = s[start:end + 1]
    return json.loads(blob)


def chunk_list(xs: List[Any], n: int) -> List[List[Any]]:
    if n <= 0:
        return [xs]
    out: List[List[Any]] = []
    for i in range(0, len(xs), n):
        out.append(xs[i:i + n])
    return out


# --------------------------
# Transcript discovery (Step 02 outputs)
# --------------------------

def find_engine_transcripts(video_dir: Path) -> Dict[str, Path]:
    """
    Looks for:
      <video>.full.whisper.json
      <video>.full.faster.json
    """
    out: Dict[str, Path] = {}
    for eng in ("whisper", "faster"):
        p = video_dir / f"{video_dir.name}.full.{eng}.json"
        if p.exists():
            out[eng] = p
    return out


# --------------------------
# Core processing
# --------------------------

@dataclass
class Settings:
    overwrite: bool
    verbose: bool

    timeline_engine: str = "whisper"

    # voting weights
    weight_whisper: float = 1.0
    weight_faster: float = 1.0

    # whole-text fallback (prevents token-level hybrid garbage when engines disagree)
    whole_text_fallback_dist: float = 0.45
    whole_text_fallback_len_ratio: float = 1.8

    # audio rescoring
    audio_rescore: bool = True
    audio_prefer: str = "clean"
    rescore_model: str = "base"
    rescore_device: str = "cuda"
    rescore_compute_type: str = "float16"
    rescore_min_dur_sec: float = 1.2
    rescore_pad_sec: float = 0.25

    # thresholds
    low_agreement_flag: float = 0.68
    audio_mismatch_flag: float = 0.45

    # ollama qc
    ollama_model: str = "llama3.2"
    ollama_api_url: str = "http://localhost:11434"
    prefer_api: bool = True
    ollama_timeout_sec: int = 180
    qc_batch_size: int = 10


def _pick_best_whole_text(
    engine_texts: Dict[str, str],
    weights: Dict[str, float],
    timeline_eng: str,
    audio_rescore_text: str,
) -> str:
    candidates = [(eng, (engine_texts.get(eng) or "").strip()) for eng in engine_texts.keys()]
    candidates = [(eng, t) for eng, t in candidates if t]
    if not candidates:
        return ""
    if len(candidates) == 1:
        return candidates[0][1]

    best_eng = candidates[0][0]
    best_score = -1e9

    for eng, txt in candidates:
        score = float(weights.get(eng, 1.0))
        if eng == timeline_eng:
            score += 0.01  # tiny bias to timeline engine
        if audio_rescore_text.strip():
            d = normalized_word_edit_distance(audio_rescore_text, txt)
            score += 0.5 * (1.0 - d)
        if score > best_score:
            best_score = score
            best_eng = eng

    return (engine_texts.get(best_eng) or "").strip()


def process_video_dir(
    source: str,
    video_dir: Path,
    download_root: Path,
    out_root: Path,
    settings: Settings,
    temp_base: Path,
) -> bool:
    engine_paths = find_engine_transcripts(video_dir)
    if not engine_paths:
        print(f"[02b] SKIP (no engine transcripts): {source}/{video_dir.name}")
        return False

    out_dir = out_root / source / video_dir.name
    out_consensus_json = out_dir / f"{video_dir.name}.consensus.json"
    out_consensus_txt = out_dir / f"{video_dir.name}.consensus.txt"
    out_scores = out_dir / f"{video_dir.name}.scores.json"
    out_qc = out_dir / f"{video_dir.name}.qc_report.json"

    if out_consensus_json.exists() and out_scores.exists() and out_qc.exists() and not settings.overwrite:
        print(f"[02b] SKIP (exists): {source}/{video_dir.name}")
        return False

    print(f"[02b] VIDEO: {source}/{video_dir.name}")
    print(f"[02b] ENGINES: {', '.join(sorted(engine_paths.keys()))}")

    engine_segments: Dict[str, List[Dict[str, Any]]] = {}
    for eng, path in engine_paths.items():
        d = read_json(path)
        if not segments_ok(d):
            print(f"[02b] WARN: {eng} has no segments -> skipping engine")
            continue
        engine_segments[eng] = d.get("segments") or []

    if not engine_segments:
        print(f"[02b] SKIP (no usable segments): {source}/{video_dir.name}")
        return False

    timeline_eng = settings.timeline_engine if settings.timeline_engine in engine_segments else next(iter(engine_segments.keys()))
    timeline = engine_segments[timeline_eng]
    print(f"[02b] TIMELINE: {timeline_eng} ({len(timeline)} segments)")

    weights = {
        "whisper": float(settings.weight_whisper),
        "faster": float(settings.weight_faster),
    }

    # Audio rescoring setup (load audio ONCE per video)
    audio_rescore_enabled = bool(settings.audio_rescore)
    audio_path: Optional[Path] = None
    y_full: Optional[np.ndarray] = None
    sr_full: Optional[int] = None

    if audio_rescore_enabled:
        download_dir = download_root / source / video_dir.name
        if download_dir.exists():
            audio_path = pick_best_audio_in_download_dir(download_dir, prefer=settings.audio_prefer)
        if audio_path is None:
            print("[02b] WARN: audio rescoring enabled but no audio found -> disabling for this video.")
            audio_rescore_enabled = False
        else:
            try:
                y_full, sr_full = load_audio_mono(str(audio_path))
            except Exception as e:
                print(f"[02b] WARN: failed to load audio for rescoring -> disabling. {type(e).__name__}: {e}")
                audio_rescore_enabled = False

    rescorer: Optional[SegmentRescorer] = None
    if audio_rescore_enabled:
        rescorer = SegmentRescorer(
            model_name=settings.rescore_model,
            device=settings.rescore_device,
            compute_type=settings.rescore_compute_type,
        )

    full_audio_dur = None
    if audio_rescore_enabled and y_full is not None and sr_full is not None:
        full_audio_dur = float(len(y_full)) / float(sr_full)

    merged_segments: List[Dict[str, Any]] = []
    score_rows: List[Dict[str, Any]] = []

    for idx, seg in enumerate(timeline):
        s = clamp_float(seg.get("start", 0.0), 0.0, 1e9, 0.0)
        e = clamp_float(seg.get("end", s), s, 1e9, s)
        seg_dur = float(max(0.0, e - s))

        # Extract per-engine text for this window (fixed method)
        engine_texts: Dict[str, str] = {}
        for eng, segs in engine_segments.items():
            engine_texts[eng] = gather_text_for_window(segs, s, e)

        # Audio rescore (skip too-short windows, add padding)
        audio_rescore_text = ""
        audio_mismatch = None
        audio_avg_logprob = None

        if (
            audio_rescore_enabled
            and rescorer is not None
            and y_full is not None
            and sr_full is not None
            and full_audio_dur is not None
            and seg_dur >= settings.rescore_min_dur_sec
        ):
            try:
                pad = float(settings.rescore_pad_sec)
                s_pad = max(0.0, s - pad)
                e_pad = min(full_audio_dur, e + pad)
                with temp_segment_wav_16k(
                    y_full=y_full,
                    sr_full=sr_full,
                    start_sec=s_pad,
                    end_sec=e_pad,
                    temp_base=temp_base,
                ) as tmp_wav:
                    audio_rescore_text, audio_avg_logprob = rescorer.transcribe_wav(
                        str(tmp_wav),
                        language="en",
                    )
                if audio_rescore_text.strip():
                    # mismatch computed later after consensus decision
                    pass
            except Exception:
                audio_rescore_text = ""
                audio_mismatch = None
                audio_avg_logprob = None

        # Decide between ROVER merge vs whole-text fallback
        engines_present = [k for k, v in engine_texts.items() if (v or "").strip()]
        consensus_text = ""

        if len(engines_present) == 0:
            consensus_text = ""
            agreement = 0.0
        elif len(engines_present) == 1:
            only_eng = engines_present[0]
            consensus_text = (engine_texts.get(only_eng) or "").strip()
            agreement = 1.0
        else:
            # exactly two engines expected here, but keep generic
            if "whisper" in engine_texts and "faster" in engine_texts:
                a = (engine_texts.get("whisper") or "").strip()
                b = (engine_texts.get("faster") or "").strip()
                dist = normalized_word_edit_distance(a, b)
                la = len(tokenize(a))
                lb = len(tokenize(b))
                len_ratio = (max(la, lb) / max(1, min(la, lb))) if min(la, lb) > 0 else 9.9

                use_fallback = (dist >= settings.whole_text_fallback_dist) or (len_ratio >= settings.whole_text_fallback_len_ratio)
                if use_fallback:
                    consensus_text = _pick_best_whole_text(engine_texts, weights, timeline_eng, audio_rescore_text)
                    # agreement score for fallback: 1 - distance between engines (bounded)
                    agreement = float(max(0.0, min(1.0, 1.0 - dist)))
                else:
                    engine_tokens: Dict[str, List[str]] = {eng: tokenize(t) for eng, t in engine_texts.items() if (t or "").strip()}
                    consensus_tokens, agreement = progressive_rover_vote(
                        engine_tokens=engine_tokens,
                        weights=weights,
                        backbone_engine=timeline_eng,
                    )
                    consensus_text = detokenize(consensus_tokens).strip()
            else:
                engine_tokens = {eng: tokenize(t) for eng, t in engine_texts.items() if (t or "").strip()}
                consensus_tokens, agreement = progressive_rover_vote(
                    engine_tokens=engine_tokens,
                    weights=weights,
                    backbone_engine=timeline_eng,
                )
                consensus_text = detokenize(consensus_tokens).strip()

        # Audio mismatch after consensus is known
        if audio_rescore_text.strip() and consensus_text.strip():
            audio_mismatch = normalized_word_edit_distance(consensus_text, audio_rescore_text)

        merged_seg = dict(seg)
        merged_seg["text"] = consensus_text
        merged_segments.append(merged_seg)

        flags: List[str] = []
        if agreement < settings.low_agreement_flag:
            flags.append(f"low_agreement<{settings.low_agreement_flag}")
        if audio_mismatch is not None and audio_mismatch > settings.audio_mismatch_flag:
            flags.append(f"audio_mismatch>{settings.audio_mismatch_flag}")

        score_rows.append(
            {
                "idx": idx,
                "start": s,
                "end": e,
                "timeline_engine": timeline_eng,
                "engine_texts": engine_texts,
                "consensus_text": consensus_text,
                "agreement_score": round(float(agreement), 4),
                "audio_rescore_text": audio_rescore_text,
                "audio_mismatch": None if audio_mismatch is None else round(float(audio_mismatch), 4),
                "audio_avg_logprob": None if audio_avg_logprob is None else round(float(audio_avg_logprob), 4),
                "flags": flags,
            }
        )

    consensus_full_text = " ".join(
        [s.get("text", "").strip() for s in merged_segments if s.get("text", "").strip()]
    ).strip()

    consensus_blob = {"text": consensus_full_text, "segments": merged_segments}
    write_json(out_consensus_json, consensus_blob)
    write_txt(out_consensus_txt, consensus_full_text)

    # Ollama QC on flagged segments only
    qc_items: List[Dict[str, Any]] = []
    for row in score_rows:
        low_agree = float(row.get("agreement_score") or 0.0) < settings.low_agreement_flag
        audio_bad = (row.get("audio_mismatch") is not None) and (float(row["audio_mismatch"]) > settings.audio_mismatch_flag)
        if low_agree or audio_bad:
            qc_items.append(
                {
                    "idx": row["idx"],
                    "start": row["start"],
                    "end": row["end"],
                    "consensus_text": row["consensus_text"],
                    "engine_texts": row["engine_texts"],
                    "agreement_score": row["agreement_score"],
                    "audio_rescore_text": row.get("audio_rescore_text") or "",
                    "audio_mismatch": row.get("audio_mismatch"),
                }
            )

    qc_batches = chunk_list(qc_items, settings.qc_batch_size)
    qc_results_by_idx: Dict[int, Any] = {}
    qc_errors: List[str] = []

    if qc_items:
        # Prefer API by default to avoid CLI non-JSON output
        if not settings.prefer_api and not shutil_which("ollama"):
            qc_errors.append("ollama CLI not found and --prefer-api not set; skipping QC.")
        else:
            for bi, batch in enumerate(qc_batches):
                items_json = json.dumps(batch, ensure_ascii=False, indent=None)
                prompt = QC_USER_TEMPLATE.format(items_json=items_json)

                if settings.verbose:
                    print(f"[02b] Ollama QC batch {bi+1}/{len(qc_batches)} segments={len(batch)}")

                try:
                    if settings.prefer_api:
                        obj = ollama_generate_json(
                            api_url=settings.ollama_api_url,
                            model=settings.ollama_model,
                            system=QC_SYSTEM,
                            prompt=prompt,
                            timeout_sec=settings.ollama_timeout_sec,
                            temperature=0.0,
                        )
                    else:
                        full_prompt = QC_SYSTEM + "\n\n" + prompt
                        raw = run_ollama_cli(settings.ollama_model, full_prompt, timeout_sec=settings.ollama_timeout_sec)
                        obj = extract_json_object(raw)

                    segs = obj.get("segments") or []
                    if not isinstance(segs, list):
                        raise ValueError("QC output missing segments list")

                    for r in segs:
                        if not isinstance(r, dict):
                            continue
                        idx_val = r.get("idx")
                        if isinstance(idx_val, int):
                            qc_results_by_idx[idx_val] = r

                except Exception as e:
                    qc_errors.append(f"batch {bi+1}: {type(e).__name__}: {e}")

    scores_blob = {
        "created_at": now_iso(),
        "source": source,
        "video_dir": video_dir.name,
        "inputs": {
            "engine_jsons": {k: str(v) for k, v in engine_paths.items()},
            "timeline_engine": timeline_eng,
        },
        "settings": {
            "weights": weights,
            "low_agreement_flag": settings.low_agreement_flag,
            "whole_text_fallback_dist": settings.whole_text_fallback_dist,
            "whole_text_fallback_len_ratio": settings.whole_text_fallback_len_ratio,
            "audio_rescore": audio_rescore_enabled,
            "audio_mismatch_flag": settings.audio_mismatch_flag,
            "rescore_model": settings.rescore_model if audio_rescore_enabled else None,
            "rescore_min_dur_sec": settings.rescore_min_dur_sec,
            "rescore_pad_sec": settings.rescore_pad_sec,
            "temp_base": str(temp_base),
        },
        "segments": score_rows,
        "summary": {
            "segment_count": len(score_rows),
            "flagged_for_qc": len(qc_items),
        },
    }
    write_json(out_scores, scores_blob)

    qc_report = {
        "created_at": now_iso(),
        "source": source,
        "video_dir": video_dir.name,
        "ollama": {
            "model": settings.ollama_model,
            "prefer_api": settings.prefer_api,
            "api_url": settings.ollama_api_url,
            "timeout_sec": settings.ollama_timeout_sec,
            "qc_batch_size": settings.qc_batch_size,
            "errors": qc_errors,
        },
        "qc_targets": qc_items,
        "qc_results_by_idx": qc_results_by_idx,
        "note": "QC suggestions are NOT applied automatically. Downstream stage may apply them selectively.",
    }
    write_json(out_qc, qc_report)

    print(f"[02b] ✅ WROTE: {out_consensus_json}")
    print(f"[02b] ✅ WROTE: {out_scores}")
    print(f"[02b] ✅ WROTE: {out_qc}")

    return True


def run_for_source(source_name: str, youtube_url: str, settings: Settings, temp_base: Path) -> int:
    root = repo_root()
    safe_source = safe_name(source_name)

    transcribe_root = root / "data" / "02.transcribe"
    download_root = root / "data" / "01.download"
    out_root = root / "data" / "02b.clean-transcribed"

    # SAFETY: output root must be under repo/data
    ensure_under_dir(out_root, root / "data", "out_root")
    ensure_under_dir(transcribe_root, root / "data", "transcribe_root")
    ensure_under_dir(download_root, root / "data", "download_root")

    source_transcribe_dir = transcribe_root / safe_source
    if not source_transcribe_dir.exists():
        print(f"[02b] ❌ Missing transcribe folder: {source_transcribe_dir}")
        return 0

    video_id = extract_video_id(youtube_url)
    video_dirs = sorted([p for p in source_transcribe_dir.iterdir() if p.is_dir()])
    if video_id:
        video_dirs = [d for d in video_dirs if f"[{video_id}]" in d.name]

    if not video_dirs:
        print(f"[02b] ❌ No video folders found in: {source_transcribe_dir}")
        return 0

    processed = 0
    for video_dir in video_dirs:
        ok = process_video_dir(
            source=safe_source,
            video_dir=video_dir,
            download_root=download_root,
            out_root=out_root,
            settings=settings,
            temp_base=temp_base,
        )
        if ok:
            processed += 1

    print(f"[02b] DONE: processed={processed}")
    return processed


def main() -> None:
    p = argparse.ArgumentParser(
        description="Consensus merge (ROVER-ish) + Ollama QC + audio rescoring (no disfluency cleanup).",
        formatter_class=argparse.RawDescriptionHelpFormatter,
    )

    p.add_argument("source_name", nargs="?", help="Source name (folder under data/02.transcribe).")
    p.add_argument("youtube_url", nargs="?", help="YouTube URL. Video URL filters by ID.")

    p.add_argument(
        "--sources",
        nargs="?",
        const="docs/sources.txt",
        help="Process all sources from a sources.txt file (default: docs/sources.txt).",
    )

    p.add_argument("--overwrite", action="store_true", help="Overwrite outputs.")
    p.add_argument("--verbose", action="store_true", help="Verbose logging.")

    p.add_argument("--timeline-engine", choices=["whisper", "faster"], default="whisper")

    p.add_argument("--w-whisper", type=float, default=1.0)
    p.add_argument("--w-faster", type=float, default=1.0)

    p.add_argument("--low-agreement-flag", type=float, default=0.68)

    p.add_argument("--no-audio-rescore", action="store_true", help="Disable audio rescoring.")
    p.add_argument("--audio-prefer", choices=["clean", "raw", "legacy"], default="clean")
    p.add_argument("--audio-mismatch-flag", type=float, default=0.45)

    p.add_argument("--rescore-model", default=os.environ.get("RESCORE_WHISPER_MODEL", "base"))
    p.add_argument("--rescore-device", default=os.environ.get("RESCORE_WHISPER_DEVICE", "cuda"))
    p.add_argument("--rescore-compute-type", default=os.environ.get("RESCORE_WHISPER_COMPUTE_TYPE", "float16"))
    p.add_argument("--rescore-min-dur", type=float, default=1.2, help="Minimum segment duration (sec) to run audio rescoring.")
    p.add_argument("--rescore-pad", type=float, default=0.25, help="Padding seconds added before/after segment for rescoring.")

    p.add_argument("--model", default=os.environ.get("OLLAMA_MODEL", "llama3.2"))
    p.add_argument("--ollama-api", default=os.environ.get("OLLAMA_API", "http://localhost:11434"))
    p.add_argument("--prefer-api", action="store_true", help="Prefer Ollama HTTP API (strict JSON).")
    p.add_argument("--timeout", type=int, default=180, help="Ollama timeout seconds.")
    p.add_argument("--qc-batch-size", type=int, default=10)

    args = p.parse_args()

    settings = Settings(
        overwrite=bool(args.overwrite),
        verbose=bool(args.verbose),
        timeline_engine=str(args.timeline_engine),
        weight_whisper=float(args.w_whisper),
        weight_faster=float(args.w_faster),
        audio_rescore=not bool(args.no_audio_rescore),
        audio_prefer=str(args.audio_prefer),
        rescore_model=str(args.rescore_model),
        rescore_device=str(args.rescore_device),
        rescore_compute_type=str(args.rescore_compute_type),
        rescore_min_dur_sec=float(args.rescore_min_dur),
        rescore_pad_sec=float(args.rescore_pad),
        low_agreement_flag=float(args.low_agreement_flag),
        audio_mismatch_flag=float(args.audio_mismatch_flag),
        ollama_model=str(args.model),
        ollama_api_url=str(args.ollama_api),
        prefer_api=bool(args.prefer_api) or True,  # default True; flag keeps it True
        ollama_timeout_sec=int(args.timeout),
        qc_batch_size=int(args.qc_batch_size),
    )

    temp_base = default_temp_base()
    root = repo_root()
    try:
        if (root / "data").resolve() in temp_base.resolve().parents:
            raise SystemExit(
                f"[02b] Refusing: temp_base is under repo/data.\n  temp_base={temp_base}\n  repo_data={root/'data'}"
            )
    except Exception:
        pass

    if args.sources is not None:
        sources_path = Path(args.sources)
        if not sources_path.is_absolute():
            sources_path = repo_root() / sources_path
        if not sources_path.exists():
            raise SystemExit(f"[02b] ❌ Sources file not found: {sources_path}")

        total = 0
        for source_name, youtube_url in parse_sources_file(sources_path):
            total += run_for_source(source_name, youtube_url, settings, temp_base=temp_base)
        print(f"[02b] ✅ ALL SOURCES DONE: total_processed={total}")
        raise SystemExit(0)

    if not args.source_name or not args.youtube_url:
        raise SystemExit(
            "[02b] Provide either --sources [file] OR:\n"
            "./scripts/training-data/02b.clean-transcribed <source_name> <youtube_url>"
        )

    run_for_source(str(args.source_name), str(args.youtube_url), settings, temp_base=temp_base)


if __name__ == "__main__":
    main()
