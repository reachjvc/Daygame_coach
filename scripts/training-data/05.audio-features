#!/usr/bin/env python3
# scripts/training-data/05.audio-features
#
# STEP 5 — AUDIO FEATURES (+ speaker embeddings)
#
# Reads:
#   data/01.download/<source_name>/<video_name>/*.wav
#   data/04.diarize/<source_name>/<video_name>/<stem>.json   (diarized JSON with speakers)
#
# Writes:
#   data/05.audio-features/<source_name>/<video_name>/<stem>.audio_features.json
#
# Use:
#   A) One source (video / playlist / channel):
#      ./scripts/training-data/05.audio-features "daily_evolution" "https://www.youtube.com/watch?v=utuuVOXJunM"
#
#   B) Batch from sources file:
#      ./scripts/training-data/05.audio-features --sources
#      ./scripts/training-data/05.audio-features --sources docs/pipeline/sources.txt
#
#   C) Single-file mode:
#      python3 scripts/training-data/05.audio-features \
#        --audio path/to/audio.wav \
#        --transcript path/to/transcript.json \
#        --out path/to/audio_features.json
#
# ================================================================================
# FEATURES WE EXTRACT AND WHY
# ================================================================================
#
# FOR TONE CLASSIFICATION (used in 06.segment-enrich):
# ┌─────────────────────┬────────────────────────────────────────────────────┐
# │ Feature             │ Used to detect                                     │
# ├─────────────────────┼────────────────────────────────────────────────────┤
# │ pitch.std_hz        │ playful (>22), confident (<18), nervous (<16)      │
# │ energy.dynamics_db  │ playful (>13), confident (8-13), energetic (>15)   │
# │ tempo.syllable_rate │ confident (5-6.5), nervous (>6.8)                  │
# │ spectral.brightness │ energetic (>1700Hz)                                │
# └─────────────────────┴────────────────────────────────────────────────────┘
#
# FOR SPEAKER IDENTIFICATION:
# Primary: pyannote_speaker field passed through from 04.diarize (SPEAKER_00, SPEAKER_01)
# Backup:  speaker_embedding (256-dim resemblyzer) available for 06.segment-enrich if needed
#
# FOR FUTURE VOICE-TO-VOICE COACHING:
# ┌─────────────────────┬────────────────────────────────────────────────────┐
# │ Feature             │ Purpose                                            │
# ├─────────────────────┼────────────────────────────────────────────────────┤
# │ pitch.mean_hz       │ Compare user's baseline to coach                   │
# │ pitch.range_hz      │ Expressiveness metric                              │
# │ pitch.direction     │ Question detection (rising) vs statement (falling) │
# └─────────────────────┴────────────────────────────────────────────────────┘
#
# NOTE: Speaker identification now uses pyannote diarization from 04.diarize.
# NOTE: If pitch.mean_hz == 0, pyin couldn't confidently detect pitch for that segment.
#       Stage 06 should skip such segments for tone classification.
# pitch.mean_hz is kept for future voice-to-voice coaching features.
#
# ================================================================================

import argparse
import hashlib
import json
import re
import shlex
from dataclasses import dataclass
from pathlib import Path
from typing import Any, Dict, List, Optional, Set, Tuple

import numpy as np

from batch.manifest_parser import load_manifest, load_manifest_sources, manifest_filter_files

try:
    import librosa  # type: ignore
except Exception:
    librosa = None

try:
    import soundfile as sf  # type: ignore
except Exception:
    sf = None

try:
    import torchaudio  # type: ignore
    import torch
except Exception:
    torchaudio = None
    torch = None

# Optional (speaker embeddings)
try:
    from resemblyzer import VoiceEncoder  # type: ignore
except Exception:
    VoiceEncoder = None


# -------------------------
# Config
# -------------------------

@dataclass
class Config:
    # Feature extraction
    sample_rate: int = 22050
    feature_extractor: str = "librosa"

    # Pitch range for speech
    # Male F0: 85-180Hz, Female F0: 165-255Hz (up to 400-500Hz when excited)
    # fmax=500Hz allows full female range including excited/animated speech
    pitch_fmin_hz: float = 65.0
    pitch_fmax_hz: float = 500.0

    # Pitch extraction method / robustness
    # NOTE: pyin gives voiced/unvoiced flags + probabilities
    # If pyin can't confidently detect pitch, we return zeros and Stage 06 skips those segments
    pitch_method: str = "pyin"  # "pyin" (recommended) or "yin"
    pitch_trim_percentile: float = 2.0  # use percentile trimming for stats (avoids 900Hz spikes without hard clamping)

    # Speaker embeddings (resemblyzer)
    # NOTE: Speaker identification now uses pyannote diarization from 04.diarize.
    # 06.segment-enrich uses pyannote_speaker field, NOT embeddings.
    # Embeddings disabled by default to save storage (~1KB per segment).
    # Enable with --store_embedding_vectors if needed for future features.
    embedder: str = "resemblyzer"
    embedder_sample_rate: int = 16000
    min_embed_raw_sec: float = 0.8
    min_embed_window_sec: float = 1.2
    embed_pad_sec: float = 0.25
    store_embedding_vectors: bool = False


# -------------------------
# Small utils
# -------------------------

def _load_flagged_videos(flag_file: Path) -> set:
    """Load CRITICAL-severity video names from a .flagged.json file.

    Only CRITICAL videos are skipped. WARNING-severity entries (including
    entries that predate the severity field) are allowed through.
    """
    if not flag_file.exists():
        return set()
    try:
        entries = json.loads(flag_file.read_text(encoding="utf-8"))
        return {
            str(e.get("video", ""))
            for e in entries
            if e.get("video") and e.get("severity", "WARNING") == "CRITICAL"
        }
    except Exception:
        return set()


def repo_root() -> Path:
    return Path(__file__).resolve().parents[2]


def safe_name(name: str) -> str:
    cleaned = re.sub(r"[^A-Za-z0-9._-]+", "_", (name or "").strip())
    return cleaned.strip("_") or "source"


def extract_video_id(url: str) -> Optional[str]:
    url = (url or "").strip()
    if not url:
        return None
    m = re.search(r"[?&]v=([^&]+)", url)
    if m:
        return m.group(1)
    m = re.search(r"youtu\.be/([^?&/]+)", url)
    if m:
        return m.group(1)
    return None


def parse_sources_file(path: Path) -> List[Tuple[str, str]]:
    sources: List[Tuple[str, str]] = []
    for raw in path.read_text(encoding="utf-8").splitlines():
        line = raw.strip()
        if not line or line.startswith("#"):
            continue
        if "|" in line:
            name, url = line.split("|", 1)
            name = name.strip()
            url = url.strip()
            if name and url:
                sources.append((name, url))
            continue
        parts = shlex.split(line)
        if len(parts) >= 2:
            sources.append((parts[0], parts[1]))
    return sources


def r3(x: float) -> float:
    return float(np.round(float(x), 3))


def sha256_file(path: Path) -> str:
    h = hashlib.sha256()
    with open(path, "rb") as f:
        while True:
            chunk = f.read(1024 * 1024)
            if not chunk:
                break
            h.update(chunk)
    return h.hexdigest()


def percentile_trim(values: np.ndarray, pct: float) -> np.ndarray:
    """
    Soft-robustness: remove only extreme outliers (spikes), without hard clamping.
    pct=2 trims lowest 2% and highest 2%.
    """
    if values.size == 0:
        return values
    pct = float(np.clip(pct, 0.0, 49.0))
    if pct <= 0.0:
        return values
    lo = np.percentile(values, pct)
    hi = np.percentile(values, 100.0 - pct)
    return values[(values >= lo) & (values <= hi)]


# -------------------------
# Audio loading (robust)
# -------------------------

def load_audio_mono(path: str) -> Tuple[np.ndarray, int]:
    """
    Load audio as mono float32 without resampling.
    Priority:
      1) torchaudio (if works)
      2) soundfile
      3) librosa (sr=None, mono=True)
    """
    # torchaudio
    if torchaudio is not None:
        try:
            wav, sr = torchaudio.load(path)
            wav = wav.to(torch.float32)

            # mono
            if wav.ndim == 2 and wav.shape[0] > 1:
                wav = wav.mean(dim=0, keepdim=True)
            if wav.ndim == 2:
                wav = wav.squeeze(0)

            y = wav.detach().cpu().numpy().astype(np.float32)
            return y, int(sr)
        except Exception as e:
            msg = str(e).lower()
            if "torchcodec" in msg:
                print("[audio-features] WARN: torchaudio requires torchcodec for this file; falling back.")
            else:
                print(f"[audio-features] WARN: torchaudio.load failed ({type(e).__name__}): {e}")

    # soundfile
    if sf is not None:
        try:
            y, sr = sf.read(path, dtype="float32", always_2d=True)
            if y.shape[1] > 1:
                y = y.mean(axis=1, keepdims=True)
            y = y[:, 0].astype(np.float32)
            return y, int(sr)
        except Exception as e:
            print(f"[audio-features] WARN: soundfile failed ({type(e).__name__}): {e}")

    # librosa
    if librosa is not None:
        y, sr = librosa.load(path, sr=None, mono=True)
        return y.astype(np.float32), int(sr)

    raise SystemExit("No audio backend worked. Install soundfile or librosa (or torchcodec for torchaudio).")


def resample_np(y: np.ndarray, sr_in: int, sr_out: int) -> np.ndarray:
    if sr_in == sr_out:
        return y.astype(np.float32)
    if librosa is None:
        raise SystemExit("librosa is required for resampling in this script.")
    return librosa.resample(y.astype(np.float32), orig_sr=sr_in, target_sr=sr_out).astype(np.float32)


# -------------------------
# Transcript loading
# -------------------------

def load_whisper_json(path: Path) -> Dict[str, Any]:
    data = json.loads(path.read_text(encoding="utf-8"))
    if "segments" not in data or not isinstance(data["segments"], list):
        raise ValueError(f"Transcript missing segments list: {path}")
    return data


# -------------------------
# Feature extraction
# -------------------------

def compute_pitch_features(y: np.ndarray, sr: int, cfg: Config) -> Dict[str, Any]:
    """
    Extract pitch features from audio segment.

    Returns only the features we actually use:
    - mean_hz: For speaker ID (TODO: replace with speaker_embedding)
    - std_hz: For tone detection (playful, confident, nervous)
    - range_hz: For future expressiveness coaching
    - direction: For future question vs statement detection
    """
    if librosa is None or y.size == 0:
        return {
            "mean_hz": 0.0,
            "std_hz": 0.0,
            "range_hz": 0.0,
            "direction": 0.0,
        }

    try:
        if cfg.pitch_method.lower() == "pyin" and hasattr(librosa, "pyin"):
            f0, voiced_flag, voiced_prob = librosa.pyin(
                y=y,
                fmin=cfg.pitch_fmin_hz,
                fmax=cfg.pitch_fmax_hz,
                sr=sr,
            )
            f0 = np.asarray(f0, dtype=np.float32)  # contains NaN for unvoiced
            voiced_flag = np.asarray(voiced_flag, dtype=bool)

            # Use pyin's native voiced detection - no additional filtering
            voiced = voiced_flag & np.isfinite(f0)

        else:
            # Fallback: yin() has no voiced probability, only gives an f0 guess everywhere
            f0 = librosa.yin(
                y=y,
                fmin=cfg.pitch_fmin_hz,
                fmax=cfg.pitch_fmax_hz,
                sr=sr,
            )
            f0 = np.asarray(f0, dtype=np.float32)

            # With yin(), treat low-energy / garbage frames as unvoiced is hard.
            # We approximate: if f0 is finite and >0, mark voiced.
            voiced = np.isfinite(f0) & (f0 > 0)

        if np.sum(voiced) < 2:
            return {
                "mean_hz": 0.0,
                "std_hz": 0.0,
                "range_hz": 0.0,
                "direction": 0.0,
            }

        f0v = f0[voiced]

        # Soft robustness (not hard clamping): trim only the extreme outliers
        f0v_stats = percentile_trim(f0v, cfg.pitch_trim_percentile)
        if f0v_stats.size < 2:
            f0v_stats = f0v  # fallback

        mean_hz = float(np.mean(f0v_stats))
        std_hz = float(np.std(f0v_stats))
        range_hz = float(np.max(f0v_stats) - np.min(f0v_stats))

        # direction = slope of linear fit over voiced frames
        idx_all = np.arange(len(f0), dtype=np.float32)
        idx = idx_all[voiced]
        slope = 0.0
        if idx.size >= 2:
            slope = float(np.polyfit(idx, f0v, 1)[0])

        # Only return features we actually use (see docstring at top of file)
        return {
            "mean_hz": mean_hz,      # For speaker ID (TODO: replace with embeddings)
            "std_hz": std_hz,        # For tones: playful, confident, nervous
            "range_hz": range_hz,    # For future: expressiveness metric
            "direction": slope,      # For future: question vs statement detection
        }
    except Exception:
        return {
            "mean_hz": 0.0,
            "std_hz": 0.0,
            "range_hz": 0.0,
            "direction": 0.0,
        }


def compute_energy_features(y: np.ndarray, sr: int) -> Dict[str, float]:
    """
    Extract energy features from audio segment.

    Returns only:
    - dynamics_db: For tone detection (playful, confident, energetic)
    """
    if librosa is None or y.size == 0:
        return {"dynamics_db": 0.0}

    rms = librosa.feature.rms(y=y)[0]  # shape [frames]
    rms = np.asarray(rms, dtype=np.float32)
    db = 20.0 * np.log10(rms + 1e-9)

    mean_db = float(np.mean(db)) if db.size else 0.0
    max_db = float(np.max(db)) if db.size else 0.0
    dynamics_db = float(max_db - mean_db)

    return {"dynamics_db": dynamics_db}


def compute_tempo_features(y: np.ndarray, sr: int, duration_sec: float) -> Dict[str, float]:
    """
    Extract tempo features from audio segment.

    Returns only:
    - syllable_rate: For tone detection (confident, nervous)
    """
    if librosa is None or y.size == 0 or duration_sec <= 0:
        return {"syllable_rate": 0.0}

    try:
        onset_frames = librosa.onset.onset_detect(y=y, sr=sr, backtrack=False, units="frames")
        onset_count = int(len(onset_frames))
        syllable_rate = float(onset_count / max(1e-6, duration_sec))
        return {"syllable_rate": syllable_rate}
    except Exception:
        return {"syllable_rate": 0.0}


def compute_spectral_features(y: np.ndarray, sr: int) -> Dict[str, float]:
    """
    Extract spectral features from audio segment.

    Returns only:
    - brightness_hz: For tone detection (energetic)
    """
    if librosa is None or y.size == 0:
        return {"brightness_hz": 0.0}

    centroid = librosa.feature.spectral_centroid(y=y, sr=sr)[0]

    return {
        "brightness_hz": float(np.mean(centroid)) if centroid.size else 0.0,
    }


# -------------------------
# Speaker embeddings
# -------------------------

def get_speaker_encoder(cfg: Config):
    if VoiceEncoder is None:
        return None
    return VoiceEncoder()


def try_embed_segment(
    encoder,
    y_orig: np.ndarray,
    sr_orig: int,
    start: float,
    end: float,
    cfg: Config,
) -> Optional[np.ndarray]:
    if encoder is None:
        return None

    # padded window for embedding (helps stability)
    s = max(0.0, start - cfg.embed_pad_sec)
    e = max(s, end + cfg.embed_pad_sec)
    raw_len = e - s

    # too short => fail
    if raw_len < cfg.min_embed_raw_sec:
        return None

    s_idx = int(round(s * sr_orig))
    e_idx = int(round(e * sr_orig))
    s_idx = max(0, min(s_idx, len(y_orig)))
    e_idx = max(0, min(e_idx, len(y_orig)))
    if e_idx <= s_idx:
        return None

    chunk = y_orig[s_idx:e_idx]
    if chunk.size == 0:
        return None

    # enforce minimum window length
    if raw_len < cfg.min_embed_window_sec:
        return None

    # resample to embedder SR
    y16 = resample_np(chunk, sr_orig, cfg.embedder_sample_rate)

    # resemblyzer likes float32 in [-1,1]
    y16 = np.clip(y16.astype(np.float32), -1.0, 1.0)

    try:
        emb = encoder.embed_utterance(y16)
        emb = np.asarray(emb, dtype=np.float32)
        return emb
    except Exception:
        return None


# -------------------------
# Main worker
# -------------------------

def build_audio_features(
    audio_path: Path,
    transcript_path: Path,
    out_path: Path,
    cfg: Config,
) -> None:
    y_orig, sr_orig = load_audio_mono(str(audio_path))
    total_duration = float(len(y_orig) / sr_orig) if sr_orig > 0 else 0.0

    transcript = load_whisper_json(transcript_path)
    segments_in = transcript.get("segments", []) or []

    encoder = get_speaker_encoder(cfg)
    # NOTE: No longer tracking speakers here - 06.segment-enrich does global clustering

    segments_out: List[Dict[str, Any]] = []

    for seg in segments_in:
        start = float(seg.get("start", 0.0))
        end = float(seg.get("end", 0.0))
        text = str(seg.get("text", "") or "").strip()
        # Pass through pyannote speaker ID from transcript (e.g., SPEAKER_00, SPEAKER_01)
        pyannote_speaker = seg.get("speaker")  # May be None if no diarization
        if not text:
            continue
        if end <= start:
            continue

        start = max(0.0, min(start, total_duration))
        end = max(0.0, min(end, total_duration))
        if end <= start:
            continue

        dur = float(end - start)

        # Slice segment from original
        s_idx = int(round(start * sr_orig))
        e_idx = int(round(end * sr_orig))
        s_idx = max(0, min(s_idx, len(y_orig)))
        e_idx = max(0, min(e_idx, len(y_orig)))
        y_seg = y_orig[s_idx:e_idx]
        if y_seg.size == 0:
            continue

        # Resample for feature extraction
        y_feat = resample_np(y_seg, sr_orig, cfg.sample_rate)

        pitch = compute_pitch_features(y_feat, cfg.sample_rate, cfg)
        energy = compute_energy_features(y_feat, cfg.sample_rate)
        tempo = compute_tempo_features(y_feat, cfg.sample_rate, dur)
        spectral = compute_spectral_features(y_feat, cfg.sample_rate)

        # Speaker embedding (clustering done in 06.segment-enrich)
        emb = try_embed_segment(encoder, y_orig, sr_orig, start, end, cfg)

        speaker_embedding_obj = None
        if emb is not None:
            if cfg.store_embedding_vectors:
                speaker_embedding_obj = {
                    "dim": int(emb.shape[0]),
                    "vector": [float(x) for x in emb.tolist()],
                }
            else:
                speaker_embedding_obj = {"dim": int(emb.shape[0]), "vector": None}

        segments_out.append(
            {
                "start": r3(start),
                "end": r3(end),
                "duration_sec": r3(dur),
                "text": text,
                # Pyannote speaker ID from transcript (SPEAKER_00, SPEAKER_01, etc.)
                # Will be mapped to coach/target/voiceover in 06.segment-enrich
                "pyannote_speaker": pyannote_speaker,
                "features": {
                    "pitch": pitch,
                    "energy": energy,
                    "tempo": tempo,
                    "spectral": spectral,
                    "speaker_embedding": speaker_embedding_obj,
                },
                "audio_clip": {
                    "file": str(audio_path.resolve()),
                    "start": r3(start),
                    "end": r3(end),
                },
            }
        )

    # Build top-level output
    out = {
        "source_audio": str(audio_path.resolve()),
        "audio_sha256": sha256_file(audio_path),
        "source_timestamps": str(transcript_path.resolve()),
        "processing": {
            "sample_rate": cfg.sample_rate,
            "feature_extractor": cfg.feature_extractor,
            "pitch_range_hz": [cfg.pitch_fmin_hz, cfg.pitch_fmax_hz],
            "pitch_method": cfg.pitch_method,
            "pitch_trim_percentile": cfg.pitch_trim_percentile,
            "embedder": cfg.embedder,
            "embedder_sample_rate": cfg.embedder_sample_rate,
            "min_embed_raw_sec": cfg.min_embed_raw_sec,
            "min_embed_window_sec": cfg.min_embed_window_sec,
            "embed_pad_sec": cfg.embed_pad_sec,
            "store_embedding_vectors": cfg.store_embedding_vectors,
        },
        "total_duration_sec": r3(total_duration),
        "segments": segments_out,
    }

    out_path.parent.mkdir(parents=True, exist_ok=True)
    out_path.write_text(json.dumps(out, ensure_ascii=False, indent=2), encoding="utf-8")


def batch_for_source(source_name: str, youtube_url: str, overwrite: bool, cfg: Config, manifest_ids: Optional[Set[str]] = None) -> int:
    root = repo_root()
    safe_source = safe_name(source_name)

    downloads_root = root / "data" / "01.download" / safe_source
    transcribe_root = root / "data" / "04.diarize" / safe_source
    out_root = root / "data" / "05.audio-features" / safe_source

    if not downloads_root.exists():
        raise SystemExit(f"[audio-features] Missing downloads folder: {downloads_root}")

    # Load flagged videos from 02.transcribe — skip these entirely
    transcribe_flag_root = root / "data" / "02.transcribe" / safe_source
    flagged = _load_flagged_videos(transcribe_flag_root / ".flagged.json")
    if flagged:
        print(f"[audio-features] CRITICAL-flagged videos (will skip): {len(flagged)}")

    video_id = extract_video_id(youtube_url)
    wav_files = sorted(downloads_root.rglob("*.wav"))
    if video_id:
        wav_files = [p for p in wav_files if f"[{video_id}]" in p.as_posix() or f"[{video_id}]" in p.name]
    if manifest_ids:
        wav_files = manifest_filter_files(wav_files, manifest_ids)

    if not wav_files:
        print(f"[audio-features] No WAV files found under: {downloads_root}")
        return 0

    processed = 0
    skipped = 0

    for wav_path in wav_files:
        rel = wav_path.relative_to(downloads_root)
        rel_dir = rel.parent
        stem = wav_path.stem

        # Skip flagged videos (hallucination, quality issues in 02.transcribe)
        if str(rel_dir) in flagged:
            skipped += 1
            continue

        # The WAV file is named: <video_name>.audio.asr.clean16k.wav
        # But transcripts are named: <video_name>.full.json
        # Extract base video name by removing .audio.asr.* suffix
        base_name = stem
        for suffix in [".audio.asr.clean16k", ".audio.asr.raw16k", ".audio"]:
            if base_name.endswith(suffix):
                base_name = base_name[:-len(suffix)]
                break

        # Bug fix: WAV files are often missing the closing ] bracket before the audio suffix
        # e.g., "Video Title [abc123.audio.asr.clean16k.wav" instead of "Video Title [abc123].audio..."
        # If base_name ends with [<video_id> (no closing bracket), add it back
        if '[' in base_name and not base_name.endswith(']'):
            # Match pattern: ends with [ followed by alphanumeric/dash/underscore (video ID)
            if re.search(r'\[[A-Za-z0-9_-]+$', base_name):
                base_name = base_name + ']'

        # Try multiple transcript naming patterns
        transcript_candidates = [
            transcribe_root / rel_dir / f"{base_name}.full.json",  # Primary
            transcribe_root / rel_dir / f"{base_name}.full.whisperx.json",  # WhisperX
            transcribe_root / rel_dir / f"{stem}.json",  # Legacy: exact stem match
        ]
        transcript_path = None
        for candidate in transcript_candidates:
            if candidate.exists():
                transcript_path = candidate
                break

        if transcript_path is None:
            print(f"[audio-features] WARN: missing transcript: {transcript_candidates[0]}")
            continue

        out_path = out_root / rel_dir / f"{stem}.audio_features.json"
        if out_path.exists() and not overwrite:
            skipped += 1
            continue

        print(f"[audio-features] {stem}")
        build_audio_features(
            audio_path=wav_path,
            transcript_path=transcript_path,
            out_path=out_path,
            cfg=cfg,
        )
        processed += 1

    print(f"[audio-features] Done: processed={processed} skipped={skipped}")
    return processed


# -------------------------
# CLI
# -------------------------

if __name__ == "__main__":
    p = argparse.ArgumentParser(
        description="Extract audio features aligned to Whisper segments + optional speaker embeddings.",
        formatter_class=argparse.RawDescriptionHelpFormatter,
    )

    p.add_argument("source_name", nargs="?", help="Source name (folder under data/01.download).")
    p.add_argument("youtube_url", nargs="?", help="YouTube URL. Video URL filters by ID.")

    p.add_argument(
        "--sources",
        nargs="?",
        const="docs/pipeline/sources.txt",
        help="Process all sources from a sources.txt file (default: docs/pipeline/sources.txt).",
    )
    p.add_argument("--manifest", help="Manifest file: only process videos listed (docs/pipeline/batches/P001.txt).")

    p.add_argument("--overwrite", action="store_true", help="Overwrite existing outputs in batch mode.")

    # Single-file mode
    p.add_argument("--audio", help="Single-file mode: audio path (.wav etc).")
    p.add_argument("--transcript", help="Single-file mode: transcript JSON path (Whisper-shape).")
    p.add_argument("--out", help="Single-file mode: output .json path.")

    # Config knobs
    p.add_argument("--sample_rate", type=int, default=Config.sample_rate)
    p.add_argument("--pitch_fmin_hz", type=float, default=Config.pitch_fmin_hz)
    p.add_argument("--pitch_fmax_hz", type=float, default=Config.pitch_fmax_hz)

    p.add_argument("--pitch_method", type=str, default=Config.pitch_method)
    p.add_argument("--pitch_trim_percentile", type=float, default=Config.pitch_trim_percentile)

    p.add_argument("--min_embed_raw_sec", type=float, default=Config.min_embed_raw_sec)
    p.add_argument("--min_embed_window_sec", type=float, default=Config.min_embed_window_sec)
    p.add_argument("--embed_pad_sec", type=float, default=Config.embed_pad_sec)
    p.add_argument("--store_embedding_vectors", action="store_true", default=False,
                   help="Store 256-dim speaker embedding vectors (disabled by default to save storage)")

    args = p.parse_args()

    cfg = Config(
        sample_rate=int(args.sample_rate),
        pitch_fmin_hz=float(args.pitch_fmin_hz),
        pitch_fmax_hz=float(args.pitch_fmax_hz),
        pitch_method=str(args.pitch_method),
        pitch_trim_percentile=float(args.pitch_trim_percentile),
        min_embed_raw_sec=float(args.min_embed_raw_sec),
        min_embed_window_sec=float(args.min_embed_window_sec),
        embed_pad_sec=float(args.embed_pad_sec),
        store_embedding_vectors=bool(args.store_embedding_vectors),
    )

    # Single-file mode
    if args.audio:
        if not args.transcript or not args.out:
            raise SystemExit("--audio requires --transcript and --out")
        build_audio_features(
            audio_path=Path(args.audio),
            transcript_path=Path(args.transcript),
            out_path=Path(args.out),
            cfg=cfg,
        )
        raise SystemExit(0)

    # Manifest batch mode
    if args.manifest:
        manifest_path = Path(args.manifest)
        if not manifest_path.is_absolute():
            manifest_path = repo_root() / manifest_path
        if not manifest_path.exists():
            raise SystemExit(f"Manifest file not found: {manifest_path}")
        sources_map = load_manifest_sources(manifest_path)
        for source_name, vid_ids in sorted(sources_map.items()):
            print(f"[audio-features] Manifest: {source_name} ({len(vid_ids)} videos)")
            batch_for_source(source_name, "", overwrite=bool(args.overwrite), cfg=cfg, manifest_ids=vid_ids)
        raise SystemExit(0)

    # Batch from sources file
    if args.sources is not None:
        sources_path = Path(args.sources)
        if not sources_path.is_absolute():
            sources_path = repo_root() / sources_path
        if not sources_path.exists():
            raise SystemExit(f"Sources file not found: {sources_path}")

        for source_name, youtube_url in parse_sources_file(sources_path):
            batch_for_source(source_name, youtube_url, overwrite=bool(args.overwrite), cfg=cfg)
        raise SystemExit(0)

    # One source mode
    if not args.source_name or not args.youtube_url:
        raise SystemExit(
            "Provide either --audio/--transcript/--out, --manifest, or --sources [file], or:\n"
            "./scripts/training-data/05.audio-features <source_name> <youtube_url>"
        )

    batch_for_source(str(args.source_name), str(args.youtube_url), overwrite=bool(args.overwrite), cfg=cfg)
