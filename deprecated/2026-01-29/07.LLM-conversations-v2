#!/usr/bin/env python3
"""
scripts/training-data/07.LLM-conversations-v2

IMPROVED Conversation Boundary Detection for Daygame Transcripts

Key improvement: Detects video type FIRST, then applies appropriate classification:
- For talking_head/podcast videos: Force ALL segments to commentary (0 false positives)
- For infield videos: Use LLM to distinguish approach vs commentary

This fixes the issue where talking-head educational content was being misclassified
as containing "approach" segments when it's all commentary.

Default I/O:
- Input : data/06.speakers/<source>/**/*.json
- Output: data/07.LLM-conversations/<source>/**/*.conversations.json

Use:
  ./scripts/training-data/07.LLM-conversations-v2 "daily_evolution" "https://youtube.com/..."
  ./scripts/training-data/07.LLM-conversations-v2 --sources

Environment:
  OLLAMA_API_URL=http://localhost:11434
  OLLAMA_MODEL=llama3.1
"""

from __future__ import annotations

import argparse
import json
import os
import re
import shlex
import time
from dataclasses import dataclass
from pathlib import Path
from typing import Dict, List, Optional, Tuple

import requests


# ---------------------------
# Ollama configuration
# ---------------------------

OLLAMA_BASE_URL = os.environ.get("OLLAMA_API_URL", "http://localhost:11434")
OLLAMA_MODEL = os.environ.get("OLLAMA_MODEL", "llama3.1")


# ---------------------------
# Windowing (LLM batching)
# ---------------------------

WINDOW_SIZE = 10
WINDOW_OVERLAP = 3


@dataclass
class SegmentAnalysis:
    segment_type: str  # "approach", "commentary", "transition"
    conversation_id: int
    is_new_conversation: bool
    confidence: float
    reasoning: str


def _call_ollama(prompt: str, retries: int = 3) -> Optional[str]:
    for attempt in range(retries):
        try:
            response = requests.post(
                f"{OLLAMA_BASE_URL}/api/generate",
                json={
                    "model": OLLAMA_MODEL,
                    "prompt": prompt,
                    "stream": False,
                    "options": {
                        "temperature": 0.3,
                        "num_predict": 500,
                    },
                },
                timeout=60,
            )
            if response.ok:
                return response.json().get("response", "")
        except requests.exceptions.RequestException as e:
            if attempt < retries - 1:
                time.sleep(1)
                continue
            print(f"[LLM_v2] Ollama error: {e}")
    return None


# ---------------------------
# Video Type Detection
# ---------------------------

def detect_video_type_from_title(title: str) -> Optional[str]:
    """Heuristic detection from title - fast, no LLM needed."""
    title_lower = title.lower()

    # Strong talking_head signals
    talking_head_keywords = [
        "guide", "advice", "tips", "mistakes", "how to", "mindset", "rant",
        "motivation", "charisma", "communication", "texting", "messaging",
        "psychology", "confidence", "anxiety", "introvert", "what to say",
        "secrets", "rules", "principles", "theory", "breakdown"
    ]

    # Strong infield signals
    infield_keywords = [
        "infield", "daygame session", "approaches", "number close",
        "instant date", "street approach", "cold approach footage"
    ]

    if any(kw in title_lower for kw in infield_keywords):
        return "infield"

    if any(kw in title_lower for kw in talking_head_keywords):
        return "talking_head"

    return None


def detect_video_type_with_llm(segments: List[Dict], video_title: str) -> str:
    """LLM-based detection by sampling segments."""

    # First try heuristic
    heuristic_type = detect_video_type_from_title(video_title)
    if heuristic_type == "talking_head":
        # High confidence - talking_head keywords are reliable
        return "talking_head"

    # Sample first 5, middle 5, last 5 segments
    sample_indices = list(range(min(5, len(segments))))
    if len(segments) > 10:
        mid = len(segments) // 2
        sample_indices += list(range(mid - 2, mid + 3))
    if len(segments) > 15:
        sample_indices += list(range(len(segments) - 5, len(segments)))

    sample_indices = sorted(set(i for i in sample_indices if 0 <= i < len(segments)))

    sample_texts = []
    for i in sample_indices[:15]:
        text = segments[i].get("text", "")[:100]
        sample_texts.append(f"[{i}] {text}")

    prompt = f"""Classify this video's content type based on the title and sample segments.

VIDEO TITLE: "{video_title}"

SAMPLE SEGMENTS:
{chr(10).join(sample_texts)}

Content types:
- "infield": Coach approaching women on the street. Contains ACTUAL openers like "excuse me, two seconds", real responses from women, number closes.
- "talking_head": Coach talking to camera about theory, tips, or experiences. No actual approaches happening.
- "podcast": Coach talking with a co-host or student in interview/discussion format. Two people discussing topics, NOT approaching women.

KEY DISTINCTION: In "infield", the coach is DOING approaches. In "talking_head" and "podcast", the coach is TALKING ABOUT approaches.

Respond with ONLY one word: infield, talking_head, or podcast

Video type:"""

    response = _call_ollama(prompt)
    if response:
        response_lower = response.lower().strip()
        for vtype in ["infield", "talking_head", "podcast"]:
            if vtype in response_lower:
                return vtype

    # Default to infield if uncertain (safer to over-classify than under)
    return heuristic_type or "infield"


# ---------------------------
# Classification for Infield Videos
# ---------------------------

def classify_infield_segments(
    segments: List[Dict],
    start_idx: int,
    conv_id: int,
    prev_type: str,
) -> List[SegmentAnalysis]:
    """Classify segments in an INFIELD video using LLM."""

    window = segments[start_idx : start_idx + WINDOW_SIZE]

    segment_texts = []
    for i, seg in enumerate(window):
        text = seg.get("text", "").strip()
        segment_texts.append(f"[{i}] {text}")

    prompt = f"""Analyze these transcript segments from INFIELD daygame footage where a coach approaches women on the street.

Classify each segment as:
- "approach": Real-time dialogue with a woman (opener, vibing, number close, actual responses from her)
- "commentary": Coach talking to camera (explanation, breakdown, intro/outro, analysis)
- "transition": Brief marker between sections

APPROACH examples (actual conversation):
- "Excuse me, two seconds, I noticed you..."
- "What's your name?" "I'm Sarah"
- "Can I get your number?"
- Short responses like "thanks", "yeah", "sure"

COMMENTARY examples:
- "Hey guys, welcome back"
- "So what I did there was..."
- "The key to this technique is..."
- "As you can see, she responded well"

Previous segment type: {prev_type}
Current conversation ID: {conv_id}

Segments:
{chr(10).join(segment_texts)}

Respond with ONLY a valid JSON array. Example:
[{{"index": 0, "type": "approach", "new_conversation": true, "confidence": 0.9}}]

JSON response:"""

    response = _call_ollama(prompt)
    if not response:
        # Fallback to heuristic
        return [SegmentAnalysis("commentary", 0, False, 0.5, "fallback") for _ in window]

    try:
        # Parse JSON from response
        json_str = None
        start = response.find("[")
        if start != -1:
            bracket_count = 0
            for i, char in enumerate(response[start:], start):
                if char == "[":
                    bracket_count += 1
                elif char == "]":
                    bracket_count -= 1
                    if bracket_count == 0:
                        json_str = response[start : i + 1]
                        break

        if json_str:
            analyses = json.loads(json_str)
            results: List[SegmentAnalysis] = []
            current_conv_id = conv_id

            for item in analyses:
                seg_type = item.get("type", "commentary")
                if seg_type not in ["approach", "commentary", "transition"]:
                    seg_type = "commentary"
                is_new = bool(item.get("new_conversation", False))
                raw_conf = item.get("confidence")
                confidence = float(raw_conf) if raw_conf is not None else 0.5

                if is_new and seg_type == "approach":
                    current_conv_id += 1

                results.append(
                    SegmentAnalysis(
                        segment_type=seg_type,
                        conversation_id=current_conv_id if seg_type == "approach" else 0,
                        is_new_conversation=is_new,
                        confidence=confidence,
                        reasoning="llm_infield",
                    )
                )

            return results
    except (json.JSONDecodeError, KeyError, ValueError) as e:
        print(f"[LLM_v2] Parse error: {e}")

    return [SegmentAnalysis("commentary", 0, False, 0.5, "parse_fallback") for _ in window]


# ---------------------------
# Classification for Non-Infield Videos
# ---------------------------

def classify_non_infield_segments(segments: List[Dict], video_type: str) -> List[Dict]:
    """For talking_head/podcast videos, force ALL segments to commentary.

    This guarantees 0 false positives for non-infield content.
    """
    for seg in segments:
        seg["conversation_id"] = 0
        seg["segment_type"] = "commentary"
        seg["boundary_detection"] = {
            "is_new_conversation": False,
            "confidence": 0.95,
            "method": f"forced_{video_type}",
        }
    return segments


# ---------------------------
# Main Processing
# ---------------------------

def process_file(input_path: Path, output_path: Path, video_title: str = "") -> Dict:
    """Process a single file with v2 strategy."""

    with input_path.open("r", encoding="utf-8") as f:
        data = json.load(f)

    segments = data.get("segments", [])

    # Infer video title from filename if not provided
    if not video_title:
        # Extract from filename like "Video Title [id].something.json"
        name = input_path.stem
        match = re.match(r"(.+?)\s*\[", name)
        if match:
            video_title = match.group(1).strip()
        else:
            video_title = name

    print(f"[LLM_v2] Processing {input_path.name} ({len(segments)} segments)")
    print(f"[LLM_v2] Video title: {video_title}")

    # Step 1: Detect video type
    video_type = detect_video_type_with_llm(segments, video_title)
    print(f"[LLM_v2] Detected video type: {video_type}")

    # Step 2: Classify based on video type
    if video_type in ["talking_head", "podcast"]:
        # Force all to commentary - guaranteed 0 false positives
        print(f"[LLM_v2] Non-infield content - forcing all segments to commentary")
        segments = classify_non_infield_segments(segments, video_type)
    else:
        # Use LLM to classify infield segments
        total = len(segments)
        current_conv_id = 0
        prev_type = "unknown"
        idx = 0

        while idx < total:
            analyses = classify_infield_segments(segments, idx, current_conv_id, prev_type)

            non_overlap_count = WINDOW_SIZE - WINDOW_OVERLAP if idx > 0 else WINDOW_SIZE
            for i, analysis in enumerate(analyses[:non_overlap_count]):
                seg_idx = idx + i
                if seg_idx >= total:
                    break

                segments[seg_idx]["conversation_id"] = analysis.conversation_id
                segments[seg_idx]["segment_type"] = analysis.segment_type
                segments[seg_idx]["boundary_detection"] = {
                    "is_new_conversation": analysis.is_new_conversation,
                    "confidence": analysis.confidence,
                    "method": analysis.reasoning,
                }

                if analysis.conversation_id > current_conv_id:
                    current_conv_id = analysis.conversation_id
                prev_type = analysis.segment_type

            idx += WINDOW_SIZE - WINDOW_OVERLAP
            if idx >= total:
                break

            if (idx % 50) == 0:
                print(f"[LLM_v2] Processed {idx}/{total} segments...")

    data["segments"] = segments
    data["video_type"] = video_type

    # Summary
    conversation_ids = set()
    type_counts: Dict[str, int] = {}
    for seg in segments:
        conv_id = int(seg.get("conversation_id", 0) or 0)
        if conv_id > 0:
            conversation_ids.add(conv_id)
        seg_type = str(seg.get("segment_type", "unknown"))
        type_counts[seg_type] = type_counts.get(seg_type, 0) + 1

    data["conversation_summary"] = {
        "total_conversations": len(conversation_ids),
        "segment_type_counts": type_counts,
        "video_type": video_type,
    }

    output_path.parent.mkdir(parents=True, exist_ok=True)
    with output_path.open("w", encoding="utf-8") as f:
        json.dump(data, f, indent=2)

    print(f"[LLM_v2] Found {len(conversation_ids)} conversations")
    print(f"[LLM_v2] Segment types: {type_counts}")

    return data


# ---------------------------
# Path helpers (same as original)
# ---------------------------

def repo_root() -> Path:
    return Path(__file__).resolve().parents[2]


def speakers_root() -> Path:
    return repo_root() / "data" / "06.speakers"


def output_root() -> Path:
    return repo_root() / "data" / "07.LLM-conversations"


def extract_video_id_from_url(url: str) -> Optional[str]:
    url = (url or "").strip()
    if not url:
        return None
    m = re.search(r"[?&]v=([^&]+)", url)
    if m:
        return m.group(1)
    m = re.search(r"youtu\.be/([^?&/]+)", url)
    if m:
        return m.group(1)
    return None


def parse_sources_file(path: Path) -> List[Tuple[str, str]]:
    sources: List[Tuple[str, str]] = []
    for raw in path.read_text(encoding="utf-8").splitlines():
        line = raw.strip()
        if not line or line.startswith("#"):
            continue
        if "|" in line:
            name, url = line.split("|", 1)
            sources.append((name.strip(), url.strip()))
            continue
        parts = shlex.split(line)
        if len(parts) >= 2:
            sources.append((parts[0], parts[1]))
    return sources


def resolve_existing_path(p: Path) -> Optional[Path]:
    if p.exists():
        return p.resolve()
    p2 = repo_root() / p
    if p2.exists():
        return p2.resolve()
    return None


def compute_output_file(input_file: Path, out_dir: Path, suffix: str = ".conversations.json") -> Path:
    stem = input_file.stem
    if stem.endswith(".classified"):
        stem = stem[:-11]
    return out_dir / f"{stem}{suffix}"


def run_detection(input_path: Path, output_path: Path, video_filter: Optional[str], overwrite: bool = False) -> None:
    """Process all files in input directory."""

    if input_path.is_dir():
        files = sorted(
            f for f in input_path.rglob("*.json")
            if not f.name.endswith(".conversations.json")
        )
        if video_filter:
            files = [f for f in files if video_filter in f.name or video_filter in f.as_posix()]

        if not files:
            print(f"[LLM_v2] No input files found under: {input_path}")
            return

        print(f"[LLM_v2] Input : {input_path}")
        print(f"[LLM_v2] Output: {output_path}")
        print(f"[LLM_v2] Files : {len(files)}")

        written = 0
        skipped = 0
        for src in files:
            rel_path = src.relative_to(input_path)
            dest_dir = output_path / rel_path.parent
            dest = compute_output_file(src, dest_dir)

            if dest.exists() and not overwrite:
                skipped += 1
                continue

            print(f"\n[LLM_v2] {src.name} -> {dest.name}")
            process_file(src, dest)
            written += 1

        print(f"\n[LLM_v2] Done: written={written}, skipped={skipped}")
        return

    # Single file
    dest = compute_output_file(input_path, output_path)
    if dest.exists() and not overwrite:
        print(f"[LLM_v2] Output exists, skipping: {dest}")
        return

    process_file(input_path, dest)


def main() -> None:
    parser = argparse.ArgumentParser(description="Detect conversation boundaries (v2 with video type detection).")
    parser.add_argument("name", nargs="?", help="Channel/playlist name")
    parser.add_argument("youtube_url", nargs="?", help="YouTube URL")
    parser.add_argument("--channel", help="Alias for name")
    parser.add_argument("--sources", nargs="?", const="docs/sources.txt", help="Process from sources file")
    parser.add_argument("--input", help="Input JSON file or directory")
    parser.add_argument("--output", help="Output directory")
    parser.add_argument("--video", help="Filter by video ID/name")
    parser.add_argument("--overwrite", action="store_true", help="Overwrite existing outputs")
    args = parser.parse_args()

    name = (args.channel or args.name or "").strip() or None

    # Check Ollama
    try:
        response = requests.get(f"{OLLAMA_BASE_URL}/api/tags", timeout=5)
        if not response.ok:
            print(f"[LLM_v2] Warning: Ollama not available")
            return
    except requests.exceptions.RequestException:
        print(f"[LLM_v2] Cannot connect to Ollama at {OLLAMA_BASE_URL}")
        return

    if args.sources:
        sources_path = Path(args.sources)
        if not sources_path.is_absolute():
            sources_path = (repo_root() / sources_path).resolve()
        if not sources_path.exists():
            raise SystemExit(f"Missing sources file: {sources_path}")

        for src_name, src_url in parse_sources_file(sources_path):
            input_path = speakers_root() / src_name
            output_path = output_root() / src_name
            video_filter = args.video or extract_video_id_from_url(src_url)
            run_detection(input_path, output_path, video_filter, args.overwrite)
        return

    if args.input:
        input_path = resolve_existing_path(Path(args.input))
        if not input_path:
            raise SystemExit(f"Input not found: {args.input}")
    else:
        if not name:
            raise SystemExit("Provide --input or a <name>")
        input_path = speakers_root() / name
        if not input_path.exists():
            raise SystemExit(f"Not found: {input_path}")

    if args.output:
        output_path = Path(args.output)
        if not output_path.is_absolute():
            output_path = repo_root() / output_path
    else:
        if not name:
            raise SystemExit("Provide --output or use standard paths")
        output_path = output_root() / name

    video_filter = args.video or extract_video_id_from_url(args.youtube_url or "")
    run_detection(input_path, output_path, video_filter, args.overwrite)


if __name__ == "__main__":
    main()
